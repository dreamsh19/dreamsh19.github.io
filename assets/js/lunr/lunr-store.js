var store = [{
        "title": "github.io 첫 글",
        "excerpt":"GitHub Pages에서 작성한 블로그 첫 글이다.  ","categories": ["Blog"],
        "tags": ["Blog"],
        "url": "https://dreamsh19.github.io/blog/first-post/",
        "teaser": null
      },{
        "title": "3장 자바스크립트 개발 환경과 실행 방법",
        "excerpt":"자바스크립트 실행 환경 - 브라우저 vs Node.js           모든 브라우저 및 Node.js는 자바스크립트 엔진을 내장하고 있다              자바스크립트 엔진 : 자바스크립트 해석 및 실행 -&gt; ECMAScipt 지원                그러나 용도가 다르다                                  브라우저           Node.js                                           HTML, CSS, 자바스크립트를 실행해 웹페이지를 브라우저에 **렌더링**하기 위함           렌더링과 관계없이 **브라우저 외부**에서 자바스크립트 실행 환경을 제공하기 위함                             DOM API를 제공 ( HTML 요소를 조작하기 위해 )           DOM API 제공 x (브라우저와 관련된 DOM을 굳이 갖고 있을 이유 없다)                             파일 시스템 제공 x(보안상의 이유)           파일 시스템 제공 o                             클라이언트 사이드 웹 API 지원           Node.js 고유 API 지원                                           DOM API : HTML을 파싱하여 객체화한 DOM을 선택 및 조작하는 기능의 집합                        브라우저에서 실행되는 자바스크립트가 사용자 컴퓨터의 로컬에서 CRUD 가능한 것은 위험하다.                   웹 브라우저      크롬            자바스크립트 엔진 : V8                    Node.js에서도 사용                       개발자 도구(Command + option + i)                    Elements(최종 렌더링된 뷰), Console(에러 및 console.log 결과), Sources(자바스크립트 코드 디버깅), Network(네트워크 요청 정보 및 성능), Application(웹 스토리지, 세션, 쿠키) 등을 제공           Sources에서 breakpoint 설정하여 디버깅 가능                           브라우저는 HTML 파일의 &lt;script&gt; 태그에 포함된 자바스크립트 코드 실행   Node.js   프로젝트 규모가 커짐에 따라 브라우저만으로 개발 어려워짐   Node.js와 npm 필요성 대두   Node.js      크롬 V8 자바스크립트 엔진으로 빌드된 자바스크립트 런타임 환경   브라우저 이외의 환경에서 자바스크립트를 동작시킬 수 있도록 함   npm(Node Package Manager)      Node.js에서 사용할 수 있는 모듈을 모아둔 저장소   각 모듈의 설치 및 관리를 위한 CLI 제공   Node.js 설치           https://nodejs.org/ 에서 설치            macOS는 /usr/local/bin/node 에 설치            설치 후       $ node -v // v14.15.4 (21.01.31 기준 LTS) $ npm -v // 6.14.10           로 정상 설치 확인       Node.js 실행   $ node   로 REPL(Read Eval Print Loop) 또는   $ node index.js   로 js 파일 실행   Visual Studio Code   설치 : https://code.visualstudio.com/Download   확장 플러그인 설치      Code Runner : 에디터에서 자바스크립트 파일 실행 시켜줌(그냥 $ node 파일.js 해주는 것)   Live Server : 가상 서버 기동(port 5500)하여 브라우저 환경에서 HTML 파일 자동 로딩해주는 확장 플러그인            소스코드 수정 시 자동 반영       클라이언트 사이드 웹 API가 포함된 자바스크립트 코드 실행을 위해 필요. (아래 예시 참고)           cf) 아래 코드를 VS Code에서 실행하면?   const arr = [1, 2]; arr.forEach(alert);   결과는 ReferenceError      브라우저 환경 : alert 함수는 클라이언트 사이드 웹 API로, 브라우저 환경에서는 정상 작동   Node.js : 클라이언트 사이드 웹 API를 지원하지 않으므로 alert 함수를 알 수 없음.   Reference   본 포스트는 이웅모, 모던자바스크립트 Deep Dive의 3장 내용을 요약 및 재구성한 것입니다.   ","categories": ["Javascript"],
        "tags": ["Javascript","브라우저","Node.js","VS Code"],
        "url": "https://dreamsh19.github.io/javascript/3%EC%9E%A5-%EC%9E%90%EB%B0%94%EC%8A%A4%ED%81%AC%EB%A6%BD%ED%8A%B8-%EA%B0%9C%EB%B0%9C-%ED%99%98%EA%B2%BD%EA%B3%BC-%EC%8B%A4%ED%96%89-%EB%B0%A9%EB%B2%95/",
        "teaser": null
      },{
        "title": "4장 변수",
        "excerpt":"변수란 무엇인가?   변수란 값을 저장하기 위한 메모리 주소에 이름을 지정한 것(실제로 그 주소를 알게 하는 건 위험하다)   컴파일러 또는 인터프리터에 의해 메모리 공간의 주소로 치환된다.   var result = 10 + 20; // 할당(assignment) result; // 참조(reference)      result는 식별자(변수의 이름)   30은 변수 값   사실 result는 30이 아니라 30이 저장되어 있는 메모리 주소를 기억하는 것. = 메모리 주소의 이름   변수 선언   var score;   변수 선언 = Allocate(값을 저장하기 위한 메모리 공간 확보) + Name Binding(변수 이름과 메모리 주소를 연결)   변수 선언 키워드 - let, const, var      let, const 는 ES6에서 처음 등장 ( var의 한계 때문에 )            var의 한계 : 블록 레벨 스코프를 지원하지 않고 함수레벨 스코프를 지원하기 때문에 의도치 않은 전역 변수 선언으로 인한 부작용 발생 ? (15.1절 참고)           키워드 = 예약어 (자바스크립트 엔진이 수행할 동작을 규정한 예약어)   score에 메모리 공간 확보   score에는 null이 아니라 undefined가 할당되어 있다.(암묵적 초기화 자동수행) 자바스크립트의  독특한 특징      undefined는 자바스크립트의 primitive type value   초기화하지 않으면 이전의 쓰레기값이 참조될 수 있다.   자바스크립트 엔진의 변수 선언은 2단계(선언, 초기화)      선언 : 변수 이름 score를 등록하여 자바스크립트 엔진에 변수의 존재를 알림            실행 컨텍스트 내에 등록       Key/value 형식의 객체로 등록됨           초기화 : 메모리 공간을 확보하고 score 주소의 메모리에 undefined 할당하여 초기화.   ReferenceError : 선언하지 않은 식별자를 참조할 때(score가 아닌 다른 것들)   변수 선언의 실행 시점과 변수 호이스팅   console.log(score); // undefined var score;   결과 : 참조에러 발생하지 않고 undefined 출력   변수 선언이 한꺼번에 먼저 되고 나서 (Runtime 이전)   -&gt; Runtime에 소스코드가 sequential하게 실행하기 때문에 by 인터프리터   변수 호이스팅 : 변수 선언문이 코드의 선두로 끌어올려진 것처럼 동작하는 자바스크립트 고유의 특징   모든 선언문은 런타임 이전에 실행된다.   값의 할당   // 할당 방법 1 var score = 30;   // 할당 방법 2 var score; // 변수 선언 - 런타임 이전에 실행 score = 30; // 값의 할당 - 런타임에 실행   1과 2는 정확히 동일하게 작동(1이 2의 단축표현일 뿐)      변수 선언은 런타임 이전에 실행   값의 할당은 런타임에 실행됨   console.log(score); // undefined score = 30; // (1) var score; // (2) console.log(score); // 30   (2)가 먼저 실행되어 score 변수 주소(ex. 0xF2)에 undefined로 초기화   -&gt; (1)에서 score에 새로운 주소를 확보하고 그 주소에 30을 할당(기존 주소 0xF2에 덮어쓰는게 아님) ?   -&gt; 기존의 0xF2는 더 이상 사용되지 않는 메모리(어떤 식별자도 참조하지 않음)   -&gt; Garbage Collector에 의해 메모리 free (자바스크립트는 managed language)      Unmanaged language : 개발자가 명시적으로 메모리 할당, 해제 ex) C   Managed language : 개발자의 직접적인 메모리 제어를 불허   값을 재할당할 때도 덮어쓰는게 아니라 새로운 주소를 확보하고 확보한 주소에 새로운 값을 저장   const 키워드로 선언된 변수는 단 한 번만 값을 할당할 수 있음.   식별자 네이밍   문자, 숫자, _, $ 포함 가능. 단 첫 문자는 숫자 제외 / 예약어 제외   var person, _name, $elem, val1; // 가능 var first-name, 1st, this; // 불가능    Reference   본 포스트는 이웅모, 모던자바스크립트 Deep Dive의 4장 내용을 요약 및 재구성한 것입니다.   ","categories": ["Javascript"],
        "tags": ["Javascript"],
        "url": "https://dreamsh19.github.io/javascript/4%EC%9E%A5-%EB%B3%80%EC%88%98/",
        "teaser": null
      },{
        "title": "5장 표현식과 문",
        "excerpt":"표현식(Expression)   값           표현식이 평가되어 생성된 결과       메모리에 저장된 값은 데이터 타입에 따라 다르게 해석된다.            ex) 0100 0001 - 숫자로는 65, 문자로는 ‘A’           값의 생성은 리터럴을 통해서   리터럴   // 리터럴 예시 100 10.5 0x41 'Hi' true null {name : 'Lee'} [1,2,3] foo(){}           사람이 이해할 수 있는 문자(아라비아 숫자, 알파벳 등) 또는 약속된 기호(“”,[],{} 등)를 사용해 값을 생성하는 표기법       = 값을 생성하기 위해 미리 약속한 표기법            자바스크립트 엔진은 런타임에 리터럴을 평가해 값을 생성       표현식           값으로 평가될 수 있는 문(Statement)       =&gt; 표현식은 값처럼 사용할 수 있다 = 값이 올 수 있는 자리에는 표현식도 올 수 있다.            아래 예는 모두 표현식이다.       // 리터럴 표현식 10 'Hello'  // 식별자 표현식 score person.name arr[1]  // 연산자 표현식 1+2 sum = 10 sum !== 10  // 함수 &amp; 메서드 호출 표현식 square() person.getName()   문(Statement)           프로그램 최소 실행 단위       변수 선언문, 할당문, 함수 선언문, 조건문, 반복문 등   세미콜론(;)은 문의 종료를 나타낸다.        코드 블록에는 세미콜론을 붙이지 않는다. 코드 블록 자체가 문의 종결을 의미하기 때문에              코드 블록 : 0개 이상의 문을 중괄호{}로 묶은 것                세미콜론은 자동 삽입되어(by 자바스크립트 엔진) 생략 가능하지만 붙이는 걸 권장한다.       문 = 표현식 + 표현식이 아닌 문                     표현식 : 값으로 평가될 수 있는 문 =&gt; 값처럼 사용할 수 있다. = 변수에 할당할 수 있다.                        표현식이 아닌 문 : 값으로 평가될 수 없는 문 =&gt; 값처럼 사용할 수 없다.                        var x = 10; // 표현식이 아닌문 x = 10; // 표현식 var foo = (var x = 10) ; // SyntaxError : 표현식이 아닌 문을 변수에 할당 var foo = (x = 10); // 가능 : 표현식을 변수에 할당                           Reference   본 포스트는 이웅모, 모던자바스크립트 Deep Dive의 5장 내용을 요약 및 재구성한 것입니다.   ","categories": ["Javascript"],
        "tags": ["Javascript"],
        "url": "https://dreamsh19.github.io/javascript/5%EC%9E%A5-%ED%91%9C%ED%98%84%EC%8B%9D%EA%B3%BC-%EB%AC%B8/",
        "teaser": null
      },{
        "title": "베이스캠프 Sprint1",
        "excerpt":"21.1.18부터 본격적인 베이스캠프가 시작되었다. 베이스캠프를 1주간 진행하면서 느낀 점을 적어보고자 한다.   지난 1주간 진행한 건 크게 3가지였다.      스프링부트 사전과제   Github 블로그 생성   맥북과 친해지기   1. 스프링부트 사전과제   구멍가게 코딩단, &lt;코드로 배우는 스프링 부트 웹 프로젝트&gt;를 보며 스프링 부트 학습을 위한 사전과제를 진행했다.   웹 개발 경험도 많지 않고 스프링도 처음 접해보기 때문에 쉽지 않았다. 그리고 gradle부터 jpa, jdbc, jquery 등 낯선 개념들이 많아서 책 읽는데 시간이 오래걸렸다. 또, 기존에 자바를 주언어로 사용해왔지만, 이해하기 어려운 문법이나 디자인 패턴이 있어서 더 많이 공부해야겠다고 느꼈다.   수요일, 목요일에는 재택으로 진행했기 때문에 업무 시작 시간에 TF끼리 사전과제 진행상황과 전날 있었던 이슈에 대해 공유했다. 그 과정에서 내가 Docker로 MariaDB를 띄우는 과정에서 발생했던 이슈를 공유했고, 그걸 공유 문서로 작성할 기회가 주어졌다. 그래서 해당 이슈에 대해 공유 문서로 작성하면서 다른 동기들에게 정확하게 알려주기 위해 더 자세히 찾아보고 공부하게 되었고, 그러면서 해당 이슈에 대해 더 깊이 이해할 수 있었다. 그렇기 때문에 공유 문서로 작성했던 내용은 오래도록 기억에 남을 것 같았고, 앞으로도 자주 공유할 수 있도록 노력해야겠다는 생각을 했다.   2. Github 블로그 생성   Github에서 제공하는 웹호스팅 서비스인 Github Pages를 이용하여 블로그를 생성했다. 로컬에서 실행하기 위한 jekyll, bundle 세팅이 쉽지 않았다. Ruby 버전의 충돌로 로컬에서 띄우는데 어려움이 있었지만 rvm을 이용하여 해결하였다. 테마는 minimal-mistakes를 채택하였고 지금은 Disqus 연동까지 완료하여 이렇게 블로그에 글을 작성하고 있다.   3. 맥북과 친해지기   예전에 약 두 달간 맥북을 사용해 본 것 외에는 평생 윈도우만 써와서 맥북과 친해지는데 시간이 필요했고 지금도 친해지는 중이다. 여러가지 단축키를 익혔고, 자동화도 몇 개 등록해서 쓰고 있다. 하지만 트랙패드와는 친해지지 못했고 손목이 너무 아파서 앞으로도 마우스를 사용해야할 것 같다.   마무리   사전과제를 진행하면서 처음 해보는 것들이 많아서 어려움을 겪었으나, 새로운 것들이었기 때문에 오히려 배우는 재미가 있었다. 앞으로도 계속 새로운 것들을 배워나갈텐데 어려움이 있겠지만 베이스캠프를 마쳤을 때 성장해 있는 내 모습이 기대가 된다. 그리고 무엇보다 주변에 훌륭한 동기들이 있어서 좋았다. 이슈가 발생했을 때 동기들로부터 많은 도움을 받았고, 나도 앞으로 동기들에게 도움이 되는 존재가 되고 싶다. 그러는 과정에서 긍정적인 상호작용을 이뤄낼 수 있는 관계가 되었으면 좋겠다.  ","categories": ["Basecamp"],
        "tags": ["Basecamp","Sprint1","Spring Boot","Github Pages","Mac"],
        "url": "https://dreamsh19.github.io/basecamp/%EB%B2%A0%EC%9D%B4%EC%8A%A4%EC%BA%A0%ED%94%84-Sprint1/",
        "teaser": null
      },{
        "title": "10장 객체 리터럴",
        "excerpt":"객체란   원시 값을 제외하면 모두 객체.                  원시 값       객체                       변경 불가능(immutable)       변경 가능(mutable)           // 객체 리터럴에 의한 인스턴스 생성 var person = { \tname: \"foo\", \tsayHello: function(){ // ES5 \t\tconsole.log(`Hello. I'm ${this.name}`); \t} }; // 세미콜론 붙임. 코드블럭과의 차이   객체는 0개 이상의 프로퍼티로 구성된 집합   프로퍼티 = 키 : 값(딕셔너리)   값은 함수가 될 수도 있고 함수인 경우 메소드라도 부름.   결국 객체는 프로퍼티와 메소드로 구성된 집합체      프로퍼티는 “상태”를 나타내는 데이터 ex) name   메소드는 ‘‘동작” ex) sayHello            메서드 : 객체의 프로퍼티 값이 함수(일반함수와 구분하기 위해) =&gt; 객체에 묶여 있는 함수           =&gt; 상태와 동작을 하나의 단위로 구조화할 수 있어 유용   인스턴스 생성   c++, java : 클래스 기반 객체지향 언어 =&gt; constructor로 인스턴스 생성   javascript : 프로토타입 기반 객체지향 언어   =&gt; 객체 리터럴로 생성(생성자 함수 등을 통해 생성할수도 있지만 객체 리터럴이 가장 일반적이고 간단)   프로퍼티 키 규칙   식별자 네이밍 규칙을 따르는 프로퍼티 키를 사용하자   var person = {     firstName : 'World', // 적합     'last-name' : 'Hello', // 가능하지만 따옴표를 붙여야한다 - 권장 x \t0 : 'zero' // 프로퍼티 키로 숫자를 사용가능하지만 내부적으로 문자열로 자동 변환됨 };\t  person[0]; // zero person['0']; // zero   프로퍼티 CRUD   var person = {     name: 'Lee' };  console.log(person.name); // Lee (가능) console.log(person['name']); // Lee (가능. 따옴표 필요) console.log(person.age); // undefined(ReferenceError가 발생하지 않으니 주의) console.log(person[name]); // ReferenceError   person.age = 20;  // age 프로퍼티가 없으면 20으로 생성 // age 프로퍼티가 있으면 20으로 업데이트   delete person.age; // age 프로퍼티가 없으면 무시(에러 발생 x) // age 프로퍼티가 있으면 삭제   ES6 객체 리터럴 확장 기능   1. 프로퍼티 축약 표현   // ES5 var x = 1, y = 2; var obj = {     x: x;     y: y; };   // ES6 let x = 1, y = 2; const obj = {x, y}; // 키 이름은 변수 이름으로 자동 생성   2. 객체 리터럴 내부 동적 프로퍼티 계산   // ES5 - 객체 리터럴 \"외부\"에서 프로퍼티 키 동적 생성해야함 var prefix = 'pre'; var i = 0; var obj = {}; obj[prefix + '-' + ++i] = i; obj[prefix + '-' + ++i] = i;   // ES6 - 객체 리터럴 \"내부\"에서도 프로퍼티 키 동적 생성 가능 const prefix = 'pre'; let i = 0; const obj = {     [`$[prefix}-${++i}`] : i,     [`$[prefix}-${++i}`] : i };   3. 메서드 축약 표현   // ES5 var obj = { \tsayHi: function(){         console.log(\"Hi\"); \t} };   // ES6 var obj = { \tsayHi(){         console.log(\"Hi\"); \t} };   단, 둘의 동작 방식은 서로 다르다. 26.2절에서 후술   심화   var person = { \t'last-name' : 'Lee'\t }; console.log(person.last-name); // (1)   (1)의 결과가 브라우저 환경과 Node.js 환경에서 다르다.      브라우저 환경 : NaN            person.last와 name의 - 연산으로 생각       person.last : undefined       name : 브라우저 환경의 전역 객체 window의 프로퍼티 중 하나, 디폴트 값은 ‘‘(빈 문자열)       따라서 undefined - ‘’ = NaN           Node.js : ReferenceError            브라우저 환경과 달리 name 식별자 존재하지 않음       따라서 ReferenceError           Reference   본 포스트는 이웅모, 모던자바스크립트 Deep Dive의 10장 내용을 요약 및 재구성한 것입니다.   ","categories": ["Javascript"],
        "tags": ["Javascript"],
        "url": "https://dreamsh19.github.io/javascript/10%EC%9E%A5-%EA%B0%9D%EC%B2%B4-%EB%A6%AC%ED%84%B0%EB%9F%B4/",
        "teaser": null
      },{
        "title": "베이스캠프 Sprint2",
        "excerpt":"21.01.25부터 21.01.29까지 베이스캠프 Sprint2를 진행했다.   Sprint2에서는 서비스 기획을 진행했다. 티켓 예매 서비스라는 큰 주제 안에서 우리 팀은 항공편 예매 서비스를 기획하기로 했다.   영화 예매를 예로 들어주셔서 그걸 그대로 해도 됐지만 신선하고 재밌는 서비스를 만들고 싶다는 것에 팀원 모두 동의해서 항공편 예매로 결정했다.   나도 항공편 예매 서비스가 정말 마음에 든다.   그런데 요구사항에 비해 오버스펙인 건 사실이다. 항공편 예매 서비스는 아무래도 편도 뿐만 아니라 왕복, 경유 등의 옵션까지 고려해야 하기 때문에 티켓 하나 == 이벤트 하나 가 아니라 이벤트 하나에 여러 개의 티켓이 대응되는 등 요구사항에 비해 복잡한 구현이 필요하다.   후에 개발 시에 이런 기획을 한 것을 후회할 것 같기도 하지만 일단 신선한 서비스를 만들어보고 싶었기 때문에 주제도 마음에 들고 앞으로의 프로젝트가 기대가 된다.   기획   철저히 기획자의 입장에서 서비스를 기획했다. 기획 회의 중에 계속 개발자의 관점에서 생각하게 되고 자꾸 개발 쪽으로 이야기가 빠지는 경우가 종종 있었는데 그때 우리 팀은 의식적으로 개발자의 관점을 배제하고 기획자의 입장에서 서비스를 기획했다. 그러다 보니 당연하게도 너무 많은 기능이 들어가서 필수 기능과 추가 기능을 분리해두어 필수 기능을 전부 구현하고 나면 추가 기능을 하나씩 넣는 것으로 결정했다. 사실 필수 기능만 제대로 구현하는 것도 쉽지 않아 보인다.   기획을 하다보니 기획자의 어려운 점에 대해서도 느낄 수 있었다.   우선 일반적인 사용자 경험을 떠올리는 것이 쉽지 않았다. 누구나 자신의 경험을 기반으로 사용자 경험을 떠올리기 때문에 팀원이 떠올리는 사용자 경험이 각각 달랐다. 우리 팀의 경우에 편도/왕복 편의 페이지 UI를 구성할 때 왕복인 경우 편도 항공편 2개를 따로 결정하도록 할 것인지, 왕복 항공편을 한꺼번에 결정하도록 할 것인지 의견 충돌이 발생했다. 꽤나 오랜 시간의 의논 끝에 결국 전자로 결정했다. 여기서 중요한 점은 의논의 결과가 전자인지 후자인지가 아니라, 전자를 주장하는 팀원과 후자를 주장하는 팀원 모두 각각 자신의 의견을 적극적으로 피력해주었다는 점이다.   그리고 서비스를 기획하는 입장에서 사용자에게 어느 정도의 자유도를 주는게 좋을지 결정하는 것이 어려웠다. 예를 들어 항공편의 경우 날짜만 정해서 검색을 하게 할지, 날짜와 시간까지 정해서 검색을 하게 할지가 고민이 되었다. 현재 서비스하고 있는 항공편 예매 시스템을 참고하여 우리 팀도 날짜만 정해서 검색하도록 결정하긴 했지만, 결정권의 정도를 정하는 일이 쉽지 않았다. 결정권을 너무 안 주면 다양한 옵션을 원하는 사용자가 불편을 겪을 것 같고, 결정권을 지나치게 많이 주면 사용자가 모든 것을 결정해야하는데서 오는 스트레스를 유발할 것 같다.   커뮤니케이션   기획은 정말 회의의 연속이었고, 이는 곧 커뮤니케이션의 연속이었다. 우리 팀은 그래도 커뮤니케이션이 잘 되는 것 같았다. 어느 정도 의견 충돌도 있었지만 조율의 과정을 통해 하나의 결론으로 의견이 모아졌기 때문이다. 어디선가 ‘‘커뮤니케이션 과정에서 충돌이 일어나지 않았다면 커뮤니케이션이 제대로 일어나지 않았을 가능성이 높다’‘라는 말도 들었던 것 같다. 그리고 의견 충돌이 있었을 때 내 의견이 받아들여진 경우도 있고, 내 의견을 굽히고 다른 사람의 의견을 받아들인 경우도 있었다. 이때, 상대방의 감정을 상하지 않게 하는 것이 중요하다는 생각을 했다. 상대방의 의견이 받아들여지지 않은 경우에도 그 의견을 제시한 상대방의 기분이 나쁘지 않게 잘 말해야하고 내 의견을 굽힐 때도 어쩔 수 없이 굽히는 게 아니라 상대방의 의견이 정말 옳다고 생각해서 받아들인다는 것을 잘 표현하는 것이 중요하다고 느꼈다. 특히나 나는 표정이 별로 없는 편이기 때문에 상대방의 오해를 사기 쉽다는 생각을 한다. 이 부분은 앞으로도 더 노력하면서 고쳐나가야할 부분이라고 생각한다.  ","categories": ["Basecamp"],
        "tags": ["Basecamp","Sprint2","기획"],
        "url": "https://dreamsh19.github.io/basecamp/%EB%B2%A0%EC%9D%B4%EC%8A%A4%EC%BA%A0%ED%94%84-Sprint2/",
        "teaser": null
      },{
        "title": "베이스캠프 Sprint3",
        "excerpt":"21.02.01부터 21.02.05까지 베이스캠프 Sprint3를 진행했다.   Sprint 3에는 교육 위주로 진행되었다.   Sprint 2에서 기획한 서비스를 구현하기 위해 꼭 필요한 교육들이 진행되었다. 교육 과정이 정말 정밀하게 설계되어 있음을 다시 한 번 느끼는 순간이었다.   교육을 듣다보니 다른 동기들에 비해 개발 경헝이 부족한 편이라는 걸 많이 느꼈다. 하지만 그것이 약점이 된다기보다 우수한 동기들에게 배울 점이 많아서 오히려 좋다고 느껴졌다. 그리고 오히려 경험이 많이 없는 상태에서 교육을 받으니 새하얀 도화지에 그림을 그려나가는 기분이라 재밌었고 정말 유익했다.   아래는 Sprint 3에서 진행한 교육들이다.      Git &amp; Github   Vim   HTML/CSS   Bash   스프링 프로젝트 설정   자바스크립트   스프링 프로젝트 설정 교육을 제외하고는 기존에 알고 있던 지식이나 경험이 어느 정도 있었으나 대충만 알고 있는 경우가 많았는데, 이번 교육을 통해 그러한 부분들을 확실하게 익혀갈 수 있었다. 반면, 스프링 프로젝트 설정 교육은 낯선 내용이라 교육 내용을 100% 받아들이긴 어려웠고,  DI, IoC 등의 개념도 제대로 이해하지 못했다. 스프링 관련해서는 내가 아는 만큼 더 많이 배울 수 있는 교육이었던 것 같은데 기존에 아는 것이 많지 않아 조금 아쉬웠다.   그리고 사실 교육을 듣긴 했지만 아직 완전히 내 것으로 만들진 못했다. 그래서 혼자 위의 교육들을 정리하는 시간이 필요할 것 같다. 그리고 정리한 내용을 이 블로그에 포스팅할 예정이다. 하나씩 작성할 때마다 위의 목록에 링크를 달 생각인데, 얼른 모든 목록에 대해 링크를 달고 싶다. 링크를 전부 달게될 때는 지금보다 훨씬 성장해 있을 것 같아 기대가 된다.  ","categories": ["Basecamp"],
        "tags": ["Basecamp","Sprint3","교육"],
        "url": "https://dreamsh19.github.io/basecamp/%EB%B2%A0%EC%9D%B4%EC%8A%A4%EC%BA%A0%ED%94%84-Sprint3/",
        "teaser": null
      },{
        "title": "베이스캠프 Sprint4",
        "excerpt":"21.02.08부터 21.02.10까지 베이스캠프 Sprint4를 진행했다.   설 연휴가 포함되어 있어 실제 근무일은 3일밖에 되지 않았다.   Sprint5의 시작이었던 월요일 오전에는 Sprint4에 이어 교육을 진행했다      HTTP/HTTPS   월요일 오후에는 DB Power Designer Tool을 이용하여 DB 모델링을 진행했다. 회의하면서 다 같이 작성했는데 급하게 작성하다 보니 후에 개발 시 발견되는 오류가 있었고, unique 제약이라던가, 각 칼럼들의 제약사항에 대해서도 충분한 논의가 이루어지진 않은 것 같다. 사실 테이블 간 연관관계나 각 칼럼들의 제약사항 이외에도 현업에서의 DB 모델링 시 어떤 것들을 더 고려해야하는지를 잘 알지 못해서 뭔가 끝내고도 끝나지 않은 듯한 기분이었다. 그리고 DB 모델링이 바뀌면 개발 프로젝트 코드 전체를 엎어야 될 수도 있을만큼 DB 모델링이 개발 프로젝트 진행 시 중요한 부분이라고 생각했는데 그 중요성에 비해 꼼꼼하게 진행하지 못한 점이 아쉬웠다.   그리고 화요일 이후로는 실제로 개발을 시작하여 다음과 같은 설정이나 기능 구현을 진행했다.   프로젝트 설정           .gitignore 설정       MacOS 관련 설정을 추가했다.            Git branch protection rule 설정       사실 이런 기능을 처음 적용해보았다. Git 교육 당시 main 브랜치로 팀원 동의 없이 merge가 되어 revoke하는 등 당황스러운 경험이 있었다. 그래서 실제 프로젝트에서는 이런 걸 방지하기 위한 것들이 있지 않을까하여 찾아보았고 git branch protection rule이라는 설정이 존재했다. 그래서 우리 팀 프로젝트에도 해당 설정을 적용했고, develop 브랜치는 2명 이상, main 브랜치는 3명 이상의 approval이 있을 때만 merge할 수 있도록 설정했다.       메인 페이지 구성   HTML 기초가 부족하여 몇 가지 사소한 이슈가 있었다.      다양한 input type   radio 버튼에서 name의 역할        form 태그에 대해서                   form 태그 안에 button 있는 경우 button type 지정안해주면 디폴트값이 submit이라 클릭이벤트 지정안해도 자동 submit 되네요… 이거 막으려면 button type=”button”으로 주고 이벤트스크립트 작성해서 클릭이벤트 지정하면 됩니다….            그리고 js 구현 시 기술교육에서 배웠던 것들과 책에서 읽은 것들을 활용할 수 있었다.   Entity 오류 수정   기존에 일괄적으로 작성되었던 Entity 클래스들에 오류 사항이나 개선 사항을 수정했다.      @ToString에서 exclude 설정에 FK 필드 추가   @Column, @JoinColumn에 대한 이해            name 설정       nullable 설정           사전과제 추가 공부   사전과제 스프링부트 프로젝트에서 사실 단일 테이블 모델에 대해서만 구현을 해봤다. 그래서 우리 DB 모델은 1:N 관계가 대부분이기 때문에 어떻게 구현해야할지 감이 안와서 지난 사전과제 때 못했던 스프링부트 프로젝트에서 1:N에 해당하는 board 프로젝트를 공부했다. 그리고 공부한게 생각보다 많은 도움이 되었다. 전혀 감을 못잡고 있던 상황에서 board 프로젝트에 나온 내용들 중 우리 프로젝트에서 적용할 내용들을 발견할 수 있었고 그에 따라서 내가 구현할 기능들을 어떻게 짜면 되겠다는 그림을 그려나갈 수 있었다. 물론 실제 구현 상에는 다양한 이슈가 있겠지만 전체적으로 그림을 그릴 수 있게 되었다는 데 큰 의의가 있는 것 같다. board 프로젝트를 공부하고 나니 M:N에 해당하는 영화리뷰 프로젝트까지 했다면 우리 프로젝트 개발에 좀 더 나은 그림을 그릴 수 있었을텐데 아쉽다는 생각이 든다.  ","categories": ["Basecamp"],
        "tags": ["Basecamp","Sprint4","교육","개발"],
        "url": "https://dreamsh19.github.io/basecamp/%EB%B2%A0%EC%9D%B4%EC%8A%A4%EC%BA%A0%ED%94%84-Sprint4/",
        "teaser": null
      },{
        "title": "베이스캠프 Sprint5",
        "excerpt":"21.02.15부터 21.02.19까지 베이스캠프 Sprint5를 진행했다.   개발만 진행한 Sprint였다. 개발하면서 당연히 많은 이슈들이 있었고 당연히 힘들었지만 역시 배운게 많은 한 주였다. 여러가지 이슈들 중에서 큼직했던 것들만 이 글에 적어보려한다.   검색 기능 구현           maven querydsl 설정       이 설정이 정말 나를 힘들게 했다. 구글링도 엄청 해보고 멘토님께도 질문하고 운영진 방에도 질문드리면서 여러가지 해결책들을 전부 적용해봤는데 해결이 안됐다. 결국 포기하는 마음으로 intellij를 껐다켰더니 잘 됐다… pom.xml에 명시한 항목 중 일부 클래스가 intellij에서 로드가 안되었던 것이다… 긍정적으로 생각하자면 덕분에 querydsl에 관련된 의존성과 maven의 pom.xml에 대해서 자세히 공부하게 되었다. 이 이슈가 없었다면 아마 pom.xml을 자세히 들여도 볼 일은 없었을 것 같다.            JPQL fetchcount()       내가 작성한 JPQL 쿼리에 대해서 페이징을 구현하기 위해 fetchcount()가 필요했는데 정상 작동이 안됐다. 구글링 해보니 복잡한 쿼리에 대해서는 fetchcount() 가 정상 작동이 안되니(단순히 select를 count로만 바꾸는 방식으로 구현되어 있다고 한다) count 쿼리를 따로 작성해줘야한다는 결론까지만 찾았고 별 다른 대안을 찾지 못했다. 이 부분은 좀 더 찾아보고 싶다.            FlightDTO 공통 구현       검색 결과 페이지와 마이 페이지에서 항공편에 대한 정보를 보여주기 때문에 개발에 있어서 공통부분이 있을 것 같다는 의견하에 마이페이지를 구현하는 팀원과 논의를 통해 공통 부분을 구현했다. 공통부분을 Abstract class로 정의하고 실제 구체화된 클래스에서 이를 상속받아 각자 필요한 부분을 덧붙여서 구현하는 방식을 채택했다. (사실 검색 결과 페이지에서 사용하는 클래스는 abstract class의 필드를 그대로 사용해도 돼서 후에 abstract 속성을 해제하여 구체화된 클래스로 사용했다.) 객체 지향 원리 중 상속을 실제로 이용하여 개발을 진행한 경험이라 신기하기도 하고 뿌듯했다.       메인 페이지 구현           css       기존에 하나도 적용이 안되어 있던 페이지에 css를 적용하면서 flex box에 대한 이해를 할 수 있었다. 하지만 css는 여전히 마음대로 안되고 티가 나는 것에 비해 시간이 너무 많이 들어서 나한테 있어서 가성비가 너무 안좋았다. 그리고 정말 시간도둑이었다. 운영진에서 많은 시간을 쏟지 말라고 말씀하신 이유를 알게 되었다. Toast-ui도 적용해보았는데 생각보다 api 문서가 친절하지 않아서 적용하는데 시간이 좀 걸렸다. 사실 html이나 js에 대한 기초지식이 부족해서 그런 것 같기도 하다. API 문서에서 당연히 알만한 부분이라고 생각하여 생략한 부분을 몰라서 헤맸던 것 같다.       검색 결과 페이지 구현      Http Session   세션과 쿠키에 대한 이해를 할 수 있었다. 처음에 쿠키로 정보를 넘기려다가 쿠키가 스트링밖에 전달이 안됨을 깨닫고 객체 자체를 넘기고 싶어서 쿠키 대신 세션을 사용하고자 했다. 원래는 DTO 객체에 담긴 내용을 객체 통째로 넘기고 싶어서 세션으로 구현을 했다. 하지만 막상 구현하고보니 꼭 필요한 정보가 몇개 되지 않아서 쿠키에 스트링으로 저장해도 됐을 것 같고 혹은 그냥 url에 파라미터로 넘기는 방식이 오히려 나을 것 같다. 추후에 수정할 예정이다.   후기   어떻게든 결과물을 만들어 내긴 했지만 시간에 쫓겨서 마무리한 점이 아쉽다. 물론 기술교육에서 의도한 바도 어떻게든 듀에 맞춰서 서비스 배포까지 마치는 걸 목표로 하는 것 같았다. 좋은 코드 필요없다고 하셨고 그 시간에 기능 하나라도 더 구현하라고 하신 말씀이 생각난다. 좋은 코드를 만들어내는 건 아마 이후의 sprint에서 진행할 것 같다.   처음에는 좋은 코드를 작성하기 위해 노력을 했던 것 같다. 테스트도 짜면서 코드 개발을 했고, 같은 기능을 구현하기 위해서 여러가지 있는 방법들 중에 best를 선택하기 위해 검색도 많이 하면서 개발을 진행했다. 그러다 보니 결국 나중에는 시간에 쫓겨서 주어진 기능조차 다 개발하지 못할 것 같다는 생각이 들었다. 그래서 결국에는 좋은 코드보다는 어떻게든 동작하게끔 하는 코드를 “빠른 시간 안에” 개발하는데 집중했던 것 같다.   그리고 시간에 쫓겨서 아쉬웠던 점은 다른 팀원이 PR을 올렸을 때 코드 리뷰를 다 하지 못한 점이다. 사실 나는 PR에 올라오는 모든 코드를 읽어보고 리뷰를 하고 싶었다. 내가 이해 안되는 부분은 반드시 짚고 넘어가고 내가 개선사항을 제시할 수 있는 부분은 제시하고 싶었다. 그래야 프로젝트 전반에 대해서 각 기능들이 어떻게 구현되어 있는지를 내가 알고 있을 수 있고, 다른 사람의 코드를 읽는 데서 성장을 많이 할 수 있다고 생각했기 때문이다. 그래서 기능 구현이 급하지 않을 때는 다른 팀원들의 코드를 한줄한줄 읽어보고 리뷰를 남기기도 했지만 모든 PR에 대해 그렇게 하지는 못해서 아쉽다.   추가적으로 배포 관련된 설정은 아직 완전히 이해하지 못해서 혼자 내 개발 서버에 띄우는 실습을 진행해봐야 할 것 같다.   ","categories": ["Basecamp"],
        "tags": ["Basecamp","Sprint5","개발"],
        "url": "https://dreamsh19.github.io/basecamp/%EB%B2%A0%EC%9D%B4%EC%8A%A4%EC%BA%A0%ED%94%84-Sprint5/",
        "teaser": null
      },{
        "title": "Intellij 단축키",
        "excerpt":"직접 쓰면서 유용하다고 느낀 intellij 단축키 모음.   사용하면서 하나씩 추가 예정                  Cmd + Option + B       구현 찾기(구현 클래스)                       Cmd + B       정의 찾기(인터페이스)                 **Cmd + [**       **이전코드로 돌아가기**                 Cmd + ;       프로젝트 설정                 **Cmd + Shift + O**       **프로젝트 내 파일 찾기 **- 탭으로 클래스, symbol 검색도 가능                 Cmd + 1       좌측 네비게이션 바로 focus                 Cmd + 4       하단 Cmd+B콘솔로 focus                 **Cmd + Option + L**       **코드 포맷팅**                 Cmd + L       해당 라인으로 가기                 **Cmd + Shift + F**       **프로젝트 전체에서 키워드 검색 R은 찾아바꾸기(replacement)**                 Cmd + E       최근 파일           ","categories": ["Intellij"],
        "tags": ["Intellij","단축키"],
        "url": "https://dreamsh19.github.io/intellij/Intellij-%EB%8B%A8%EC%B6%95%ED%82%A4/",
        "teaser": null
      },{
        "title": "베이스캠프 Sprint6",
        "excerpt":"21.02.22부터 21.02.26까지 베이스캠프 Sprint6을 진행했다.   Sprint5에서 진행한 개발 결과물에 대해서 리뷰하는 스프린트였다. Sprint5에서는 어떻게든 기획대로 작동하는 결과물을 내는 것이 목표였다면, Sprint6에서는 그것을 조금 더 나은 방향으로 만드는 것이 목표였다.   여기서 말하는 더 나은 방향이라는 것은      성능과 구조적 이점을 위한 더 나은 DB 모델링   좋은 코드 작성            가독성이 좋은 코드 작성       불필요한 자원을 낭비하지 않는 코드 작성       버그 발생 여지가 적은 코드       목적에 맞는 독립적인 모듈 구분           라는 생각이 들었다. s   DB 모델링 교육   DB 모델링 교육은 사실 우리가 구성한 ERD 및 다른 팀들의 ERD들에 대한 피드백이 위주였다. 우리 팀의 피드백에 대해서는 테이블 구조를 바꿀만큼의 피드백은 들어오지 않아서 다행히 프로젝트를 갈아엎을 정도는 아니었다. 대신 여러 조직 및 사람과의 협업하기에 좋은 디비 모델링에 대해서도 알 수 있게되었고, 소문자를 대부분 많이 쓰고, MySQL에서는 연월일시분초에 대해 timestamp보다는 datetime을 많이 쓴다는 점 등 현업에서 많이 쓰는 규칙에 대해서 알 수 있었다. 우리팀은 테이블을 전부 대문자로 써서 전부 고치는데 단순 작업 시간이 들긴 했지만..   또, 성능을 위해서 반정규화 등의 방법을 사용하기도 한다는 것을 알게 되었다. 사실 난 이전까지 비즈니스 모델이 정해지면 그에 대한 디비 모델은 정답이 하나로 정해져있다고 생각했는데 현업에서는 그렇지 않을 수도 있겠다는 생각을 할 수 있었다.   Clean code   전반적으로 clean code를 컨셉으로 한 스프린트였던 것 같다. 이를 위한 교육도 들었는데 개인적으로 좋은 코드를 읽고 배우는 것을 좋아하기 때문에 교육 자체도 흥미롭게 들었다. 그리고 이사님께서 우리 팀에 대한 코드리뷰를 진행해주셨는데 생각보다 세세하게 코드리뷰를 해주셔서 놀라웠다. Spring의 annotation에 대한 리뷰부터 java String concatenation에 대한 메모리 이슈 등 정말 꼼꼼히 봐주셔서 놀랐다. 제한된 시간에 코드 리뷰를 하다보니 로그인과 예약 부분에 대한 코드리뷰만 진행해도 시간이 부족했고 그래서 내가 작성한 코드(검색 기능)에 대한 리뷰를 받지 못했는데 그게 정말 아쉬웠다. 사실 나보다 경험이 훨씬 많으신 분이 내 코드를 리뷰해준다는 것은 정말 소중한 기회이기 때문이다.   CLI controller 구현   웹 client로 들어오는 요청말고도 CLI로 들어오는 요청을 처리하는 컨트롤러도 구현했다. 원래 스프링으로 웹 client에만 대응할 수 있는 줄 알았는데 cli로도 할 수 있다는 걸 처음 알았다.   Controller는 서버의 문지기   CLI controller를 구현하면서 자연스럽게 컨트롤러의 목적도 알게 되었고, 그동안 불명확했던 컨트롤러와 서비스의 경계선에 대해서도 이해하게 되었다. 내가 이해하기에 컨트롤러는 서버의 문지기 역할인 것 같다. 서버에 들어오는 모든 요청을 받아내는 최전선이면서 잘못된 요청을 거르고, 제대로 된 요청에 대해서는 각 요청에 맞는 서비스로 보내주는 역할이라고 생각한다. 그렇게 보낸 요청에 대해 서비스가 결과를 내주면 그 결과를 다시 client에게 보내주는 역할을 하는 것이라고 이해했다. 그러면서 자연스럽게 서비스 layer의 역할에 대해서도 이해하게 되었는데, 결국 다양한 종류의 요청에 대해 어떤 식으로(어떤 로직을 이용하여) 답을 줄지는 서비스 layer에서 결정하고 구현하는 것이다. 즉, 비즈니스 로직은 모두 서비스 layer에서 구현되어야 하는 것이다.   개인적으로 구조화된 코드를 작성하는 것을 좋아하고, 또 그것이 좋은 코드라고 알고 있다. 좋은 코드의 기준은 여러가지가 있지만 여기서 좋은 코드라함은 각 코드의 기능이 독립적으로 분리되어 있어 수정 및 유지 보수가 용이한 코드를 말한 것이다. 그래서 지난 주에 개발을 진행할 때도 구조화된 코드를 작성하기 위해(각 layer의 역할 분리가 확실하고 독립적인) 많은 노력을 기울였는데 잘 된 것인지는 잘 모르겠다. 앞으로 유지 보수 및 개선을 위해 코드를 관리하다 보면 알게 될 것 같다.  ","categories": ["Basecamp"],
        "tags": ["Basecamp","Sprint6"],
        "url": "https://dreamsh19.github.io/basecamp/%EB%B2%A0%EC%9D%B4%EC%8A%A4%EC%BA%A0%ED%94%84-Sprint6/",
        "teaser": null
      },{
        "title": "베이스캠프 Sprint7",
        "excerpt":"21.03.02부터 21.03.05까지 베이스캠프 Sprint7을 진행했다.   웹 서버 스케일 아웃을 컨셉으로 한 Sprint였다. 사실 Sprint 6까지는 한 대의 서버로 모든 것을 처리하던 웹 서비스였다. 그리고 입사 전 과거 나의 웹 개발 경험은 딱 Sprint 6까지 였던 것 같다. 웹 서비스를 위해 코드를 작성하고 배포하고 리팩토링하는 정도였다. 당연히 서버도 한 대였다. 만들었던 서비스가 스케일 아웃을 할만큼의 트래픽을 받아본 적도 없었고, 그렇기 때문에 Sprint 7부터는 그동안의 경험과는 다른, 정말 현업에 가깝고, 또 현업이 아니면 겪을 수 없는 경험을 할 수 있었다. 그리고 Sprint 7을 시작할 때 스케일 아웃에 대해 얼마나 무지했냐면 스케일 아웃과 스케일 업의 차이조차 모르고 있었다. 관련하여 블로그에 차이점을 정리하였다. Scale Out vs Scale Up   베이스캠프 Sprint7에서는 확장에 있어서 고가용성의 목적도 충족하기 위해 스케일 아웃을 채택하였다.   세션 구현   웹 서버를 두 대로 늘리면서 발생한 가장 첫 번째 문제는 메모리를 공유하지 않는다는 점이었다. 기존에 서버가 한 대일때는 해당 서버의 메모리에 담아두고 써도 문제가 되지 않았다. 그런데 두 대 이상의 서버로 운영 시 클라이언트 요청이 A서버로 들어와서 A 메모리에 연산결과를 담아두고 다음 요청이 B서버로 들어오는 경우 A의 연산결과를 알 방법이 없다. 대표적으로 문제가 되는 부분이 세션이었다. 기존의 세션은 모두 메모리에 저장되는 구조였기 때문이다. 따라서 두 서버가 공유하는 메모리, 즉 공유 스토리지 역할을 하는 무언가가 필요했고, 그것을 Redis와 DB를 이용해서 각각 구현하는 과제가 주어졌다. 각각을 구현하기 전에 구조를 설계하는 회의를 진행했는데 거기서 spring profile을 이용하여 profile에 따라 모듈을 갈아끼는 구조를 제시했고, 실제 구현에 채택되었다. 특히 나름 객체지향 원칙을 준수한 구조라고 생각해서 뿌듯했다.   제시한 구조는 공통 인터페이스인 SessionFactory를 작성하고 이를 implement하는 구현 클래스인 RedisSessionFactory와 DbSessionFactory를 작성하여 profile이 redis-session일때는 SessionFactory로 RedisSessionFactory를 Bean으로 등록하고, profile이 db-session일때는 DbSessionFactory를 Bean으로 등록하는 방식이다.   이를 위한 구조를 만들기 위해 spring profile에 대해서도 공부할 수 있었다.   그리고 redis를 이용한 세션 관리는 현업에서도 많이 쓰는 걸로 알고 있는데, redis 관련 지식이나 경험이 전무했기 때문에 직접 구현해보고 싶었지만 아쉽게도 다른 팀원이 맡게 되었다.   L7 헬스체크   스케일 아웃을 위해 필수적인 요소가 바로 로드밸런서이고, 로드밸런서가 각 서버에 요청을 분배하기 위해서는 각 서버의 상태, 즉 요청을 받을 수 있는 상태인지 체크해야한다. 그걸 바로 헬스체크라고 하는데, 이를 application level에서 하는 걸 L7 헬스체크라고 한다. 모니터링 url을 지정해놓고(ex. /monitor/l7check) API 요청을 보내는 방식이다. 서버가 죽으면 당연히 로드밸런서에서 요청을 할당을 안하겠지만 서버가 살아있는데도 서버가 요청을 받지 않도록 하고 싶을 때(예를 들어 무중단 배포) 활용할 수 있다.   L7 health check를 위해 spring actuator의 HealthIndicator를 활용했다. API GET 요청을 통해 서버의 up/down 여부를 status code로 반환했다. Up일때는 OK(200), down일 때는 Service Unavailable(503). 그리고 후에 무중단 배포 시의 활용을 고려하여 PUT 요청을 통해 up/down 상태를 변경할 수 있도록 API를 구현했다.   여기서 한계점은 ACL 체크를 하지 않는다는 점이다. 어느 호스트에서나 down API 요청을 보내면 서버의 상태를 down으로 바꿀 수 있기 때문에 위험한 구조이다. (각 서버의 RIP를 알고 있다면 누구나 서버의 상태를 down으로 바꿔서 서비스 장애를 유발시킬 수 있다). ACL을 추가하는 작업은 추후에 필요해보인다.   Entity 변경   Sprint 6에서 DB 모델링 검수 결과에 따라 DB 모델에 변화가 생기면서 JPA Entity에도 코드 변경이 필요했다. 이 부분은 충돌 방지를 위해 한 사람이 맡는게 좋을 것 같아 내가 진행했다. 다행히 테이블이나 관계의 수정은 없고 테이블명이나 칼럼명의 수정 정도였지만, 그래도 코드를 일괄적으로 바꾸는 건 꽤나 섬세함이 필요한 작업이었다. 이때 일괄수정하면서 유용하게 활용한 기능이 있다.      Intellij refactor 기능            함수명, 변수명을 바꾸면 이를 참조하는 모든 곳에서의 이름도 바꿔준다.           Intellij replacement시 preserve case 옵션            replace 시 대소문자를 보존해준다       예를 들어 해당 옵션을 켜고 airportFromTo -&gt; flightPath을 하면                    airportFromTo -&gt; flightPath           AirportFromTo -&gt; FlightPath                       로 알아서 바꿔준다.           그 외   그 외에 다른 팀원들이 스케일 아웃을 위해 한 작업들은 아래와 같다.      NHN Cloud의 Object Storage 연동   NHN Cloud Deploy로 배포 설정            사실 처음에는 Deploy 기능이 왜 필요한지 몰랐다. 그런데 서버가 100대가 된다고 생각하면 그걸 하나씩 들어가서 배포하는 건 정말 귀찮을 뿐만 아니라 실수로 인한 장애의 가능성도 농후하다는 것을 깨달았다.           ","categories": ["Basecamp"],
        "tags": ["Basecamp","Sprint7","Scale out"],
        "url": "https://dreamsh19.github.io/basecamp/%EB%B2%A0%EC%9D%B4%EC%8A%A4%EC%BA%A0%ED%94%84-Sprint7/",
        "teaser": null
      },{
        "title": "Frontend 코드리뷰 정리",
        "excerpt":"지난 금요일에 NHN FE개발랩 박정환 전임님의 루키사자TF 프론트엔드 코드리뷰가 있었다. 이번 베이스캠프에서는 모두 바닐라 JS로 프론트엔드를 구현했고, 그렇기 때문에 대부분 비슷한 실수를 했을 것이라 예상되어 해당 코드리뷰를 모든 TF가 참관하는 식으로 진행을 했다.   이 글은 코드리뷰에서 말씀해주신 내용을 요약 및 정리한 글이다.      실제로 서비스할때는 외부 CDN(폰트나 이미지, 라이브러리 등으로 많이 활용) 안 쓰는게 좋다            외부 CDN 장애 시 우리 서비스에도 영향이 있기 때문에       -&gt; 스태틱 리소스로 다운받아서 사용 권장                html은 동작(기능) 없이. 구조(모양, 형태)만 설계            기능이나 동작, 로직은 js로            css도 중복되는 부분은 새로운 css로 모듈화하자            https://www.caniuse.com : 해당 기능이 어떤 브라우저에서 지원하는지 확인 가능            정적 분석에 걸리는 실수들은 정말 불필요하다. 기계적으로 해결가능하다                       ES Lint나 prettier 등을 활용하여 자동으로 조정 가능                        Pre-commit hook으로 등록하는 식으로 활용 가능                        var는 쓰지마라. const, let을 쓰자              처음에 싹 다 const로 쓰고 바꿔야하는 값이라면 그때 let으로 바꾸는 방법도 추천                String을 합칠 때 + 보다 template literal(백틱 ``)을 써라            for i 대신 무조건 foreach써라.              for i는 동적으로 컬렉션이 바뀌는 경우에 버그 발생 가능       Foreach에서도 인덱스 접근 가능하다.                DOM에 변경이 생기는 애들은 함수로 만드는게 관리 측면에서 용이(ex. changeText())            선언부와 호출하는 부분을 분리하는 것이 좋다.              var, function으로 선언된 애들은 호이스팅 발생       Const, let은 호이스팅 안함 =&gt; 이걸로 쓰는게 가독성 측면에서 좋다                함수 선언식보다는 함수 표현식을 써라            Eventlistener는 add해주고 나면 remove해주는게 memory leak을 방지.              vanilla js의 경우 페이지 떠날때 모두 remove해주면 된다.       요새는 프레임워크를 써서 상관없긴하다.                Js 실행시 DOM 탐색이 가장 비싼 연산이다.       =&gt; DOM element 객체는 변수로 저장해놓는게 좋다.(다시 찾지 않도록)            작성한 코드의 ECMA 버전 맞추기       전역에서 바로 실행하지 말고 IIFE(즉시실행함수) 사용하자.  ","categories": ["Frontend"],
        "tags": ["Frontend","Code Review","Vanilla JS","Basecamp"],
        "url": "https://dreamsh19.github.io/frontend/Frontend-%EC%BD%94%EB%93%9C%EB%A6%AC%EB%B7%B0-%EC%A0%95%EB%A6%AC/",
        "teaser": null
      },{
        "title": "베이스캠프 Sprint8",
        "excerpt":"21.03.08부터 21.03.12까지 베이스캠프 Sprint8을 진행했다. 전 Sprint에서 웹 서버 Scale out을 진행했다면, 이번 Sprint에서는 스케일 아웃된 웹 서버에 DB 샤딩까지 진행해보는 것이 목표였다. DB가 하나여서 병목이 되어버리면 웹 서버를 스케일 아웃한 의미가 없어지게 된다.   쿼리 검수   DB 샤딩에 앞서 SQL이론 교육 및 쿼리 검수를 받았다. 전반적으로 ‘좋은 쿼리’에 대한 교육이었던 것 같다.   SQL이론 교육은 정말 흥미로운 부분이 많았다. 단순히 SQL문법에 관한 내용은 아니었고, 각 SQL문이 실제로 어떤 식으로 작동하고, 그렇기 때문에 지양해야하는 SQL문 등에 대해서 배울 수 있었다. 책으로만 배우던 SQL과 전혀 다른 부분이었고 현업에서도 성능을 위해 꼭 알아야할만큼 유용한 내용들이 많았다. 특히 DB 상에서 성능 상의 이슈로 물리적인 Foreign key를 사용하지 않고, 이로 인한 참조 무결성 관리는 애플리케이션, 즉 개발자가 주로 한다는 사실은 전혀 몰랐던 것이기에 놀라웠다. 그리고 물리 FK를 모두 제거할 생각에 Entity 클래스나 메소드를 다 뜯어고쳐야 하는 건가 싶었지만 팀원이 물리 FK만 생성하지 않는 옵션을 추가할 수 있는 방법을 찾아내서 다행히 로직 코드를 고칠 일은 없었다.   쿼리 검수 결과는 대부분의 TF에서 비슷한 검수 결과를 받았다. 이전에 검수를 원하는 쿼리 두개를 선정해서 검수 요청을 드렸고, 그 중에 하나는 내가 작성한 항공편 검색 조건 조회 쿼리였다. 지적해주신 문제점 중에 내가 고쳐야할 부분은 크게 두가지였다.      select문에서 중복되는 칼럼 제외   group by 사용시 select문에서 가져오는 칼럼 모두 명시   1번은 사실 ORM을 쓰다보니 불가피한 부분이었는데(Entity를 통째로 가져오기 때문에) 해당 검수 결과를 바탕으로 쿼리를 고치다 보니 Entity의 칼럼을 하나하나 지정해주면서 ORM을 쓰는 의의가 무색해지는(?) 기존에 비해 지저분한 코드가 되어버렸다. ORM의 여러 장점 중 타입 안정성 정도만 지켜지는 코드라고 느껴졌는데 성능을 위해 이부분은 포기해야하는 건가 싶었다.   2번은 결론부터 말하자면 내가 문법을 잘못 알고 있었던 것이다. 이전까지 group by에는 해당 tuple을 구분짓는 칼럼들만 작성해도 되는 것으로 알고 있었고(예를 들어 select * ... group by PK 와 같은..), 검수 결과를 알려주셨을 때도 어떤 차이가 있는지 잘 이해가 가지 않았다. 그래서 교육을 진행해주신 남준희 선임님께 다시 질문을 드렸고, 명쾌한 답을 들을 수 있었다.      수행이 가능하다곤 하지만 문법상 select절에는 group by절에 명시된 컬럼을 다 작성해주셔야 됩니다. MySQL의 경우 위와 같이 실행하면 문제 없이 수행되지만 타 DBMS의 경우 오류 발생합니다.    애초에 잘못된 문법으로 알고 있었고, MySQL에서만 오류없이 수행되는 쿼리였다.   이 교육의 내용은 정리해서 이 블로그에 남겨놓는 게 좋을 것 같다.   샤딩   계정서버 디비 샤딩을 진행했다. 테이블이 회원 테이블 하나인 계정서버 DB에 대해 진행했다. 태스크 분배 시에 이 부분은 내가 맡아서 개발하고 싶다고 어필하여 주도적으로 개발하게 되었다. 샤딩에 대한 부분은 이전부터 확실히 알고 넘어가고 싶었기 때문이다.   처음에 참고할만한 블로그들을 찾아봤는데 Spring AOP, Spring Expression부터 시작해서 java annotation 만들기, AbstractDataSourceRouter 등 모르는 개념 투성이었고, 추상화 레벨이 높아서 코드를 이해하는 것 조차 쉽지 않았다. 그래서 코드의 구조와 내용을 이해하는데만 하루를 투자했다. 결국 위의 개념들을 모두 활용하여 샤딩 구현을 해냈다. 베이스캠프 기간에 작성한 코드 중에서 가장 고심을 많이하면서 코드를 작성했고, 여러가지 트러블 슈팅을 거쳐서 코드가 탄생했는데, 트러블 슈팅 과정들도 정리해서 이 블로그에 남겨놓는게 좋을 것 같다.   로그인 인터페이스 통합   모든 TF의 로그인 인터페이스를 통합하는 과제였다. 이 부분도 나에게 굉장히 흥미로웠다. 계정 서비스를 통합하기 위해서는 각 TF별로 외부에서 로그인이 가능하도록 API를 제공해야함을 의미하고, 다른 TF와 스펙에 대한 합의가 필요한 부분이었다. 그동안은 우리 TF에서 작성한 API는 우리 TF 서비스에서만 사용했기 때문에 API 내부 로직에 오류가 있어도 우리끼리 해결하면 되는 문제였지만, 이제는 다른 서비스에도 영향을 줄 수 있는 부분이 생긴 것이다.   어쨋든 API 스펙 정의를 위해 모든 TF에서 대표 인원이 참가하여 회의를 진행했다. 나도 회의에 참석해서 적극적으로 의견을 개진하는 경험을 할 수 있었고, 결과적으로 깔끔한 API 스펙이 정해져서 다른 팀원이 해당 API 구현을 맡아서 개발했다. 일반적으로 현업에서 서비스를 개발할 때 API 설계 시 부서간 또는 외부와 협업하는 방식을 배울 수 있는 기회였다.   그 외   이 외에도 내가 주도적으로 개발하지 않았지만 아래와 같은 태스크를 진행했다.           Security Key Manager       기존에 프로젝트 파일 내에 무방비하게 기록되어 있던 민감 정보(db 계정 및 비밀번호 등)를 없애기 위한 작업이었다. SKM을 활용하면 발급받은 일종의 토큰을 이용하여 API 요청하면 해당 정보를 받아오는 구조로 구현할 수 있고, 프로젝트 파일 내에 정보가 노출되는 걸 막을 수 있었다.            스케일 아웃 코드리뷰 반영       사실 다른 팀원들은 이 부분 때문에 고생을 많이했다. Sprint7의 스케일 아웃 시 구현한 Redis 세션과 DB Session에 대해서 우리 TF 전체가 세션 구조에 대해 아예 잘못 이해하고 있었던지라 전면 수정이 필요했기 때문이다.      ","categories": ["Basecamp"],
        "tags": ["Basecamp","Sprint8","Sharding","Cell Architecture"],
        "url": "https://dreamsh19.github.io/basecamp/%EB%B2%A0%EC%9D%B4%EC%8A%A4%EC%BA%A0%ED%94%84-Sprint8/",
        "teaser": null
      },{
        "title": "베이스캠프 Sprint9",
        "excerpt":"21.03.15부터 21.03.21까지 베이스캠프 Sprint9를 진행했다. 크게 두 가지의 컨셉을 가진 Sprint였는데 두가지는 바로      QA   운영   이다. 실질적인 개발이 완료된 서비스에 대해 출시를 위한 준비를 하는 Sprint라고 볼 수 있을 것 같다.   QA   일종의 알파테스트 느낌으로 다른 TF와 크로스 QA를 진행하기도 하고 우윤정 수석님이 추가적으로 QA를 진행해주셨다. 당연하게도 많은 버그나 개선사항들이 발견되었고, 대부분 마이너한 것들이어서 그 양은 많았지만 수정하는데는 오래걸리지 않은 것 같다.   기억 남는 버그가 두 개가 있는데 중 하나는 브라우저의 뒤로 가기 행동 시 버그가 있었는데, 관련해서 브라우저의 BFCache(Back-forward Cache)에 대해 몰랐었기에 버그 픽스하는데 시간을 많이 들였던 것 같다. 나머지 하나는 QA 제보를 통해 알게 된 건 아니고 혼자 테스트하다가 왕복 검색 시 가는 편 선택 후 오는 편 선택 페이지로 넘어갈 때 가끔 500 Error가 발생하는 걸 발견했다. 해당 버그는 redirectAttribute의 addFlashAttribute()으로 인한 버그였는데 개인적으로 흥미로운 트러블 슈팅 과정이라 블로그에 남길 예정이다.   QA 내용에 대해 하나씩 버그픽스해내가며 태스크를 완료처리하는 재미가 있었다. 그 중에서도 한가지 아쉬웠던 점은 개선사항으로 제시된 것 중 하나가 왕복 항공편 선택시 가는 편과 오는 편을 계속 바꿔가며 비교하기에 굉장히 불편한 구조라는 것이다. 그런데 이부분은 나도 알고 있었고, 이러한 불편함을 예상해서 기획 단계에서도 검색 결과 화면 상단 헤더에 재검색할 수 있는 모듈을 넣자고 기획했었다. 그러나 개발단계에서 시간 상 스펙아웃했던 부분인데 그 부분이 뼈아프게 돌아왔다.   운영   서비스 출시 이후에 안정적인 서비스 운영을 위한 커리큘럼이 진행되었다. 서비스 오픈은 끝이 아니라 시작이라는 말씀이 기억에 남는다. 직접 경험하기에 앞서 이경한 수석의 교육이 있었는데, 정말 감명을 많이 받았다. 개인적으로 베이스캠프 기간 중에 들었던 많은 교육 중에 가장 인상 깊었던 것 같다. 교육은 대부분 이경환 수석님의 지난 수년간의 운영 경험에 대한 공유가 주 내용이었다.   교육을 듣고 나니 서비스 운영을 위해서는 애플리케이션에 대해서만 알아야하는 게 아니라는 걸 깨달았다. 정말 예상치도 못한 다양한 곳에서 장애가 발생할 수 있다는 걸 알게 되었다. 캐시로 인한 장애부터 시작해서 심지어는 파일 시스템의 inode개수로 인한 장애까지.. 정말 예측은 고사하고 장애 원인을 찾아낸 게 더 놀라운 수준이었다.  그 장애를 해결하기 위해서는 당연하게도 그 다양한 부분에 대한 지식이나 경험이 있어야하며, 대응에도 역시 노하우가 필요함을 깨달았다. 그리고 그 다양한 지식과 경험을 갖추고 공유해주시는 이경환 수석님이 정말 대단하다고 느껴졌다.   교육이 끝나고는 직접 운영을 위한 실습을 진행했다. 크게 아래와 같은 하위 태스크로 나누어졌고, 난 그중에서 1,2번을 맡아서 진행했다.      Log &amp; Crash 적용   Watchdog 연동   운영 통계 스크립트 작성   무중단 배포   자동 재기동(crontab 이용)   Nsight 연동   Log &amp; Crash   Log &amp; Crash는 서버의 로그를 수집하여 검색 및 정렬 기능을 제공하는 클라우드 서비스이다. 기존에 서버에 직접 하나씩 들어가서 로그를 들여다보는 방식보다 훨씬 수월한 것이다. 이걸 우리 프로젝트에 적용해야했는데, 이를 위한 설정을 하는 과정에서 logger에 대한 이해를 높일 수 있었다. 간단할 줄 알았는데 logger에 대한 이해가 부족해서 생각보단 오래걸렸다. 사실 그동안 Log4j2니, logback이니, Slf4j니 다 똑같은 logger인 줄 알았는데 그게 아니었다. Slf4j는 일종의 인터페이스이고 Log4j2와 logback이 그 구현체인 것 같았다. 그리고 Log &amp; Crash를 위해서는 logback을 위한 설정을 해주어야했는데, 그 과정에서 logback.xml 작성 시 console과 logncrash 두가지 appender를 설정하는 법을 알 수 있었다. 그리고 spring profile(dev/prod)에 따라 로깅을 따로하기 위해 spring profile을 가져오는 방식으로 구성했는데, 그 과정에서 spring property를 logback.xml에서 쓰는 방법을 알 수 있었다.   Watchdog   Watchdog은 애플리케이션이 잘 작동하고 있는지 모니터링해주는 것이었다. VIP와 RIP(사실 VIP와 RIP가 Virtual IP, Real IP라는 걸 이때 처음 알았다.) 각각에 대한 헬스체크 및 API 체크를 추가해주었고, 배포 시 watchdog 전파 중지 설정까지 해주었다. 모니터링과는 큰 관련이 없지만 여기서 한가지 기억에 남는 것은 XPath(XML Path)라는 걸 처음 알았다. XPath란 DOM을 탐색하듯이 XML을 탐색하기 위한 언어 느낌이었다. 모니터링을 위해 메인 페이지 로딩 시 form 태그가 뜨는지 확인하기 위해 XPath를 활용하여 체크를 했는데 이런게 있다는 걸 몰랐다는 사실 자체가 정말 개발의 세계는 넓다는 걸 새삼 느끼게했다.   그리고 새벽에 watchdog 전파가 되는 대참사가 발생했는데, 전파가 잘돼서 watchdog 설정이 잘된 것은 확인했지만(?) 정말 너무 아찔했다. 전파 그룹을 분리할 필요 또한 절실히 느꼈다.   그 외   이 외에도 다른 팀원들이 진행한 무중단 배포, crontab을 이용한 자동 재기동과 같은 것들도 현업에서 정말 많이 사용할 것 같은 기술들이었다. 현업에 가서도 운영을 분명히 하게 될텐데 이러한 경험들이 정말 많은 도움이 될 것 같다.   ","categories": ["Basecamp"],
        "tags": ["Basecamp","Sprint9","QA","DevOps"],
        "url": "https://dreamsh19.github.io/basecamp/%EB%B2%A0%EC%9D%B4%EC%8A%A4%EC%BA%A0%ED%94%84-Sprint9/",
        "teaser": null
      },{
        "title": "Scale Out과 Scale Up",
        "excerpt":"서버 확장을 위한 방법에는 크게 두가지가 있는데 바로 Scale out과 Scale up이 그것이다. 한 대의 서버에서 감당할 수 있는 부하를 감당할 수 있도록 하는 것이 두 방법의 공통된 목표이다. 한 사람에게 주어지는 일이 많아서 감당이 안되면 이에 대한 대책으로 두 가지를 생각할 수 있다.      일을 하는 사람을 여럿으로 늘리는 것(Scale Out)   일을 더 잘하는 사람으로 교체하는 것(Scale Up)   이 바로 그것이다.   기존의 서버가 1GHz CPU, 2GB 메모리였다면,      스케일 아웃은 1GHz CPU, 2GB 메모리를 가진 서버를 한 대 더 붙이는 방법   스케일 업은 기존의 서버를 2GHz CPU, 4GB 메모리로 업그레이드하는 것   (수치가 2배인 것은 예시일뿐 꼭 2배일 필요는 없다)      당연하게도 두가지 방법 모두 장단점이 있기 때문에 목적에 따라 적합한 방법을 사용한다. 베이스캠프 Sprint에서는 스케일 아웃 방법을 선택했다. (고가용성의 목적도 충족하기 위해서) 아래는 내가 나름대로 정리한 각 방법의 특징 및 장단점이다. 위에 사람에 비유한 것을 떠올리면 쉽게 이해가 되었다. 비슷한 능력의 사람을 여럿 고용할 때와 뛰어난 능력의 사람을 한 명 고용할 때의 상황과 비슷하다고 느껴졌다.                          Scale Out       Scale Up                       확장성       - 수평 확장 - 지속적 확장이 가능하다(1대-&gt;2대-&gt;10대-&gt;…)       - 수직 확장 - 성능 확장에 한계가 있다.(물리 장비의 기술적 한계가 있다.)                 장애 발생 시       - 서버 하나가 죽어도 서비스 장애는 일어나지 않음 - 고가용성       - SPOF(Single Point of Failure) - 이 서버가 죽으면 서비스 전체 장애                 관리 및 운영       - 생각할게 많아짐 = 어려움 (멀티스레드 환경에서의 어려움을 생각하면 될듯)  - 당장 로드밸런서부터 필요 - 메모리를 공유하지 않기 때문에 공유 스토리지가 필요 ex) Redis 등       - 확장에 따른 추가 관리 및 운영 필요 없음                 활용       웹 서버 확장, 분산처리 시스템       DB 서버           References      https://m.blog.naver.com/islove8587/220548900044   https://idchowto.com/?p=29915  ","categories": ["Server"],
        "tags": ["Server","Scale out","Scale up"],
        "url": "https://dreamsh19.github.io/server/Scale-Out%EA%B3%BC-Scale-Up/",
        "teaser": null
      },{
        "title": "Mac에서 스크린샷 바탕화면만 될때",
        "excerpt":"이슈   새로 산 로지텍 M590 마우스를 맥북에서 쓰기 위해 logi Options (로지텍 마우스의 버튼 커스터마이징 및 logitech flow 등의 설정을 하게 해주는 프로그램. 제공하는 기능에 대한 자세한 사항은 링크 참조)를 이용해 버튼을 커스터마이징하던 중 스크린샷도 가능하다는 걸 깨달아서 적용하려고 했다. 개인적으로 맥북의 스크린샷 단축키가 Cmd+Shift+3(또는 4)와 같은 식이어서 단축키가 직관적이지 않고, 심지어 클립보드에 복사하려면 Control까지 총 4개의 손가락을 이용해야해서 불편하다고 생각하고 있었다. 그래서 logi Options에서 버튼 하나에 스크린샷을 지정하고 실행을 해보니 스크린샷은 잘되는데 이런식으로 바탕화면만 캡처가 되었다.      원인   당연히 logi Options의 버그라고 생각하고 구글링을 했지만 아무리 찾아도 안나왔다. 그러던 중 logi Options가 아니라 에버노트에서 캡처했을 때 위와 같이 바탕화면만 캡처가 된다는 글을 발견했고, 해결법도 그곳에서 찾았다. 원인은 바로 애플리케이션의 화면 기록 권한이 없었기 때문이다.   Mac은 스크린샷을 찍을 때 화면 기록 권한을 체크한다. 우리가 단축키나 스크린샷 앱으로 스크린샷을 찍을 때는 Mac 자체에서 화면 기록을 하는것이니 의도대로 스크린샷이 된다. 그러나 애플리케이션(나의 경우 logi Options)에서 스크린샷을 찍기 위해서는 화면 기록을 위한 권한이 필요하고 그 권한이 없으면 위 사진처럼 아무것도 없는 바탕화면만 캡처가 된다.   해결   스크린샷을 허용하려는 애플리케이션의 화면 기록 권한을 허용해주면 된다.   Mac의 시스템 환경설정 - 보안 및 개인 정보 보호에 들어가서   개인 정보 보호 탭 - 화면 기록에 들어가 원하는 애플리케이션에 체크표시를 해준다.      다시 해보니 최초 시도 시에 애플리케이션의 화면 기록을 허용하겠냐는 문구를 띄워주는데 왜 처음엔 그 문구가 안떴는지 모르겠다.   추가   보통 스크린샷은 찍은 후 한번 활용하고 더 이상 활용하지 않기 마련이다. 애플 단축어 예시 중에 괜히 Share screenshot and delete가 있는게 아니다. 그런데 Mac은 스크린샷을 무조건 파일로 저장해서 번거로웠다. 그래서 Control키까지 같이 누르면 클립보드에만 저장이 된다는 걸 알아내서 그렇게 쓰고 있었는데 이것도 손가락을 4개나 써야해서 여전히 불편했다. 그런데 이번에 새로운 걸 알게되었다.   Mac 스크린샷 설정에서 스크린샷을 찍었을 때 저장할 위치를 지정할 수 있다. 그런데 그 위치 중에 클립보드도 가능하기 때문에 캡처해서 바로 붙여넣기만으로 캡처본을 활용할 수 있다. Spotlight에 스크린샷을 치면 해당 앱을 들어갈 수 있고, 거기서 옵션에서 저장 위치를 지정할 수 있다. 사실 위에 기술한 화면 기록 권한에 대한 내용을 안 것보다 스크린샷을 바로 클립보드에 저장해주는 이 기능을 알게 된 게 훨씬 유용한 것 같다.      References      https://discussion.evernote.com/forums/topic/122038-screen-snap-capture-not-working/   ","categories": ["Mac"],
        "tags": ["Mac","ScreenShot","Logi Options"],
        "url": "https://dreamsh19.github.io/mac/Mac%EC%97%90%EC%84%9C-%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7-%EB%B0%94%ED%83%95%ED%99%94%EB%A9%B4%EB%A7%8C-%EB%90%A0%EB%95%8C/",
        "teaser": null
      },{
        "title": "Go 기초",
        "excerpt":"Package           package main에서 시작            대문자로 시작해야 exported.   Import 시 exported name만 접근 가능.       package main    import ( \t\"fmt\" \t\"math\" )    func main() { \tfmt.Println(math.Pi) // 3.141592653589793 \tfmt.Println(math.pi) // compile error(cannot refer to unexported name math.pi) }            Function           타입이 변수 이름 뒤에       func add(x int, y int) int { \treturn x + y }    // 위와 같음 func add(x, y int) int { // 연속적인 경우 마지막에만 명시 가능 \treturn x + y }                여러 개 리턴 가능       func swap(x, y string) (string, string) { \treturn y, x }\t                naked return : 가독성 떨어지므로 짧은 함수에만 활용 권장       func split(sum int) (x, y int) { \tx = sum * 4 / 9 \ty = sum - x \treturn }    // 같은 표현 func split(sum int) (int, int){ \tx := sum * 4 / 9 \ty := sum - x     return x, y }           Variable           선언은 var 변수명 type       func main(){ \tvar i, j int = 1, 2 \tvar c, python, java = true, false, \"no!\"     // initializer와 함께 사용 시 type 생략 가능 \tk:= 3 // var k = 3과 같은 표현. 그러나 함수 안에서만 사용 가능     \t     // 명시적으로 초기화 안했을 경우 \"Zero\" value로 초기화     // Zero value = 0(숫자 타입), false(boolean 타입), \"\"(string 타입), nil(pointer 타입)     var j int \tvar f float64 \tvar b bool \tvar s string \tfmt.Printf(\"%v %v %v %q\\n\", j, f, b, s) // 0 0 false \"\" }    k:=3 // syntax error: non-declaration statement outside function body                   Basic types       bool string int  int8  int16  int32  int64 uint uint8 uint16 uint32 uint64 uintptr  // int, uint, uintptr은 32-bit 시스템에서는 32 bit, 64-bit system에서는 64 bit byte // alias for uint8 rune // alias for int32. represents a Unicode code point float32 float64 complex64 complex128                Type casting : 반드시 명시적으로 표기       var x, y int = 3, 4 var f float64 = math.Sqrt(float64(x*x + y*y)) var z uint = uint(f) var z uint = f // cannot use f (type float64) as type uint in assignment    // 명시 안 한 경우는 right side와 같은 타입 func main() { \ta := 42 \tb := 42.1 \tc := 42+5i \tfmt.Printf(\"Types are %T %T %T\\n\", a, b, c) // Types are int float64 complex128 }                const       const Pi = 3.14 func main() { \tconst World = \"世界\" \t// const 변수는 := 로 선언 불가능 \tconst i := 1 // syntax error: unexpected :=, expecting = }           흐름제어   For           유일한 반복문 (while 없음)            for문 () 없고 {} 있음       package main    import \"fmt\"    func main() { \tsum := 0 \tfor i := 0; i &lt; 10; i++ { \t\tsum += i \t}      \tsum := 1 \tfor sum &lt; 1000 { // ; 제거시 while처럼 사용 가능 \t\tsum += sum \t}      \tfor { // 무한 루프 \t} }           If      마찬가지로 () 없고 {} 있음   func sqrt(x float64) string { \tif x &lt; 0 { \t\treturn sqrt(-x) + \"i\" \t} \treturn fmt.Sprint(math.Sqrt(x)) }  func pow(x, n, lim float64) float64 {   \t// if문 내 scope를 갖는 변수 선언 및 할당 가능. ;으로 구분. 마지막에 조건문 \tif v := math.Pow(x, n); v &lt; lim { \t\treturn v \t} else { \t\tfmt.Printf(\"%g &gt;= %g\\n\", v, lim) \t}   \t// return v (undefined : v) \treturn lim }   Switch      break문 없음. 무조건 조건 여러 개 중 하나만 실행(다른 언어는 break 없으면 다 돈다)            조건 맞는거 만나면 실행하고 switch문 빠져나옴           정수일 필요 없음   case가 상수일 필요도 없음   조건이 없어도 됨   package main  import ( \t\"fmt\" \t\"runtime\" )  func main() { \tfmt.Print(\"Go runs on \") \tswitch os := runtime.GOOS; os { \tcase \"darwin\": \t\tfmt.Println(\"OS X.\") \tcase \"linux\": \t\tfmt.Println(\"Linux.\") \tdefault: \t\tfmt.Printf(\"%s.\\n\", os) \t}      \tfmt.Println(\"When's Saturday?\") \ttoday := time.Now().Weekday() \tswitch time.Saturday { \tcase today + 0: // 상수일 필요 없다. \t\tfmt.Println(\"Today.\") \tcase today + 1: \t\tfmt.Println(\"Tomorrow.\") \tcase today + 2: \t\tfmt.Println(\"In two days.\") \tdefault: \t\tfmt.Println(\"Too far away.\") \t}      \t// 긴 if-else문의 연속을 switch로 깔끔하게 쓸 수 있다.    \tt := time.Now() \tswitch { \tcase t.Hour() &lt; 12: \t\tfmt.Println(\"Good morning!\") \tcase t.Hour() &lt; 17: \t\tfmt.Println(\"Good afternoon.\") \tdefault: \t\tfmt.Println(\"Good evening.\") \t} }    Defer      자신이 속한 함수의 return 이후 실행되도록(java finally와 유사)   함수 argument는 바로 넣지만 실행만 미루는 것   이때 stack 방식으로 쌓아놓는다. LIFO   활용 예 : 다른 함수, 모듈이 일을 다하고 마지막에 clean-up할때 활용 가능   참고 https://blog.golang.org/defer-panic-and-recover   func main() { \tfmt.Println(\"counting\")  \tfor i := 0; i &lt; 10; i++ { \t\tdefer fmt.Print(i) \t}  \tfmt.Println(\"done\") } /*  실행 결과 counting done 9876543210 */  func c() (i int) {     defer func() { i++ }()     return 1 } // c()의 return값은 2   Pointer           C 포인터와 동일한 문법            그러나 C와 달리 포인터 연산 x            var p *int // p는 int 포인터 i := 42 p = &amp;i fmt.Println(*p) // read i through the pointer p *p = 21         // set i through the pointer p           Struct      collection of fields   package main  import \"fmt\"  type Vertex struct { \tX, Y int }  func main() { \tv := Vertex{1, 2} \tv.X = 4     fmt.Println(v)} // 4 2 \tp := &amp;v // 포인터도 가능 \tp.X = 1e9 // 포인터 이용 field 접근 문법 다름 \t// cf. C의 p-&gt;X 또는 (*p).x 와 동일 \t// Go에서 (*p).X = 1e9도 가능하긴 하지만 위의 방식이 더 간결 \tfmt.Println(v)} // 4 2 }           Struct literals : javascript의 객체 literal과 유사       // 세가지 모두 가능 var ( \tv1 = Vertex{1, 2}  // argument 순서 상관 o \tv2 = Vertex{X: 1}  // Y:0 is implicit, argument 순서 상관 x \tv3 = Vertex{}      // X:0 and Y:0 )           Array      var 변수명 [크기]type   크기 고정. resize 불가 =&gt; slice 활용   func main() { \tvar a [2]string \ta[0] = \"Hello\" \ta[1] = \"World\" \tprimes := [6]int{2, 3, 5, 7, 11, 13} // Array literal \tfmt.Println(a) \tfmt.Println(primes) }   Slice           slice : []type  (크기 지정 x)            dynamic size            array보다 훨씬 많이 사용            slice는 array에 대한 참조일뿐. 데이터를 저장하진 않음              array에서 앞뒤 bound에 핀만 하나씩 꽂아 놓는 느낌       slice 이용해서 값 변경 시 array 데이터 변경 발생                slice literal은 array literal에서 크기값만 빠진 형태       package main    import \"fmt\"    func main() { \tnames := [4]string{ \t\t\"John\", \t\t\"Paul\", \t\t\"George\", \t\t\"Ringo\", \t} \ta := names[0:2] \tb := names[1:3]    \tb[0] = \"XXX\" \tfmt.Println(a, b) // [John XXX] [XXX George]  \tfmt.Println(names) // [John XXX George Ringo]           \tq := []int{2, 3, 5, 7, 11, 13} // slice literal \ts := []struct { // struct에 대한 slice도 이런식으로 작성 가능 \t\ti int \t\tb bool \t}{ \t\t{2, true}, \t\t{3, false}, \t\t{5, true}, \t\t{7, true}, \t\t{11, false}, \t\t{13, true}, \t}    \t// 앞뒤 bound 생략 가능 (python과 유사) \tvar a [10]int \ta[:2] // == a[0:2] \ta[1:] // == a[1:10] \ta[:] // == a[0:10] }                slice는 length와 capacity를 가진다              length : slice가 실제로 가리키는 길이(slice시작~slice끝). len(s)       capacity : slice 시작~slice의 대상이 되는 array끝. cap(s)       capacity를 넘는 upper bound를 걸 수 없다.           package main  import \"fmt\"    func main() { \ts := []int{2, 3, 5, 7, 11, 13} // len=6 cap=6 [2 3 5 7 11 13] \ts = s[:0] // len=0 cap=6 [] \ts = s[:4] // len=4 cap=6 [2 3 5 7] \ts = s[2:] // len=2 cap=4 [5 7] \ts = s[:4] // len=4 cap=4 [5 7 11 13] \ts = s[:5] // runtime error: slice bounds out of range [:5] with capacity 4 }                Zero value = nil  ( null pointer 역할)       var s []int fmt.Println(s, len(s), cap(s), s == nil) // [] 0 0 true                make : built-in function              java new와 유사       array 안에 값을 모두 zero value로 초기화           func main(){ \ta := make([]int, 5) // len=5 cap=5 [0 0 0 0 0] \tb := make([]int, 0, 5) // len=0, cap=5 [] }                append : func append(s []T, vs ...T) []T                       built-in function                        append로 인해 값을 저장하는 array의 capacity를 초과한 경우 새 array까지 자동으로 할당           func main() { \tvar s []int // len=0 cap=0 [] \ts = append(s, 0) //len=1 cap=1 [0]. nil에도 작동 \ts = append(s, 2, 3, 4) // len=5 cap=6 [0 1 2 3 4],  \ts = s[:6] // len=6 cap=6 [0 1 2 3 4 0]  }                                range로 순회가능 (map에도 적용 가능)       func main() { \tpow := []int{1, 2, 4, 8, 16, 32, 64, 128} \tfor i, v := range pow {      \t// range의 return 값: 2개(index, value(복사본))     \t// v 변경해도 pow에 변화 없음 \t\tfmt.Printf(\"2**%d = %d\\n\", i, v) \t} \tfor i := range pow {     \t// index만 가져옴   \t} }           Map           zero value = nil            nil에는 key 추가 불가능 ( assignment to entry in nil map 발생)       func main() {   \t \tm := make(map[string]Vertex) \tm[\"Bell Labs\"] = Vertex{ \t\t40.68433, -74.39967, \t} \tfmt.Println(m[\"Bell Labs\"])      \t// map literals \tvar m1 = map[string]Vertex{ \t\t\"Bell Labs\": Vertex{ \t\t\t40.68433, -74.39967, \t\t}, \t\t\"Google\": Vertex{ \t\t\t37.42202, -122.08408, \t\t}, \t}    \t// 또 다른 map literals (Vertex 생략 가능) \tvar m2 = map[string]Vertex{     \t\"Bell Labs\": {40.68433, -74.39967},     \t\"Google\": {37.42202, -122.08408}, \t} }                key 접근              value 2개 일 때 두번째 값은 키의 존재여부(bool)           func main() { \tm := make(map[string]int) \tm[\"Answer\"] = 42 // 할당 \tm[\"Answer\"] = 48 // 변경 \tdelete(m, \"Answer\") // 삭제 \tv, ok := m[\"Answer\"]  // 0, false \tm[\"Answer\"] = 0  \tv, ok = m[\"Answer\"] // 0, true }           Function value           함수도 다른 변수들처럼 함수 argument 및 return 값으로 사용 가능            package main    import ( \t\"fmt\" \t\"math\" )    func compute(fn func(float64, float64) float64) float64 { \treturn fn(3, 4) }    func main() { \thypot := func(x, y float64) float64 { \t\treturn math.Sqrt(x*x + y*y) \t} \tfmt.Println(compute(hypot)) // 5 \tfmt.Println(compute(math.Pow)) // 81 }              Function closure      adder() 함수는 closure. sum 변수에 bind 되어 있음   func adder() func(int) int { \tsum := 0 \treturn func(x int) int { \t\tsum += x \t\treturn sum \t} }  func main() { \tpos, neg := adder(), adder() \tfor i := 0; i &lt; 10; i++ { \t\tfmt.Println( \t\t\tpos(i), \t\t\tneg(-2*i), \t\t) \t} }   References      https://tour.golang.org/basic   ","categories": ["Go"],
        "tags": ["Go"],
        "url": "https://dreamsh19.github.io/go/Go-%EA%B8%B0%EC%B4%88/",
        "teaser": null
      },{
        "title": "Go Methods & Interfaces",
        "excerpt":"Method           Go는 클래스가 없음            대신 함수에 Receiver를 넣어 메소드 작성 가능            type Vertex가 선언된 패키지 내에서만 메소드 정의 가능(java 클래스 파일 내에서 관련 메소드 모두 정의하는 것과 유사)            Receiver(객체) 원본에 접근해서 값을 변경하고자 하면 Pointer Recevier 사용 (C++ 참조 전달과 유사)              Pointer receiver를 사용하지 않으면 객체를 복사하여 값을 변경(함수 argument 넘겨주는 것처럼)                type Vertex struct { \tX, Y float64 }    // Value receiver func (v Vertex) Abs() float64 { // v Vertex : receiver \treturn math.Sqrt(v.X*v.X + v.Y*v.Y) }    // Pointer receiver func (v *Vertex) Scale(f float64) { \tv.X = v.X * f \tv.Y = v.Y * f }    func main() { \tv := Vertex{3, 4} \tfmt.Println(v.Abs()) // 5 \tv.Scale(10) // == (&amp;v).Scale(10). Pointer indirection(1) \t(&amp;v).Abs() // == v.Abs(). Pointer indirection(2) \tfmt.Println(v.Abs()) // 50 (pointer receiver를 사용하지 않으면 5) }           Pointer indirection   (1) v는 value이고 Scale은 pointer receiver를 갖지만 Compile error 나지 않음. Go interpreter가 v.Scale()을 알아서 (&amp;v).Scale()로 바꿔서 해석하기 때문   (2) 반대방향도 가능. (&amp;v).Abs()를 알아서 v.Abs()로 바꿔서 해석   단, 메소드(Receiver가  있는 함수)의 receiver에 대해서만 가능. 함수 argument에서는 pointer indirection 일어나지 않음   Pointer Receiver를 사용하는 이유                                직접 receiver를 수정할 수 있다.                                           메소드 호출마다 발생하는 복사를 방지할 수 있다.                                일반적으로 하나의 type에 대한 메소드는 pointer receiver만 갖거나 value receiver만 갖도록 한다. 둘이 섞어 쓰지 않는다.              이유는 ? 인터페이스에 할당 시에 통일성이 떨어져 활용이 번거로워진다.       인터페이스에 포인터로 할당하면 pointer receiver를 갖는 메소드만 사용이 가능하다. Value receiver도 vice versa           Interface           메소드의 집합            package main    import ( \t\"fmt\" \t\"math\" )    type Abser interface { \tAbs() float64 }    func main() { \tvar a Abser \tf := MyFloat(-math.Sqrt2) \tv := Vertex{3, 4}    \ta = f  // a MyFloat implements Abser \tfmt.Println(a.Abs()) // 1.4142135623730951    \ta = &amp;v // a *Vertex implements Abser \tfmt.Println(a.Abs()) // 5     \ta = v\t// Compile error : cannot use v (type Vertex) as type Abser in assignment: \t\t\t// Vertex does not implement Abser (Abs method has pointer receiver)    }    type MyFloat float64    func (f MyFloat) Abs() float64 { \tif f &lt; 0 { \t\treturn float64(-f) \t} \treturn float64(f) }    type Vertex struct { \tX, Y float64 }    func (v *Vertex) Abs() float64 { \treturn math.Sqrt(v.X*v.X + v.Y*v.Y) }                명시적인 implement 표시 없이 해당 메소드(위의 예에서 Abs())를 구현하면 interface를 implement했다고 판단.            이 때 Abs()가 구현되지 않은 타입을 할당하면 컴파일 에러 발생(위 코드에서 Vertex의 Abs() 코드를 주석처리했을 때       cannot use &amp;v (type *Vertex) as type Abser in assignment: \t*Vertex does not implement Abser (missing Abs method)           Interface은 (value, concrete type)의 튜플   Interface에 nil을 할당하는 경우      이때 할당된 value가 nil인 것일뿐, interface 자체는 non-nil(concrete type은 가지고 있다)   package main  import \"fmt\"  type I interface { \tM() }  type T struct { \tS string }  func (t *T) M() {   if t == nil { // (1)  \t\tfmt.Println(\"(nil)\") \t\treturn \t} \tfmt.Println(t.S) }  func main() { \tvar i I \tvar t *T \ti = t \tfmt.Printf(\"(%v, %T)\\n\", i, i) // (&lt;nil&gt;, *main.T) 할당된 value는 nil, concrete type은 non-nil \ti.M() /* 결과 : (nil). 여기서 Null pointer exception 발생하지 않음  \t\t\t\t\t따라서 메소드 내에서 null pointer에 대한 handler를 작성하는게 일반적 like (1) */ }   Interface 자체가 nil인 경우      value와 concrete type 모두 nil   메소드 호출시 runtime error 발생   package main  import \"fmt\"  type I interface { \tM() }  func main() { \tvar i I   fmt.Printf(\"(%v, %T)\\n\", i, i) // (&lt;nil&gt;, &lt;nil&gt;) 둘다 nil   i.M() // runtime error: invalid memory address or nil pointer dereference }   Empty interface      모든 타입의 조상(모든 타입은 0개 이상의 메소드를 가지고 있다.)   java의 Object class와 유사   Unknown type을 핸들링할 때 사용 ex) fmt.Print   package main  import \"fmt\"  func main() { \tvar i interface{}   describe(i) // (&lt;nil&gt;, &lt;nil&gt;)   i = 42    describe(i) // (42, int) \ti = \"hello\"   describe(i) // (hello, string) }  func describe(i interface{}) { \tfmt.Printf(\"(%v, %T)\\n\", i, i) }   Type assertion      Type casting과 유사. 인터페이스를 특정 concrete type으로 변환   Syntax : interface.(type)   Return 값이 하나일 때            Type assertion 가능한 경우 : type assertion 결과 리턴 (1)       불가능한 경우 : panic: interface conversion  발생 (2)           Return 값이 둘 일때            Type assertion 가능한 경우 : (결과, true) 리턴 (3)       불가능한 경우 : (해당 type의 zero value, false) 리턴 (panic 발생 x) (4)           func main() { \tvar i interface{} = \"hello\"  \ts := i.(string) \tfmt.Println(s) // (1) hello \ts, ok := i.(string) \tfmt.Println(s, ok) // (3) hello, true \tf = i.(float64) // (2)  \tf, ok := i.(float64)   \tfmt.Println(f, ok) // (4) 0, false }      Type switch            복수의 Type assertion을 해주는 switch문           func do(i interface{}) {  \tswitch v := i.(type) { // type switch 문을 위해 type이라는 예약어 사용해야함 \tcase int: \t\tfmt.Printf(\"Twice %v is %v\\n\", v, v*2) \tcase string: \t\tfmt.Printf(\"%q is %v bytes long\\n\", v, len(v)) \tdefault: \t\tfmt.Printf(\"I don't know about type %T!\\n\", v) \t} }   Stringer   type Stringer interface {     String() string }     string으로 변환할 수 있는 모든 타입을 담은 인터페이스   fmt 패키지 안에 존재   java의 Object 클래스가 toString()을 갖고 있는 것과 유사   Type에 String() 메소드를 구현하여 Stringer 인터페이스를 활용하는 함수에서 사용 가능   type Person struct { \tName string \tAge  int }  func (p Person) String() string { // String() 클래스 구현 == implements Stringer \treturn fmt.Sprintf(\"%v (%v years)\", p.Name, p.Age) }  func main() { \ta := Person{\"Arthur Dent\", 42} \tz := Person{\"Zaphod Beeblebrox\", 9001} \tfmt.Println(a, z) }    Error           built-in interface       type error interface {     Error() string }                i, err := strconv.Atoi(\"42\") // 두번째 인자로 error를 리턴하는 함수 if err != nil { // 함수 정상 작동시 err == nil   fmt.Printf(\"couldn't convert number: %v\\n\", err)    // print함수에서 error의 값을 가져올 때 err.Error()를 호출(err.Error() 우선순위 &gt; err.String() 우선순위)   return } fmt.Println(\"Converted integer:\", i)           Readers           io package에 정의되어 있음            Go standard library에 다양한 concrete implementation 존재              files, network connections, compressors, ciphers 등등                io.Reader는 Read() 메소드 가지고 있음       func (T) Read(b []byte) (n int, err error) // byte slice b에 데이터를 채워넣고 채워넣은 수(n) 리턴. 꽉채웠으면 b의 크기와 동일 // stream 종료시 err로 io.EOF 리턴                package main    import ( \t\"fmt\" \t\"io\" \t\"strings\" )    func main() { \tr := strings.NewReader(\"Hello, Reader!\")    \tb := make([]byte, 8) \tfor { \t\tn, err := r.Read(b) \t\tfmt.Printf(\"n = %v err = %v b[:n]= %q\\n\", n, err, b[:n]) \t\tif err == io.EOF { \t\t\tbreak \t\t} \t} } /* 결과 n = 8 err = &lt;nil&gt; b[:n]= \"Hello, R\" n = 6 err = &lt;nil&gt; b[:n]= \"eader!\" n = 0 err = EOF b[:n]= \"\" */           References      https://tour.golang.org/methods   ","categories": ["Go"],
        "tags": ["Go"],
        "url": "https://dreamsh19.github.io/go/Go-Methods-&-Interfaces/",
        "teaser": null
      },{
        "title": "Go Concurrency",
        "excerpt":"Goroutine      Go runtime에 의해 관리되는 경량화 스레드   메인 goroutine이 종료되면 프로그램이 종료됨   Channel      서로 다른 goroutine 간 데이터 전달(통신) 수단   make를 통해 초기화해야 사용 가능   Channel 자체가 내부에서 mutex처럼 동작 -&gt; 별도의 lock 관리 필요 x   Sender는 버퍼에 값을 넣을 때까지 block됨(asleep 상태)   Receiver는 버퍼에 값이 들어올 때까지 block됨(asleep 상태)   버퍼가 꽉찬 것 = Receiver가 값을 받을때까지 대기중인 상태   ch := make(chan int) // 채널 버퍼 크기 1 ch &lt;- v    // 채널 ch에 v를 전송한다. v := &lt;-ch  // ch로 부터 값을 받는다  chs := make(chan int, 2) // 채널 버퍼 크기 2   Deadlock           모든 goroutine이 asleep 상태              fatal error: all goroutines are asleep - deadlock!                  package main \t   import \"fmt\" \t   func main() {       ch := make(chan int, 2)       ch &lt;- 1       ch &lt;- 2       ch &lt;- 3 /* fatal error: all goroutines are asleep - deadlock!                   goroutine 1 [chan send] */       fmt.Println(&lt;-ch)       fmt.Println(&lt;-ch)   }           현재 goroutine은 메인 goroutine 하나이고, 이미 꽉찬 채널에 3을 넣으려고 하면 메인 goroutine이 block되어 asleep 상태가 됨 =&gt; 모든 goroutine이 asleep이므로 데드락              package main \t   import \"fmt\" \t   func main() {       ch := make(chan int, 2)       ch &lt;- 1       ch &lt;- 2       fmt.Println(&lt;-ch)       fmt.Println(&lt;-ch)       fmt.Println(&lt;-ch) /* fatal error: all goroutines are asleep - deadlock!                           goroutine 1 [chan receive] */   }           위와 마찬가지로 메인 goroutine은 하나이고, 빈 채널에서 값을 가져오려고 하면 메인 goroutine이 채널에 값이 들어올 때까지 asleep 상태가 됨 =&gt; 모든 goroutine이 asleep이므로 데드락       Range and Close   package main  import ( \t\"fmt\" )  func fibonacci(n int, c chan int) { \tx, y := 0, 1 \tfor i := 0; i &lt; n; i++ { \t\tc &lt;- x \t\tx, y = y, x+y \t}     close(c) // (1) }  func main() { \tc := make(chan int, 10) \tgo fibonacci(cap(c), c) \tfor i := range c { \t\tfmt.Println(i) \t} }           close(c) : 더 이상 채널 c에 값을 넣지 않겠다고 하는 것              sender가 호출하는게 자연스럽고 그래야한다.       receiver가 호출 시 sender가 이미 close된 채널에 값을 넣을 수 있고 close된 채널에 값을 넣을시 panic 발생                range c : 채널 c의 receiver 역할                         v, ok := &lt;- ch                   이때, ok == ( ch에 값이 존재 || !(ch이 closed) )           즉, ch에 값이 하나도 없고, ch이 close되었을때만 false                        range c 는 위의 ok값이 false가 될 때까지 계속 값을 가져옴                          for v := range c   for v,ok &lt;-ch; ok // 같은 표현                                따라서 range loop을 벗어나려면 반드시 close 해주어야함.                      (1)을 주석처리하면  fatal error: all goroutines are asleep - deadlock! 발생(유일한 goroutine인 메인 goroutine이 range문에서 asleep상태가 되기 때문)           range를 사용하는 등의 상황이 아니면 파일처럼 꼭 닫아줄 필요는 없음                           ​   Select      케이스 하나가 runnable할 때까지 block 됨   package main  import \"fmt\"  func fibonacci(c, quit chan int) { \tx, y := 0, 1 \tfor { \t\tselect { \t\tcase c &lt;- x: \t\t\tx, y = y, x+y \t\tcase &lt;-quit: \t\t\tfmt.Println(\"quit\") \t\t\treturn \t\t} \t} }  func main() { \tc := make(chan int) \tquit := make(chan int) \tgo func() { // 익명함수 \t\tfor i := 0; i &lt; 5; i++ { \t\t\tfmt.Print(&lt;-c) \t\t} \t\tquit &lt;- 0 \t}() \tfibonacci(c, quit) } // 결과 01123quit      case로 default도 지정 가능   sync.Mutex           공유 자원에 대해 하나의 goroutine만 접근가능하도록            해당로직의 앞뒤로 lock &amp; unlock                         var mutex sync.Mutex   mutex.Lock()   doSomethingWithSharedResource()   mutex.Unlock()                                  package main \t   import (       \"fmt\"       \"sync\"       \"time\"   ) \t   // SafeCounter is safe to use concurrently.   type SafeCounter struct {       mu sync.Mutex       v  map[string]int   } \t   func (c *SafeCounter) Inc(key string) {       c.mu.Lock()       // Lock so only one goroutine at a time can access the map c.v.       c.v[key]++       c.mu.Unlock()   } \t   func (c *SafeCounter) Value(key string) int {       c.mu.Lock()       // Lock so only one goroutine at a time can access the map c.v.       defer c.mu.Unlock()       return c.v[key]   } \t   func main() {       c := SafeCounter{v: make(map[string]int)}       for i := 0; i &lt; 1000; i++ {           go c.Inc(\"somekey\")       } \t       time.Sleep(time.Second)       fmt.Println(c.Value(\"somekey\"))   }           References      https://tour.golang.org/concurrency   https://gosudaweb.gitbooks.io/effective-go-in-korean/content/concurrency.html   ","categories": ["Go"],
        "tags": ["Go"],
        "url": "https://dreamsh19.github.io/go/Go-Concurrency/",
        "teaser": null
      },{
        "title": "Linux Top 명령어",
        "excerpt":"리눅스에서 프로세스 정보를 살펴 볼 수 있는 top 명령어에 대해 다뤄보고자 한다.   top      top = table of processes   $ top -b -n 1 실행 결과            옵션없이 top실행 시 interval(디폴트 3초)을 두고 refresh       -b : batch mode. 더 이상 인풋을 받지 않고 해당 순간의 top를 출력              top 출력 결과중에서 이 포스트에서 자세히 살펴볼 부분은 빨간색 박스로 표시한 부분이다.           Cpu, Mem, Swap : 하드웨어 리소스 사용량            PR, NI : 프로세스 우선순위            VIRT, RES, SHR : 프로세스가 사용하는 메모리 사용량            S : 프로세스 상태       프로세스의 메모리   VIRT           프로세스가 사용하고 있는 virtual memory 전체 용량       커도 문제되지 않음   Memory commit : 예약을 했을 뿐 실제로 할당을 하진 않음   RES           현재 프로세스가 사용하고 있는 물리 메모리양       이 부분이 메모리 사용과 관련해서 실제로 중요한 부분   이게 높은 걸 찾아야 실제 메모리 점유율이 높은 프로세스를 찾을 수 있다.   SHR           다른 프로세스와 공유하는 shared memory 양            ex) 리눅스 glibc 라이브러리 - 대부분의 리눅스 프로세스가 참조하는 라이브러리       Memory commit           프로세스가 커널에 필요한 메모리를 요청하면, 커널이 사용가능한 메모리 영역을 주고 해당 영역을 프로세스에 주었다는 것을 저장해두는 과정. 이때, 실제 할당이 일어나지 않음       Lazy. 실제로 사용할 때 page fault 발생하여 그 때서야 물리 메모리에 binding되어 RES에 포함   Why lazy?            가장 큰 이유는 fork() 때문       대부분 fork() 직후에 exec()을 실행하기 때문에 메모리 미리 할당해봐야 exec()을 바로 호출하면 쓸모없어진다.       이를 방지하기 위해 COW(Copy-on-write) 기법을 활용하고       memory commit 방식이 없으면 COW도 불가능           sar -r의 %commit : 시스템의 메모리 커밋 비율            물리 메모리에 할당한 척만하고 실제 사용하지 않는 메모리 비율(전체 메모리 대비)       = 물리 메모리에 할당될 가능성이 있는 비율 =&gt; 비율이 높으면 시스템 부하 가능           그렇다면, vm을 물리 메모리보다 더 많은 양만큼 overcommit하게 되는데 무한대로 할당받을 수 있는가? == vm.overcommit_memory  옵션에 따라 다름.   vm.ovecommit_memory            0(default) : 요청 메모리가 Page Cache+Swap Memory+Slab Reclaimable보다 작을때 commit일어남. 현재 free memory 크기와 무관       1 : 무조건 commit       2 : vm.overcommit_ratio (물리 메모리에 대한 %, 기본값 50)의 비율 반영하여 Swap memory+(물리메모리 크기 * vm.overcommit_ratio) 보다 작으면 commit. 이보다 크면 메모리 할당 거부하여 에러 발생 시킴(malloc()에서 null Return)           어찌되었든 overcommit을 지원하고 max값을 어떻게 결정하느냐가 다를 뿐.   프로세스 상태      top의 S(Status)를 통해 확인   상태 종류            D : uninterruptible sleep : I/O(디스크, 네트워크) 대기 중.                    Run queue에서 제거. Wait queue에 등록           이게 많으면 시스템에 영향. 왜냐면 다 R로 돌아갈 애들이기 때문에           따라서 시스템 부하 계산에 포함시킨다.                       R : ready-to-run                    run queue에 포함된 프로세스(running+waiting)           실제로 cpu 자원 소모 중인 프로세스                       S : (Interruptible) sleeping.                    S는 아무나 깨울 수 있지만, D는 해당 I/O 드라이버만 깨울 수 있다는 점에서 차이                       T : traced or stopped. strace 등으로 프로세스의 시스템 콜을 추적하고 있는 상태. 흔하지 않음       Z : zombie. 부모 프로세스가 먼저 죽었을 때 남겨진 자식 프로세스. 부모가 회수를 못하기 때문에 문제                    CPU나 메모리를 사용하지 않음.           그럼 왜 문제? pid가 정리되지 않은 채로 쌓여서 pid 고갈 문제 발생           cf) pid 최댓값 = kernel.pid_max                           프로세스 우선순위      PR, NI을 통해 확인   PR: priority. 스케줄러가 활용하는 실제 우선순위값. 값이 작을수록 우선순위 높다            유저 프로세스의 경우 PR = 디폴트 우선순위+nice value           NI : nice value. 주로 우선순위를 높일때(값을 낮출때) 사용.   디폴트 우선순위 = 20   nice -n -10 와 같은 명령으로 우선순위를 낮출 수 있다.            이때 코어수에 따라 의도대로 안될 수도 있다.       예를 들어 2-core인 경우 A 프로세스를 B 프로세스보다 먼저 실행시키고 싶어서 A프로세스의 nice 값을 10낮춘다고 해서 B프로세스보다 A프로세스가 우선하진 않는다(A,B가 병렬로 실행되기 때문에). 싱글코어에서는 가능           renice 등도 가능.   RT : Realtime            반드시 특정 시간 안에 종료되어야 하는 중요한 프로세스       ex) 커널에서 사용하는 데몬       사용자 프로세스 아님       RT 스케줄러가 따로 존재. CFS 스케줄러보다 먼저 실행됨           References           DevOps와 SE를 위한 리눅스 커널 이야기 2장 내용을 요약 정리한 것입니다. gitbook 링크            https://jujupapa.tistory.com/37      ","categories": ["Linux"],
        "tags": ["Linux","DevOps"],
        "url": "https://dreamsh19.github.io/linux/Linux-top-%EB%AA%85%EB%A0%B9%EC%96%B4/",
        "teaser": null
      },{
        "title": "애플리케이션 에러 코드 설계",
        "excerpt":"이슈      http status code와 분리된 애플리케이션 레벨의 에러코드가 필요   예를 들어, http status code 500 에러 발생 시 애플리케이션 내부적으로 어떤 에러가 발생했는지 http status code만 보고 파악하기 어렵기 때문에   애플리케이션 에러의 형식을 정의하여 관리를 용이하게 하기 위해서도 필요   End-user보다는 개발자의 트러블 슈팅을 위한 것   Application level 에러 코드에 대해서는 정확히 표준이 없어서 애플리케이션마다 구조가 상이   좋은 애플리케이션 레벨 에러 디자인 패턴      에러상황에 맞는 적절한 HTTP status code를 리턴하는 데 우선 집중할 것.            애플리케이션 레벨 에러는 하나의 HTTP status code 안에서 세부적으로 에러를 구분하기 위한 목적이므로           HTTP status code와 별개로 Response body를 활용한다.            애플리케이션 레벨 코드로 HTTP status code를 혼용해서 쓰지 않는다.           하나의 애플리케이션 레벨 코드는 하나의 HTTP status code에만 종속되도록 한다. (M:N이 되지 않도록)   단순 에러코드만 리턴하지 않고 human-readable한 정보를 포함하도록 한다         에러코드는 꼭 Integer type일 필요는 없다.            알파벳을 포함할수도 있고, error code라는 enumeration을 정의하여 활용할 수도 있다           운영 환경에서는 보안에 유의할 것            운영 시 일반적인 서비스 구조에서는 에러 스택정보를 API 에러 메세지에 포함 시키지 않는 것이 바람직 하다. 옵션에 따라 dev 모드등으로 기동시, REST API의 에러 응답 메세지에 에러 스택 정보를 포함해서 리턴하도록 하면, 디버깅에 매우 유용하게 사용할 수 있다.           여러가지 예시   1. IETF 표준      REST API의 에러 핸들링의 표준화를 위해서 RFC 7807에서 제시한 에러핸들링 일반 구조.            제시만 했을 뿐 표준화가 되진 않았다.           총 5개의 필드로 구성            type : 세부 에러 코드. (꼭 숫자일 필요는 없음)       title : 에러에 대한 간략한 설명       status(optional) : HTTP response code       detail : 에러에 대한 자세한 설명       instance : 에러 발생 근원지 URI           예시   {     \"type\": \"/errors/incorrect-user-pass\",     \"title\": \"Incorrect username or password.\",     \"status\": 401,      \"detail\": \"Authentication failed due to incorrect username or password.\",     \"instance\": \"/login/log/abc123\" }         2. 관련 도큐먼트 Link 포함      에러 코드 번호와 해당 에러 코드 번호에 대한 Error dictionary link를 제공하는 방법            이를 위해서는 Error dictionary의 관리가 선행되어야함           Twillo, Oracle 등            Oracle 도큐먼트 예시 : https://docs.oracle.com/cd/E24329_01/doc.1211/e26117/chapter_bea_messages.htm#sthref7       Amazon SES API 예시 : https://docs.aws.amazon.com/ses/latest/DeveloperGuide/using-ses-api-error-codes.html           예시   {     \"status\":\"401\",     \"message\":\"Authenticate\",     \"code\":\"200003\",     \"more info\":\"http://www.twillo.com/docs/errors/20003\" }        3. Trace id 포함      애플리케이션에서 로깅에 사용한 trace id를 error response body안에 담고 후에 트러블슈팅할 때 해당 아이디를 로깅 시스템에서 조회할 수 있도록한다.   직접적인 에러 정보를 노출하지 않기 때문에 보안에 유리   Facebook Graph API의 fbtrace_id     {     \"message\": \"Missing redirect_uri parameter.\",     \"type\": \"OAuthException\",     \"code\": 191,     \"fbtrace_id\": \"AWswcVwbcqfgrSgjG80MtqJ\" }           References      https://www.baeldung.com/rest-api-error-handling-best-practices   https://sanghaklee.tistory.com/57   https://softwareengineering.stackexchange.com/questions/209693/best-practices-to-create-error-codes-pattern-for-an-enterprise-project-in-c   https://developers.facebook.com/docs/graph-api/using-graph-api/error-handling   https://docs.aws.amazon.com/ses/latest/DeveloperGuide/using-ses-api-error-codes.html   https://stackoverflow.com/questions/51317619/error-code-pattern-for-api   https://bcho.tistory.com/954  ","categories": ["Error"],
        "tags": ["Application","Error","Error code"],
        "url": "https://dreamsh19.github.io/error/%EC%95%A0%ED%94%8C%EB%A6%AC%EC%BC%80%EC%9D%B4%EC%85%98-%EC%97%90%EB%9F%AC-%EC%BD%94%EB%93%9C-%EC%84%A4%EA%B3%84/",
        "teaser": null
      },{
        "title": "Nginx 414 Request Uri Too Large",
        "excerpt":"성능 테스트를 위해 go 기반의 http 로드 테스팅 툴인 vegata 를 이용하여 진행하던 중 이상한 점을 발견했다.   이슈      vegeta를 이용하여 테스트를 진행했을 때는 모두 200 OK를 반환   그런데 동일한 요청을 로컬에서 브라우저 또는 Postman으로 테스트했을 때는 414 Request URI Too Large가 반환   상태 코드의 이름에서 알 수 있듯이, Request URI가 너무 길어서 발생한 것임(실제로 요청의 쿼리 파라미터가 매우 긴 형태였다)을 유추할 수는 있었지만 그렇다면 vegeta로 요청을 보냈을때도 마찬가지로 414가 반환되어야 하는데, 왜 200이 반환되었는가?   같은 요청에 대해 다른 결과가 나오는 것이 의아하기도 했고, http status code 중 414는 처음 접하는 에러인지라 이 에러에 대해 찾아보게 되었다.   Http status code 414   Http status code 414는 URI Too Long에 해당하는 status code로, 서버가 정한 요청 uri의 최대길이보다 더 큰 요청을 보냈을 때 반환하는 상태 코드이다.   주로 발생하는 경우는 아래와 같다.      post 요청을 get 요청으로 전환하여 쿼리 파라미터가 아주 길어지는 경우   redirection loop에 빠지는 경우(redirection uri가 다시 자신을 가리키는 경우 등)   혹은 악의적으로 uri를 길게 구성하여 보안 취약점을 이용하려는 경우   사용하는 웹서버는 nginx로 구성했기 때문에 nginx에서 해당 에러와 관련된 설정을 찾아보았다.   Nginx client request header buffer   Nginx의 경우에는 client request header buffer라는 것이 존재하고, 클라이언트에서 요청을 보내면, 해당 요청의 request header를 버퍼에 저장하는 방식으로 작동한다. 그런데 그 버퍼의 종류에는 두가지가 있는데, 일반적인 요청의 경우에 사용하는 버퍼와 request header가 큰 요청의 경우이다.   1. 일반적인 요청   Nginx의 경우 client_header_buffer_size 라는 변수로 일반적인 요청에 대한 헤더 버퍼 크기를 지정한다. 디폴트 값은 1KB이다. 대부분의 경우는 1KB를 넘지 않기 때문에 이 버퍼를 거쳐간다. 그리고 이 버퍼의 사이즈를 초과하는 요청이 들어왔을 경우에는 아래에서 다룰, 조금 더 큰 버퍼로 처리하게 된다.      2. Header size가 큰 요청   요청 header size가 위에서 지정한 client_header_buffer_size 를 초과하는 경우 보다 큰 새로운 버퍼(large buffer라고 칭하겠다)에 요청 헤더를 저장하게 된다. 이때, 그 크기는 large_client_header_buffers 값을 통해 지정하게 된다. 여기서는 large buffer의 개수와 크기를 같이 지정해주게 되는데 디폴트 값은 개수 4개, 크기는 8KB이다. 즉, 헤더 크기가 client_header_buffer_size 를 넘는 요청은 최대 4개까지 동시에 처리가 가능함을 의미한다.      이때 large buffer의 경우에는 항상 떠있는 건 아니고 요청이 있을 때 할당하는 것이라고 한다. 애초에 목적 자체가 일반적으로 사용되는 목적보다는 헤더 크기가 큰 특수한 요청만 처리하는 목적이기 때문인 것으로 생각된다. 그리고 커넥션이 keep-alive 상태가 되면 할당을 해제한다.   그리고 버퍼 개수보다 많은 요청이 들어오는 경우에는 새로운 요청을 큐에 저장하여 나중에 처리하는지 혹은 애초에 요청 자체를 거부하는지는 확인이 필요하다.   결론적으로 저 large buffer의 크기(large_client_header_buffers )보다 큰 헤더를 가진 요청이 들어오는 경우에 414 status code를 리턴하게 된다.   Client에 따른 헤더 크기 차이   그렇다면 동일 요청이라면 결과가 일관되게 나와야 할 것 같은데 vegeta로 요청 시 200이 나오고 postman으로 요청 시 414가 나오는 이유는 무엇일까?   결론부터 말하자면, 동일 요청이어도 클라이언트에 따라 request header 크기가 달라져서 결과가 달랐던 것이다. Request header의 경우 url을 제외하고도 기타 정보(User-agent) 등을 추가적으로 포함하는데 이러한 기타 정보는 요청을 구성하는 클라이언트에 따라 달라지게 된다. 그래서 실제로 동일 요청에 대해 클라이언트별로 Request header 크기를 확인해보았다.   1. curl   curl의 경우 vegeta와 마찬가지로 414가 나오지 않고 200이 반환되었다. 그래서 이때의 request header 크기를 확인해보았다. curl의 경우 -w 옵션을 이용하여 아래와 같은 명령어로 request size를 알 수 있다.(실제 반환되는 결과는 request 전체의 크기, 즉 header+body의 크기이지만, get 요청이기 때문에 request body가 존재하지 않아 request 전체의 크기=header의 크기가 된다)   curl -o /dev/null -w \"size_request:%{size_request}\" {url}   해당 요청의 결과 사이즈는 5,575B로, 8K(nginx 디폴트 설정)를 넘지 않아 200을 리턴하게 된다.   2. Postman   Postman의 경우 414가 반환되었다.      이때 request size는 아래와 같은 size 탭을 통해 확인할 수  있으며, request header 크기가 9.29KB로 8K를 초과하여 414를 리턴하게 된다.   추가적으로 url 길이를 줄여가며, 요청을 보냈을 때 크기가 8.18KB일 때까지 200이 반환되다가 8K(8,192B)를 넘어가는 순간 414가 반환되었다.   3. Chrome browser   크롬 브라우저의 경우 414가 반환되었다.      크롬 브라우저의 request header size를 확인하는 방법은 조금 까다로웠다. 위의 그림에서 처럼 크롬 개발자도구 &gt; Network 탭에서 해당 요청을 선택한후 Copy as cURL을 선택하여 복사한 후 위의 1번 curl의 방법으로 확인했다.   확인 결과 크기가 9,797B로 8K를 넘어 414를 리턴하게 된다.   해결 방법   보통 이처럼 GET 방식의 요청에 쿼리 파라미터로 데이터로 전달하는 과정에서 414 에러가 발생하는 경우에 POST 방식으로 전환하는 것이 정석이라고 한다. 하지만 부득이하게 get 요청을 유지해야하는 경우가 있을 수도 있다. API 스펙을 변경하는 것은 해당 API를 호출하는 다른 모든 서비스들의 변경이 필요하기 때문이다. 따라서 불가피하게 GET 요청 방식을 유지해야하는 경우,  nginx의 large_client_header_buffers 의 크기를 더 큰 값으로 수정함으로써 문제를 해결할 수 있다. (일반 요청 버퍼 사이즈인 client_header_buffer_size 를 늘려도 되지만, 일반 요청에 대해서도 지나치게 큰 버퍼를 할당하는 것은 메모리 낭비이다.)  하지만 이러한 해결방식이 갖는 문제점이 있는데      버퍼의 개수 제한이 있기 때문에 헤더 사이즈가 큰 요청이 빈번하게 들어오는 경우 병목이 발생할 수 있다.   최대 길이의 상한을 예측하기 어려운 경우, 즉 해당 버퍼 크기를 넘는 요청이 들어오지 않는다는 보장을 할 수 없는 경우 결국 같은 문제에 봉착하게 된다.   따라서, 헤더 크기가 큰 요청이 빈번하지 않고, 요청 헤더 크기의 상한을 예측가능한 경우에는 유용한 방법으로 생각된다.   References           https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/414            http://nginx.org/en/docs/http/ngx_http_core_module.html#large_client_header_buffers            https://sub0709.tistory.com/175            https://serverfault.com/questions/892006/chrome-devtools-request-header-size      ","categories": ["Nginx"],
        "tags": ["Nginx","414","http status code"],
        "url": "https://dreamsh19.github.io/nginx/Nginx-414-Request-URI-Too-Large/",
        "teaser": null
      },{
        "title": "SSH key를 이용한 SSH 인증 자동화",
        "excerpt":"이슈   ssh를 이용하여 원격 서버(주로 게이트웨이 서버)를 매번 접속하는데 접속할 때마다 계정, 도메인, 비밀번호를 매번 치는 게 번거로웠다. 그래서 이를 자동화하기 위한 스크립트를 만들면 좋을 것 같다는 생각이 들었다. 처음에는 패스워드를 스크립트로 입력하도록 하는 자동화를 생각하며 방법을 찾아보았으나 패스워드 없이 ssh key를 이용하여 로그인하는 방법이 있다는 걸 알게 되었고, 그 방법이 더 좋아보여서 블로그에 남기고자 한다. 추가적으로 ansible에서 활용 가능성까지 다루고자 한다.   SSH key를 이용한 인증   우선 ssh를 이용하여 로컬 서버에서, 원격의  remote.com 서버의 irteam 계정으로 접속하고자 하는 상황을 생각해보자.   접속을 시도하는 쪽이 꼭 로컬 서버일 필요는 없지만(로컬 서버에서 또 다른 로컬 서버로 접속하는 경우) 이 글에서는 편의상 접속을 시도하는 쪽을 로컬 서버, 접속 대상이 되는 쪽을 리모트 서버라고 칭하려고 한다.   로컬 서버에서   $ ssh irteam@remote.com  의 명령어 입력후 패스워드까지 입력해서 인증 과정을 거쳐 패스워드가 일치하면 원격 접속에 성공하게 된다.   그러나 ssh key를 이용하면 패스워드 대신 ssh key를 이용하여 인증을 진행하여 접속 가능 여부를 판단한다.   1. ssh key 생성   접속을 시도하는 로컬 서버에서 최초(기존에 ssh key를 생성한 적이 없는 경우)에 아래의 명령어를 통해 ssh key를 생성해야한다. 기존에 ssh key가 존재하는 경우 이 과정을 생략가능하다.  일종에 자신을 나타내는 id를 만드는 과정으로 생각할 수 있다.   $ ssh-keygen -t rsa -b 2048   그럼 아래와 같은 내용이 출력되는데, ssh key를 어디에 저장할지 등등 설정에 관한 내용이다.   Generating public/private rsa key pair. Enter file in which to save the key (/home/username/.ssh/id_rsa):  Enter passphrase (empty for no passphrase):  Enter same passphrase again:  Your identification has been saved in /home/username/.ssh/id_rsa. Your public key has been saved in /home/username/.ssh/id_rsa.pub.   특별히 지정이 필요한 경우가 아니라면, 아무 내용을 입력하지 않고 엔터를 입력하면 디폴트 값이 적용되므로, 엔터를 연타해준다   $ ls ~/.ssh authorized_keys id_rsa          id_rsa.pub      known_hosts   그러면 홈 디렉토리(~) 아래 .ssh 디렉토리 안에 id_rsa, id_rsa.pub가 생성된 걸 확인할 수 있다. 각각 private key와 public key를 저장하고 있다.   2. ssh key 복사   위에서 생성한 로컬 서버의 ssh key 중 공개키(public key)를 리모트 서버에 전달하는 과정이 필요하다. 앞으로 이 공개키를 이용하여 통신을 할 것이니 리모트 서버에 그 키를 저장하는 과정 정도로 볼 수 있다. 로컬 서버에서 아래와 같은 명령어로 로컬 서버의 공개키를 리모트 서버에 전달할 수 있다.   $ ssh-copy-id irteam@remote.com irteam@remote.com's password:    최초 인증이기 때문에 패스워드를 입력하면 공개키 복사가 완료된다.   3. ssh 접속   $ ssh irteam@remote.com   공개키를 전달한 후 로컬 서버에서 ssh로 접속시 더 이상 패스워드를 묻지 않고 접속에 성공하게 된다!   추가적으로 리모트 서버에서 아래 명령어를 입력시   $ cat ~/.ssh/authorized_keys   로컬 서버의 공개키(id_rsa.pub에 저장되어 있던 키)가 저장되어 있는 것을 확인할 수 있다.   Ansible에서의 활용   Ansible은 ssh를 기반으로 여러 호스트에 명령을 하달하는 소프트웨어로, 원격으로 여러 서버에서 배포를 자동화할때 주로 쓰인다.   이때 여러 호스트에 접속시 패스워드가 필요하기 때문에 플레이북 작성 시 인증 과정을 포함하는 경우가 있다. 그런데 위의 ssh key를 사전에 등록해놓으면 패스워드 입력없이 ssh로 원격 호스트에 접속이 가능하다. 따라서 접속 대상이 되는 호스트들에 대해 ssh key를 사전에 등록함으로써 플레이북에서 인증 과정을 제거할 수 있다. Ansible을 이용해 접속하는 호스트(예를 들어 배포 대상이 되는 서버)들의 경우 일반적으로 그 대상이 결정이 되어있고, 자주 변하는 정보가 아니기 때문에 ssk key를 이용한 인증을 유용하게 사용할 수 있다.   References      https://serverfault.com/questions/241588/how-to-automate-ssh-login-with-password   ","categories": ["ssh"],
        "tags": ["ssh","automation"],
        "url": "https://dreamsh19.github.io/ssh/SSH-key%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-SSH-%EC%9D%B8%EC%A6%9D-%EC%9E%90%EB%8F%99%ED%99%94/",
        "teaser": null
      },{
        "title": "블로그를 다시 시작하며",
        "excerpt":"오래도록 방치 되어온 블로그를 다시 시작해보려고 한다.   그동안의 회고   입사하면서 만든 블로그이다. 업무하면서 혹은 개인적인 공부하면서 배운 것들을 기록하고자 만들었다.   그러나 언제나 그렇듯 취지는 좋았으나, 현생에 치이다보면 블로그는 뒷전이 됐고 그렇게 21년 6월을 마지막으로 더 이상 새로운 글은 올라오지 않게 됐다.  그렇다고 블로그를 아예 머릿속에서 지웠던 건 아니었다. 늘 소재만 간단하게 todo앱에 메모해두었고, 그 소재는 글이 되지 못하고 쌓이기만 해서 100개가 넘는 지경에 이르렀다.  그리고 최근에 긴 추석 연휴가 있었는데, 이때 현생에서 벗어나 나만의 시간을 좀 가지면서 새로운 목표들을 설정하게 됐고, 블로그 운영도 그 목표들 중 하나에 포함되었다.   하지만 지금까지 해왔던 것처럼 하다가는 결국 또 같은 결과로 이어질 게 뻔해서 구체적인 액션을 취하기로했다.  그렇게 이어진 첫 액션은 “글쓰기 모임에 들어가자”였고, 그런 모임을 찾아보던 중 글또라는 집단이 있다는 걸 알게 됐다.  무려 8기까지 운영된 오래된 집단이었고, 마침 시기적절하게 신규 9기를 모집한다는 글을 보고 참여하기로 맘 먹었다.  운이 좋게도, 글또 9기에 합류하게 되었고, 글또 활동의 첫글을 이렇게 작성하게 됐다.   첫 글을 기술적 내용으로 작성할까도 고민을 했지만, 새로운 블로그(v2..?)로 탈바꿈한 것을 구분하기 위한 대문글 같은게 있으면 좋을 것 같다는 생각이 들어서 이 글을 작성하게 됐다.   이슈와 액션   그럼 그동안은 왜 블로그 운영이 제대로 안됐을까?  개인적으로 생각해본 원인을 정리하면 아래 두가지이다.      블로그 운영에 대한 정책이 없다.   글을 잘 써야한다는 부담이 크다.   1. 블로그 운영에 대한 정책이 없다.   이게 사실 가장 큰 이유였던 것 같다.  구체적인 정책이 없으니, 누가 등떠밀지도 않는데 한도끝도 없이 미뤄질 수 밖에.   그래서 이제부터의 블로그는 명확한 운영 정책을 가져가고자 한다.  그리고 사실 이건 글또라는 집단에서 이미 만들어준 게 있다. 2주에 한번 특정한 날짜까지 제출이다.  물론 글또의 정책을 계속 가져갈 수는 없으니 나만의 정책을 만들기는 해야겠지만, 처음 정책을 설정하는 입장에서 기존의 정책을 참고할 수 있다는 것은 좋은 기회인 것 같다.  글또의 정책을 그대로 따라가다보면, 나에게 맞는 정책이 어떤 건지도 알 수 있게 되고, 그거에 따라 조정해나가면 되겠지라는 생각이다.  일단 정책이 없던 상태에서 정책이 있는 상태로 나아간 것이 중요하다고 생각한다.   2. 글을 잘 써야한다는 부담이 크다.   아무래도 퍼블릭한 공간의 나의 기록을 남기는 것 자체가 큰 부담으로 다가오는 건 사실이다. 그렇다보니 미루게 된다.  하지만 블로그의 본질이 퍼블릭한 것인데, 이걸 회피할 수는 없다. 결국에는 부담을 어떻게 줄이냐의 문제일텐데, 왜 부담을 느꼈는지부터 고민해보았다.   사실 “잘” 써야한다는 것 때문에 부담을 느끼고, 지금도 사실은 그렇게 느낀다.  그리고 이 부담이 유난히 크게 느껴지는 이유는, 처음부터 글을 완성하려고 하는데, 그 완성된 글은 “잘 쓰여진 글”이어야 하기 때문에 그런 듯하다.   그렇다면 결국 답은 잘 쓰여지지 않은 글로 초안을 시작하는 것이다.  그러다 보면 날 것의 글을 그대로 퍼블리싱할 수는 없으니, 탈고는 불가피(?)하게 된다.   코드로 치면, 글 하나는 pr이고, 그 글을 위한 안들은 commit이라고 생각하면 될듯하다.  커밋들을 쌓아서 pr을 올리고, 리뷰를 거쳐서 머지가 되듯이, 초안과 탈고를 거친 것들을 모아서 하나의 글이 완성되는 것이라고 생각하려고 한다. (그동안은 사실 1 pr == 1 commit 이었다. 초안이 곧 완성본이었다)     (전무한 pr..)   글로 쓰다보니, 실제로 이 비유를 블로그 운영할때 그대로 적용해도 될 것 같다는 생각이 문득 들었다. 앞으로 블로그 글 작성은 이렇게 해야겠다.   그리고 궁극적으로 지향하는 바는 글쓰기 자체를 “가벼운” 습관으로 갖게 만드는 것이다.  사실 지금도 노트북 앞에 앉아서 각잡고 글을 쓰고 있지만,  버스에서 별 생각없이 유튜브를 보듯이, 글쓰기도 별 생각없이 길가는 길에 몇글자 쓰고, 버스 타서 몇글자쓰고 이렇게 가볍게 조금조금씩 할 수 있는 상태가 되는것을 목표로 하려고 한다.   결론   이 글을 시작으로 블로그를 다시 시작해보려고 한다. 대신 구체적인 액션과 함께, 지속 가능하도록.  ","categories": ["blog"],
        "tags": ["글또"],
        "url": "https://dreamsh19.github.io/blog/%EB%B8%94%EB%A1%9C%EA%B7%B8%EB%A5%BC-%EB%8B%A4%EC%8B%9C-%EC%8B%9C%EC%9E%91%ED%95%98%EB%A9%B0/",
        "teaser": null
      },{
        "title": "mlocate db로 인한 disk full",
        "excerpt":"이슈      매일 새벽 시간대 특정 서버 루트(/) 디렉토리의 디스크 풀 알람이 발생했다가 해소되는 현상이 발생했다.   루트 디렉토리이기 때문에 지속 발생 시 서비스에 영향을 줄 수 있으므로 원인을 파악하기 시작했다.   원인 디렉토리 및 파일      디스크 풀의 원인이 되는 디렉토리 및 파일을 알아내기 위해 아래 스크립트를 크론탭에 등록하여 로그에 기록했다.            매분 해당 스크립트를 통해 디스크 사용율이 높은 디렉토리를 기록했다.       (디스크가 계속 차있었다면, 해당 시점에 용량을 차지 하고 있는 디렉토리를 확인하면 됐을텐데, 이번 경우에는 디스크가 찼다가 다시 줄어드는 패턴이었기 때문에 문제 인지 시점에는 문제가 된 디렉토리를 알 수가 없었다.)         sudo du -h --max-depth=2 --exclude={proc,home1} / | grep -vE \"^0\\s\" | sort -hr                           확인 결과 아래와 같은 패턴이 발견되었다.            03시 19분부터 /var/lib 디렉토리의 디스크 사용량이 급증하기 시작       03시 29분까지 지속       03시 33분에 정상화           결국 문제가 된 /var/lib 디렉토리를 살펴보았다.         살펴보니 /var/lib/mlocate 디렉토리의 modified time이 03시 32분(디스크 사용량 정상화시점)과 일치했고, 해당 디렉토리에는 7.5GB 크기의 mlocate.db 파일 하나만 존재했다.   그래서 이 파일의 용도가 무엇이며, 어떻게 만들어지는 것인지 추적하기 시작했다.   mlocate.db     mlocate.db는 리눅스 locate 유틸을 위한 색인용 데이터베이스이다.                   locate는 find와 유사한 용도의 유틸이며, find와의 차이점은 아래 find와 locate의 차이점 참고.           그렇다면 이 db 파일을 만드는 쪽은 어디인가?                   해당 mlocate.db는 updatedb 프로세스에 의해 생성되며, updatedb는 cron daily에 의해 하루에 한번 실행된다.           그래서 cron.daily를 확인해보니 mlocate 스크립트가 있었다.            그리고 mlocate 스크립트 내부를 확인해보니 실제로는 updatedb 바이너리를 실행하는 것이었다.           [root@server ~]$ ls -al /etc/cron.daily/ total 24 drwxr-xr-x.  2 root root   57 Jul  1  2022 . drwxr-xr-x. 94 root root 8192 Aug 23  2022 .. -rwx------.  1 root root  219 Apr  1  2020 logrotate -rwxr-xr-x.  1 root root  618 Oct 30  2018 man-db.cron -rwx------.  1 root root  208 Apr 11  2018 mlocate [root@server ~]$ sudo cat /etc/cron.daily/mlocate #!/bin/sh nodevs=$(awk '$1 == \"nodev\" &amp;&amp; $2 != \"rootfs\" &amp;&amp; $2 != \"zfs\" { print $2 }' &lt; /proc/filesystems)  renice +19 -p $$ &gt;/dev/null 2&gt;&amp;1 ionice -c2 -n7 -p $$ &gt;/dev/null 2&gt;&amp;1 /usr/bin/updatedb -f \"$nodevs\"      그리고 실제 크론에 의해 수행되는 게 맞는지 확인하기 위해 크론 로그를 보니 약 새벽 3시경 실행이 됐던 것으로 확인했고,   mlocate 수행 시작 시점(03:19)과 수행 종료(03:32) 시점이 디스크 풀 발생 및 해소시간과 일치함을 발견했다.     [root@server ~]$ sudo grep 'cron.daily' /var/log/cron (생략) Mar 29 03:01:01 server anacron[15318]: Will run job `cron.daily' in 18 min. Mar 29 03:19:01 server anacron[15318]: Job `cron.daily' started Mar 29 03:19:01 server run-parts(/etc/cron.daily)[29377]: starting logrotate Mar 29 03:19:01 server run-parts(/etc/cron.daily)[29385]: finished logrotate Mar 29 03:19:01 server run-parts(/etc/cron.daily)[29377]: starting man-db.cron Mar 29 03:19:01 server run-parts(/etc/cron.daily)[29399]: finished man-db.cron Mar 29 03:19:01 server run-parts(/etc/cron.daily)[29377]: starting mlocate Mar 29 03:32:52 server run-parts(/etc/cron.daily)[26074]: finished mlocate Mar 29 03:32:52 server anacron[15318]: Job `cron.daily' terminated           결국 updatedb 프로세스가 디스크풀을 유발 및 해소시키는 원인으로 가장 의심스러운 상황이었다.   그럼 왜 디스크풀이 발생할까?     updatedb 프로세스는 색인 파일(mlocate.db)을 갱신하는 프로세스이다.   디스크 풀이 발생했다가 자연해소되는 패턴으로 보아, temp db 파일을 만들고, mv하는 방식으로 동작하지 않을까라고 예상하고 실제 소스코드를 확인해봤다.   실제 소스코드를 확인해보니 예상과 동일하게 temp파일을 만들고 rename하는 방식으로 동작하는 것을 확인했다.                                mlocate/src/updatedb.c at 0ce05077df942995aef99d1d009b7fa4372dc96c · msekletar/mlocate · GitHub                                           mlocate/src/updatedb.c at 0ce05077df942995aef99d1d009b7fa4372dc96c · msekletar/mlocate · GitHub                           결국 updatedb 프로세스가 수행되는 도중에는 실제 색인 파일 + 임시 색인 파일이 동시에 존재하게 되어, 색인 파일크기의 최대 2배 가까이 디스크를 사용하면서 디스크 풀이 발생한 것이고   updatedb 갱신 완료 시점에는 하나의 파일만 남게 되어 디스크 사용량이 다시 줄어들어드는 것이다.   해결방안   근본적인 원인은 색인 파일인 mlocate.db 파일 사이즈가 큰 것이 원인이고, 결국 이 색 인 파일의 크기는 전체 디스크 사용율에 비례한다.  문제가 된 서버의 경우도, 실제로 1TB 이상을 사용하고 있던 서버였기 때문에 색인 파일 크기도 그에 따라 커졌던 것이다.  그렇기 때문에 전체 파일시스템 중 사용하지 않는 파일을 지울 수 있다면, mlocate.db 파일 사이즈도 줄어들어 자연스럽게 해결될 수 있다.   하지만 우리의 경우에는 실제로 해당 파일들이 모두 사용하고 있는 파일들이었기 때문에 디스크 사용율은 유지하면서 간단하게 해결할 수 있는 방안을 검토했다.      /etc/updatedb.conf의 PRUNEPATHS에 특정 디렉토리 추가            /etc/updatedb.conf는 이름에서도 알 수 있듯이 updatedb 프로세스의 설정 정보를 담고 있는 파일이다.       그리고 설정 중에는 PRUNEPATHS 환경 변수를 통해서 지정할 수 있는 설정이 있는데, 이 설정은 색인에서 제외할 수 있는 경로들을 지정할 수 있는 설정이다.       그렇다면, 디스크 사용율이 높지만 locate에서 참조될 일이 없는 경로를 색인에서 제외하면, mlocate.db 파일이 커지는 문제를 해결할 수 있다.                  updatedb의 옵션(output)을 통해 db 경로 변경                   updatedb 스크립트의 옵션에 결과 색인 파일명을 지정할 수 있는 옵션(-o)이 있다.       지정하지 않는 경우 디폴트(/var/lib/mlocate/mlocate.db)가 된다.       해당 옵션을 통해 마운트된 크기가 큰 디스크 영역에 색인 파일을 저장한다면, 당장 문제가 된 디스크 영역의 디스크 풀은 해결할 수 있다.       다만, 색인 파일의 크기가 큰 것은 여전히 남아있는 문제이다.           cron.daily에서 mlocate를 제외            사실 서비스와 관련 없는 리눅스의 유틸이기 때문에 주기적으로 색인파일을 업데이트해줄 필요가 없어보였다.           3가지 안 중 우리는 1안을 적용했다.   3안은 locate에 의존적인 프로세스의 존재 여부에 대한 조사가 필요했기 때문에 제외를 했고, 1안과 2안 중 좀 더 근본적으로 색인 파일의 크기를 줄일 수 있는 1안을 채택 및 적용했다.   1안을 적용 후 수동으로 updatedb 명령어를 수행한 결과,      7.5G에 달했던 색인 파일이 1.9M까지 줄어들었다.   그외   find와 locate의 차이점  일반적으로 리눅스 환경에서 조건에 맞는 파일을 찾을때 find 명령어를 많이 사용했는데, 이번에 locate도 동일한 기능을 하는 것을 알게됐다.  그렇다면 두 가지 유틸의 차이는 무엇일지 궁금하여 찾아보았다.   find는 명령을 수행한 시점에 파일시스템을 직접 탐색하는 반면, locate는 사전에 색인을 만들어두고(cron.daily에 의해 하루에 한번) 색인에 기록된 내용을 바탕으로 결과를 알려준다.  그렇기 때문에, 조회 속도 관점에서는 locate가 빠를 수 있지만, 정보의 실시간성을 반영하지 못하는 문제가 있다.  결국 일반적인 db에 직접 검색 vs 색인을 통한 검색의 트레이드 오프를 그대로 가져간다고 볼 수 있다.     References     GitHub - msekletar/mlocate: mlocate hacking   updatedb(8): update database for mlocate - Linux man page   locate(1): find files by name - Linux man page   updatedb.conf(5): config file for updatedb - Linux man page   [Linux] Locate 와 FIND 명령어 차이점  ","categories": ["Linux"],
        "tags": ["Linux"],
        "url": "https://dreamsh19.github.io/linux/mlocate-db%EB%A1%9C-%EC%9D%B8%ED%95%9C-disk-full/",
        "teaser": null
      },{
        "title": "JsonInclude를 활용한 데이터 사이즈 축소",
        "excerpt":"이슈      서버에서 요청마다 남기는 로그 중 json으로 남기는 로그가 있었고, 서버에서 처리하는 요청수가 늘어남에 따라 로그 크기가 커지면서, 로그 크기를 줄여야하는 상황이 발생했다.   로그를 살펴보다보니, \"key\":\"value\" 를 구조로 된 json 포맷 특성상 의미없는 value(null, empty string, 빈 배열 등)를 갖는 필드에 대해서도 모두 키값이 기록되고 있음을 확인했고, 그 비율이 적지 않음을 확인했다.   value가 의미없는 경우 필드 자체를 남기지 않게 되면 로그 크기가 많이 줄어들 것으로 예상되어 작업에 착수했다.   다만, 필드별로 “의미없음”의 의미가 다를 수 있기 때문에 각 필드별로 “의미없음”의 기준에 따라 json에 포함하지 않는 것이 필요했다.   현황   일반적으로 jvm 계열 언어(자바, 코틀린, 스칼라 등)에서 데이터 직렬화/역직렬화에 많이 사용하는 jackson 라이브러리를 사용해서 POJO 객체(아래 예시에서는 LogEntry)를 json으로 직렬화하고 있었다.  기존 코드 상에는 POJO 클래스 전체에 대해 JsonInclude.Include.NON_NULL 정도만 적용이 되어 있었다.  @JsonInclude(JsonInclude.Include.NON_NULL)   public class LogEntry { \t// 생략 \tprivate final String name; \t// 생략 }   그렇기 때문에 필드별로 세분화된 JsonInclude 정책은 따로 없었고, 필드별 세분화된 설정을 위해서 코드 상에서 null로 바꿔주는 식으로 구현이 되어있었다.  예를 들어, String 타입의 특정 필드가 empty string(““)인 경우 포함하지 않는 요구사항을 위해서 아래와 같이 null로 변환하여 넣어주는 식이었다.   this.name = \"\".equals(name)? null : name;   위와 같은 코드는 비즈니스 로직이라기보다 단순히 직렬화에 포함시키지 않기 위한 목적의 코드라고 생각이 들었고,  코드의 가독성 측면에서도 JsonInclude라는 명시적인 애너테이션을 사용하는 것이 self-documenting 관점에서 더 직관적일 것 같다는 생각이 들었다.   그래서 JsonInclude의 구현을 찾아 들어가보니 NON_NULL 외에도 활용이 가능한 여러가지 enum이 있다는 것을 알게 됐다.  이 enum을 활용한다면, 코드 상에서의 별도 처리 없이 필드별 json 직렬화 기준을 오버라이딩할 수 있을 것 같다는 생각에 관련된 java doc을 자세히 살펴보았다.   JsonInclude enum 종류   JsonInclude안에 들어갈 수 있는 값들은 기본적으로 “json에 어디까지 포함할지”에 그 범위에 대한 enum 타입들이며, 직렬화 단계에서만 적용되는 애너테이션이다.  (처음에는 역직렬화 단계에서는 어떻게 적용이 되는걸까 생각했으나, 역직렬화 단계에서는 사실 필요가 없다. 필드가 없으면 역직렬화 단계에서 해당 프로퍼티 타입의 디폴트 값이 될 것이고, 필드가 있다면 그대로 역직렬화를 하면 되기 때문이다. 역직렬화시 관련된 애너테이션에는 @JsonIgnoreProperties가 있다.)   아래에 jackson 라이브러리의 현재 기준 latest인 2.16.1 버전 기준으로, 사용가능한 JsonInclude의 enum 타입들에 대한 내용을 정리했다.  아래로 갈수록 json에서 제외되는 기준이 확장된다(혹은 포함될수 있는 기준이 타이트해진다)고 볼 수 있다.   1. ALWAYS     무조건 json에 포함한다.   별도로 지정하지 않았을때의 디폴트값이다.            jackson-annotations/src/main/java/com/fasterxml/jackson/annotation/JsonInclude.java at e1910b8b7bfda325d450a30f1a121b5cf75d819a · FasterXML/jackson-annotations · GitHub           2. NON_NULL     명시적인 null일때만 제외   당연하게도, null이 될 수 없는 원시타입에 대해서는 ALWAYS와 동일하게 동작한다.   3. NON_ABSENT     NON_NULL 기준을 상속하며,   컨테이너 타입(Optional, AtomicReference 등)의 내부가 비어있을 때 제외한다.            예를 들면, Optional의 isEmpty()인 경우 제외           주로 Optional 타입과 함께 사용한다.   2.6부터 사용 가능   4. NON_EMPTY     NON_ABSENT 기준을 상속하며,   컬렉션, 배열, String이 empty인 경우 제외한다.            Collection, Map의 isEmpty 인 경우       자바 배열 length==0       empty String(“”)           단, jackson 2.6에 대해서는 예외사항이 있음            2.6 버전에서는 원시타입 등에 대해 empty의 범위가 확장되었으나(예를 들면 int의 0도 empty로 포함하였으나), 2.7 버전에서 롤백되었음.       2.6을 제외한 버전에서는 위와 같은 경우 NON_DEFAULT를 사용           5. NON_DEFAULT     클래스(POJO)에 적용된 경우와 그렇지 않은 경우로 동작이 나뉜다.   클래스에 적용된 경우            디폴트 POJO 객체(Zero Argument 생성자에 의해 생성된 객체)와 equals() 연산결과가 true를 리턴하는 경우 제외           클래스 외에 적용된 경우(global 설정 또는 프로퍼티에 적용된 경우)            NON_EMPTY 기준을 상속하며,       원시타입 및 Wrapper 타입의 디폴트 값(ex. int의 0)인 경우 제외.       Date/Time 중 0L에 대응되는 경우 제외.           6. USE_DEFAULTS     실제 동작에는 영향을 주지 않고, Inclusion에 대한 디폴트 설정을 따르겠다는 것을 명시적으로 설정할때 사용한다.   즉, 명시하지 않은 것과 동일하게 동작한다. (명시하지 않아도 디폴트를 따를테니)   그럼에도 해당 값이 필요한 이유는 해당 값을 사용하게 되면, 디폴트 사용을 강제할 수 있기 때문에 다른 개발자가 inclusion 설정을 오버라이딩하는 것을 방지할 수 있다.   그리고 디폴트를 사용하겠다는 것을 명시적으로 선언하는 것이므로, 문서화로서의 의미도 있다.   실제 적용   위 내용을 바탕으로, 실제로는 아래 세 가지를 적용했다.     NON_NULL            대부분의 경우 NON_NULL로 충분했기 때문에, (명시적으로 null인 경우가 실제로 로그에도 남길 필요 없는 경우였다.)       클래스 단위에 주로 적용했다.       클래스 단위에 적용하는 경우, 해당 클래스 내 모든 프로퍼티는 그 속성을 따라가기 때문에 클래스 내 디폴트 설정으로 사용하기 적합했다.       그리고 그 외에 추가로 직렬화 조건이 필요한 프로퍼티의 경우 아래 두가지로 오버라이딩 하는 방식을 채택했다.           NON_EMPTY            로그 내 List 타입의 필드가 있었는데, empty List 인 경우에는 남기지 않기 위해 적용했다.       오히려 String 타입의 프로퍼티의 경우, empty string(““)인 경우에 명시적으로 남겨야하는 경우가 많아서 NON_EMPTY 대신 NON_NULL을 적용했다.           NON_DEFAULT            원시 타입(int)에 대해 0인 경우에는 남기지 않도록 하기 위해 적용했다.       참고로, 0과 null을 구분해야하고, 0인 경우 로그에 남아야하는 경우에는 wrapper type + NON_NULL의 조합을 채택했다.           실제 적용한 최종 결과를 요약하면 아래와 같은 형태이다.   @JsonInclude(JsonInclude.Include.NON_NULL)   public class LogEntry {  \t@JsonProperty(\"tid\")   \tprivate final String transactionId;    \t@JsonProperty(\"price\")   \tprivate final Double price;  // 0.0과 null을 구분하고, 0.0은 로그에 남겨야하는 경우  \t@JsonInclude(JsonInclude.Include.NON_DEFAULT)   \t@JsonProperty(\"et\")   \tprivate final int elapsedTime;  \t@JsonInclude(JsonInclude.Include.NON_EMPTY) \t@JsonProperty(\"res_list\") \tprivate final List&lt;Response&gt; responseList; }   그리고 위 내용을 적용 결과, 해당 설정만으로 로그 사이즈를 기존대비 25%나 줄일 수 있었다!   References      GitHub - FasterXML/jackson-annotations at jackson-annotations-2.16.1   https://www.javadoc.io/doc/com.fasterxml.jackson.core/jackson-annotations/2.16.1/index.html   [SpringMVC] 업무에서 활용한 @JsonInclude 사용법 정리  ","categories": ["Java"],
        "tags": ["Java"],
        "url": "https://dreamsh19.github.io/java/JsonInclude%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%82%AC%EC%9D%B4%EC%A6%88-%EC%B6%95%EC%86%8C/",
        "teaser": null
      },{
        "title": "Java Optional Deep Dive",
        "excerpt":"자바에서 Optional을 사용하면서, Optional의 설계 철학과 사용법을 제대로 알고 사용하고 싶어서 자세히 찾아보게 되었다.   1. Optional은 왜 등장했는가?   자바 개발자라면 모를수가 없는 악명 높은 예외가 바로 NullPointerException(이하 NPE)이다.  널 참조는 billion dollar mistake 라고 불릴만큼, 악명 높은 디자인 실수(?)인데, 그 이유는 개발자의 실수에 의해 error-prone하기 때문이지 않을까 싶다.  Optional은 이러한 의도치않은 NPE를 방지하고자 등장했는데, 여러가지 자료를 찾아보고 개인적으로 정리한 Optional의 등장 배경은 크게 아래 두가지인 듯하다.   API 디자인의 명확성 확보 및 NPE 방지   Optional의 설계의도와 관련하여 자바 아키텍트 Brian Goetz가 한 말이라고 한다.     Optional is intended to provide a limited mechanism for library method return types where there needed to be a clear way to represent “no result,” and using null for such was overwhelmingly likely to cause errors.    개인적으로 이 말이 Optional 설계의도를 가장 잘 드러내는 말이라고 생각한다.  저기서 주목한 점은 Optional의 애초에 “메소드의 리턴 타입”을 위해 설계되었다는 점이다. (후술할 Optional의 잘못된 사용과 연관되어 있는 부분이기도 하다.)   Optional이 없던 시절에는 메소드 리턴을 “결과 없음”을 표현하기 위해서는 null을 리턴하는 경우가 종종 있었고,  null을 리턴할 수 있다는 것 자체가 caller 쪽에 예외처리의 책임을 전가하는 것이기 때문에 NPE에 취약할 수 밖에 없다.   하지만 메서드가 Optional을 리턴한다면, caller 입장에서는 두 가지가 달라진다.  첫번째로, 메서드 결과가 empty가 될 수 있음을 명시적으로 알 수 있고,  두번째로, 어쨌든 값을 꺼내서 쓰기 위해서는 Optional을 unwrap하는 과정이 강제되기 때문에, 기존의 널 체크와 유사한 로직이 강제된다.   오라클의 Optional 관련 글 중에 Optional의 의도를 잘 이해할 수 있는 부분이 있어 발췌하였다.     It is important to note that the intention of the Optional class is not to replace every single null reference. Instead, its purpose is to help design more-comprehensible APIs so that by just reading the signature of a method, you can tell whether you can expect an optional value. This forces you to actively unwrap an Optional to deal with the absence of a value.    코드의 가독성   String version = computer.getSoundcard().getUSB().getVersion();   위와 같은 코드가 있다고 가정해보자. 비즈니스 로직 상에서 흔하게 볼 수 있는 로직의 형태이다.  그러나, null-safe한 코드 작성을 위해서는 아래와 같이 작성해야한다.  String version = \"UNKNOWN\"; if(computer != null){   Soundcard soundcard = computer.getSoundcard();   if(soundcard != null){     USB usb = soundcard.getUSB();     if(usb != null){       version = usb.getVersion();     }   } }  더 길어진다면, 코드에 점점 indentation이 길어지고, 코드의 가독성은 점점 더 떨어질수 밖에 없다.  하지만 Optional은 메소드 체이닝을 지원하여 위와 같은 코드를 아래와 같이 작성할 수 있게 된다.   String version = computer.flatMap(Computer::getSoundcard)                           .flatMap(Soundcard::getUSB)                           .map(USB::getVersion)                           .orElse(\"UNKNOWN\");   2. Optional의 생성   Optional 클래스의 프로퍼티는 아래와 같이 구현되어있다.  public final class Optional&lt;T&gt; {    \tprivate static final Optional&lt;?&gt; EMPTY = new Optional&lt;&gt;();    \tprivate final T value;  }   Optional은 기본적으로 컨테이너일 뿐이기 때문에, 프로퍼티로 담고 있을 레퍼런스 하나가 전부이다.  (스태틱 필드로 EMPTY 인스턴스가 있긴 하지만, Optional의 의도와는 별개로 싱글턴 객체를 공유함으로써 시간 및 공간상 효율을 위한 프로퍼티이다.)   그리고 Optional의 생성자는 아래 단 두개 뿐이며, 둘다 private이다.  private Optional() {   \tthis.value = null;   }  private Optional(T value) {   \tthis.value = Objects.requireNonNull(value);   }  이는 곧, 직접적인 생성자 호출을 통한 생성을 금지하려는 의도이다. 그렇다면, 실제 Optional 인스턴스를 생성하기 위해서는 어떻게 해야할까?   이를 위해서 Optional 클래스에서는 아래 세가지 static 메소드를 제공한다.  public static &lt;T&gt; Optional&lt;T&gt; of(T value) {   \treturn new Optional&lt;&gt;(value);   }  public static&lt;T&gt; Optional&lt;T&gt; empty() {   \t@SuppressWarnings(\"unchecked\")   \tOptional&lt;T&gt; t = (Optional&lt;T&gt;) EMPTY;   \treturn t;   }  public static &lt;T&gt; Optional&lt;T&gt; ofNullable(T value) {   \treturn value == null ? empty() : of(value);   }   3. 올바른 사용법     map, filter 등의 각종 메소드는 사실 직관적이기 때문에 따로 정리할 필요는 없을 듯하고,   다만, 잘못 사용하는 경우 주의가 필요하여 정리하고자 한다.   3.1. 함수 파라미터로 Optional 사용   이는 사실 앞에서 기술한 Optional의 등장 배경과 철학을 알지 못하는 경우에 사용하기 쉽다.  아래와 같이 파라미터로 Optional을 받는 메서드(또는 생성자)가 있다고 가정해보자.  얼핏보면, Optional 이라는 이름에 걸맞게 attachment가 있는 경우와 없는 경우를 잘 표현한 듯 보인다.  public SystemMessage(String title, String content, Optional&lt;Attachment&gt; attachment) { \t// 생략 \tattachment.ifPresent(System.out::println); }   그러나, 이는 Optional의 의도상 지양하는 코드이다.  그 이유는 위와 같은 메서드는 아래와 같은 코드를 호출가능하게 만들고, 이는 결국 런타임에 NPE가 발생할 수 밖에 없다.  SystemMessage(\"title\", \"content\", null)     NPE를 방지하기 위해 만든 Optional을 잘못 사용하여 오히려 NPE가 발생하는 꼴이다.   3.2. 직렬화   Optional은 Serializable 인터페이스를 구현하지 않는다. 즉, 직렬화를 하려고 하면 NotSerializableException이 발생한다.  이 역시 사람들이 “null이 될 수 있는”의 의미로 Optional을 사용하면서 발생한 오해(?)이다. 사람들은 Optional을 클래스 내에서 “null이 될 수 있는 프로퍼티”로 사용하려고 했고, 그러면서 자연스럽게 직렬화에 대한 니즈가 발생했다.  그럼에도 현재까지 Optional에 Serializable 구현을 추가하지 않았는데, 그 이유는 Optional을 “메서드의 리턴타입”으로만 사용하게 하는 것이 본래 그 의도에 맞다고 판단했기 때문이라고한다.     Shouldn’t Optional be Serializable?   3.3. 불필요한 orElse() 사용   Optional는 orElse()와 orElseGet() 두가지를 제공한다. 얼핏보면 비슷해보이는데, 두 가지의 차이는 무엇일까?   두 가지의 차이점은 “lazy하게 동작하는가”에 차이가 있다. 구현을 보면 알 수 있다.   public T orElse(T other) {   \treturn value != null ? value : other;   }  public T orElseGet(Supplier&lt;? extends T&gt; other) {   \treturn value != null ? value : other.get();   }   orElseGet()은 Optional이 null이 아닐때만 lazy하게 결과를 가져온다.  하지만, orElse()는 함수 호출 이전에 이미 파라미터로 값을 넣어주어야하기 때문에 null 여부와 무관하게 항상 연산이 발생하게 되고, null이 아닌 경우에는 값이 쓰이지도 않기 때문에 이는 낭비이다.  만약 그 연산의 비용이 비싼 연산이라면(예를 들면, db와 통신 등) 불필요한 비용을 계속 발생시키게 된다.   4. 한계   개인적으로 생각한 Optional의 한계는 아래 두 가지인듯하다.     시간과 공간상 오버헤드가 발생한다            아무래도, wrap/unwrap 하는 과정이 필요하고 추가적인 메모리할당이 필요하기 때문에 오버헤드는 불가피해보인다.           완전히 강제할 수 없다.            개발자가 위의 “사용법”을 유념하고 사용해야한다는 것 자체가 한계로 보인다.       기존에 null을 사용하던 것에 비해서는 훨씬 에러 가능성이 줄어들었지만, 여전히 Optional.ofNullable(null).get() 과 같은 형태의 코드를 컴파일 타임에 잡아낼 수 없다.       이는 결국 Optional이 자바 언어 레벨에서 지원하는 것이 아니라 단순 코드 레벨에서 제공하기 때문에 발생할 수 없는 한계가 아닐까 생각한다.           References      Tired of Null Pointer Exceptions? Consider Using Java SE 8’s Optional!   Null safety - Kotlin Documentation   Optional (Java Platform SE 8 )   serialization - Why java.util.Optional is not Serializable, how to serialize the object with such fields - Stack Overflow   [Java] 언제 Optional을 사용해야 하는가? 올바른 Optional 사용법 가이드 - (2/2) - MangKyu’s Diary   Guide To Java 8 Optional - Baeldung   Shouldn’t Optional be Serializable?  ","categories": ["Java"],
        "tags": ["Java"],
        "url": "https://dreamsh19.github.io/java/Java-Optional-Deep-Dive/",
        "teaser": null
      },{
        "title": "Airflow 00시 배치 실패 해결기(feat. render_template_as_native_obj 옵션)",
        "excerpt":"이슈     Airflow 상에서 매시간(hourly) 도는 DAG가 어느 순간부터 00시대에만 항상 실패하는 이슈가 있었다.   해당 태스크는 실패하더라도, 플랜 B가 동작하기 때문에 서비스에 지장은 없었지만, 반복적으로 00시에 실패하는 것이 우연이 아닐 것이라 생각하여 자세히 살펴보기 시작했다.   원인 파악     문제가 된 DAG 내 태스크는 하둡 특정 시간대의 디렉토리의 파일 존재여부를 체크하는 PythonSensor 오퍼레이터로 작성된 태스크였다.   구체적으로는, 아래와 같은 형태였다.            이때 날짜(date)와 시간(hour)은 배치 수행시간(data_interval_start)에서 파싱해서 가져온다.           def check_flag_file(date, hour): \tfile_path=f'{conf.hdfs}/dt={date}/hr={hour}/_*' \t// 후략      check_\bsuccess = PythonSensor( \ttask_id='check_\bsuccess', \tpython_callable=check_flag_file, \top_kwargs={\"date\": date, \"hour\": hour}, \tpoke_interval=10, \t// 후략 )      그래서 실패한 자정 시간대의 태스크 기록을 보니, 아래와 같이 ‘00’의 형태가 아닌 ‘0’으로 들어가고 있었다.         타겟 디렉토리명은 시간이 두자리로 포맷팅된 형태였고, hr=00 이 아닌 hr=0 디렉토리를 조회하니, 항상 존재하지 않아 태스크가 실패했던 것이다.   그럼 직접적인 원인은 확인했으니, 왜 ‘00’이 아닌 ‘0’으로 들어가고 있는지 더 자세히 알아보기 시작했다.   int 타입의 포맷팅 이슈가 아닐까?  00이 0으로 바뀌었으니, 처음으로 든 생각은 int 타입의 변수가 어떠한 이유로 한자리로 포맷팅되면서 발생한 이슈가 아닐까?라는 생각이었다.  그러나, 단순 포맷팅 문제라면, 00시 뿐만 아니라, 01시, 02시부터 09시까지 모두 문제가 됐어야했다.  하지만 실제로 문제가 된건 00시 뿐이었고, 01시~09시까지는 의도대로 두자리수로 처리되어 문제가 없었다.  그래서 단순 포맷팅 이슈는 아닐꺼라고 판단하여 구체적으로 변수가 변환되는 과정을 추적하기 시작했다.   Airflow 템플릿 내부 동작   위의 태스크 기록을 봤을때 kwargs로 들어갈때 이미 ‘0’이 들어갔다는 것은 곧 템플릿이 렌더링된 시점에 이미 ‘0’이 되어버린 것이다. 그래서 airflow의 템플릿의 동작 방식을 알아보기 시작했다.  airflow는 템플릿 문법을 지원하며, 내부적으로 템플릿 엔진으로 jinja를 채택하고 있다.     https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/operators.html#jinja-templating   그래서 문서를 읽던 중, render_template_as_native_obj 옵션에 대한 이야기가 있었고, 이 내용이 눈에 띄었다. 이 옵션이 눈에 띈 이유는 최근에 해당 DAG에 render_template_as_native_obj=True 옵션을 적용하는 배포가 있었기 때문이다.  해당 옵션은 템플릿화된 변수를 렌더링할때, 단순문자열(디폴트 설정, render_template_as_native_obj=False)로 렌더링할 것이냐, 아니면 문자열을 파싱해서 파이썬 내장 객체로 렌더링할 것이냐를 지정하는 옵션이다.  공식 문서에서 설명하고 있는 사용 예시는, dict 형태의 input을 전달받고 싶을때 해당 옵션을 켜줌으로써 string -&gt; dict로의 변환을 개발자가 아닌 airflow 설정만으로 가능하게 한다.      그래서 최근에 해당 옵션이 적용된 만큼, 실제 설정을 적용했을때의 내부 동작을 좀더 확인해보기 시작했다.   render_template_as_native_obj 옵션을 켜면, 내부적으로 jinja2의 NativeEnvironment의 render() 함수를 활용하여 파이썬 내장객체를 리턴한다.  그래서 NativeEnvironment의 render() 함수의 소스코드를 살펴보았고, 일부를 발췌했다.   // 전략 try: \treturn literal_eval(raw) except (ValueError, SyntaxError, MemoryError): \treturn raw     jinja/src/jinja2/nativetypes.py at aa3d688a15aece0a0de0b59f94dda870c724bc87 · pallets/jinja · GitHub   결국 내부적으로 ast.literal_eval() 함수를 호출하여 문자열을 내장객체로 파싱하고, 예외가 발생(파싱에 실패)하는 경우 raw, 즉 문자열 원본을 그대로 리턴하는 로직이다.  (ast는 Abstract Syntax Trees를 의미하는 파이썬 내장 패키지이며, 일종의 파이썬 문법을 파싱하는 패키지정도로 이해했다.)   그렇다면, ast.literal_eval() 함수의 리턴 결과를 직접 확인해보기로 했다. (현재 팀에서 airflow를 구성하는데 사용한 python 3.6.5 버전으로 확인했다)   [root@server ~]$ python3 Python 3.6.5 (default, May  8 2018, 12:10:43) [GCC 4.4.7 20120313 (Red Hat 4.4.7-18)] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. &gt;&gt;&gt; import ast &gt;&gt;&gt; res = ast.literal_eval('00') &gt;&gt;&gt; res 0 &gt;&gt;&gt; type(res) &lt;class 'int'&gt;   확인 결과, 문제가 됐던 상황처럼 문자열 ‘00’이 int 0으로 파싱되었다!   그렇다면 ‘01’은 int로 파싱이 안되는 것일까?   &gt;&gt;&gt; res = ast.literal_eval('01') Traceback (most recent call last):   File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;   File \"/usr/local/lib/python3.6/ast.py\", line 48, in literal_eval     node_or_string = parse(node_or_string, mode='eval')   File \"/usr/local/lib/python3.6/ast.py\", line 35, in parse     return compile(source, filename, mode, PyCF_ONLY_AST)   File \"&lt;unknown&gt;\", line 1     01      ^ SyntaxError: invalid token   ‘01’은 파싱에 실패하여 SyntaxError가 발생하였다.  그리고 위에 발췌한 NativeEnvironment의 render()의 구현에 따르면, SyntaxError가 발생한 경우 원본 문자열인 ‘01’을 리턴하게 된다.  이것을 통해 ‘00’과 ‘01’의 파싱 결과가 다르다는 사실을 확인했다.   결국 이것을 확인함으로써 모든 것이 명확해졌다.  render_template_as_native_obj=True 옵션을 적용하면서 템플릿을 렌더링하는 로직이 달라졌고,  해당 옵션을 적용했을때의 템플릿 엔진이 문자열 ‘00’을 int 0으로 파싱하게 되면서 00시에는 오류가 발생했고,  문자열 ‘01’은 파싱에 실패하여 문자열 원본 ‘01’이 그대로 리턴되면서 그외의 시간에는 문제가 발생하지 않았던 것이다.   해결   render_template_as_native_obj 옵션이 명확하게 원인인 것을 알았으니, 다시 사용하지 않는게 명확한 해결책이지만, 해당 옵션은 다른 태스크에서 활용하고 있었기 때문에 이 해결책은 기각하였다.   결국 우리에게 필요한 것은 아래와 같으므로,     int 0이 들어왔을때 00을 리턴한다.   string ‘01’~’23’이 들어왔을때 01~23을 리턴한다.   아래와 같이 명시적인 “형변환 후 포맷팅” 로직을 적용하여 해결하였다.   def check_flag_file(date, hour): \tfile_path=f'{conf.hdfs}/dt={date}/hr={int(hour):02}/_*' \t// 후략     결국 해결은 render_template_as_native_obj 옵션을 활용하진 않았지만, 해당 옵션에 대한 이해가 부족했다면, 적용하지 못했을 해결 방법이다.   추가적인 궁금증   위 내용을 통해 ast.literal_eval() 이 이 문제의 범인임은 알았다. 그렇다면 ‘00’은 int 0으로 파싱하면서 ‘01’은 파싱을 못하는 건 ast의 스펙인걸까?  그래서 ast.literal_eval() 소스코드를 좀 더 살펴보기 시작했다.  결국 ast 역시 파이썬 내장 compile() 함수에 파싱을 위임하고 있었다.       cpython/Lib/ast.py at v3.6.5 · python/cpython · GitHub   그럼 결국 이건 ast의 스펙이 아니고, 파이썬의 스펙이라는 것인데, 그래서 아래와 같이 확인해봤다.  [root@server ~]$ python3 Python 3.6.5 (default, May  8 2018, 12:10:43) [GCC 4.4.7 20120313 (Red Hat 4.4.7-18)] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. &gt;&gt;&gt; a=00 &gt;&gt;&gt; a 0 &gt;&gt;&gt; type(a) &lt;class 'int'&gt; &gt;&gt;&gt; a=01   File \"&lt;stdin&gt;\", line 1     a=01        ^ SyntaxError: invalid token   확인결과, 파이썬의 스펙인듯하다. 파이썬 문법상 리터럴 00은 int 0으로 파싱되지만, 리터럴 01은 파싱에 실패한다.   이걸 이상하게 생각하는 건 나뿐만이 아니었던 듯하다. 관련해서 구글링을 해보니 아래와 같은 글을 찾을 수 있었다.  Why does Python 3 allow “00” as a literal for 0 but not allow “01” as a literal for 1? - Stack Overflow   요약하자면, \"0\"+이 스페셜 케이스이고, 이걸 도입했을 당시의 명확한 이유가 기억이 나지 않는다고 한다;;  그래서 많은 사람들이 이러한 스페셜 케이스를 없애자고 제안차 버그 리포트를 올렸으나, 반영이 되지 않아 현재까지 \"0\"+ 리터럴은 0으로 파싱되고 있다고 한다.      References      Operators — Airflow Documentation   Native Python Types — Jinja Documentation (2.11.x)   jinja/src/jinja2/nativetypes.py at aa3d688a15aece0a0de0b59f94dda870c724bc87 · pallets/jinja · GitHub   cpython/Lib/ast.py at v3.6.5 · python/cpython · GitHub   ast — Abstract Syntax Trees — Python 3.12.2 documentation   Why does Python 3 allow “00” as a literal for 0 but not allow “01” as a literal for 1? - Stack Overflow  ","categories": ["Airflow"],
        "tags": ["Airflow","Python"],
        "url": "https://dreamsh19.github.io/airflow/Airflow-00%EC%8B%9C-%EB%B0%B0%EC%B9%98-%EC%8B%A4%ED%8C%A8-%ED%95%B4%EA%B2%B0%EA%B8%B0(feat.-render_template_as_native_obj-%EC%98%B5%EC%85%98)/",
        "teaser": null
      },{
        "title": "로컬에-HTTPS-웹서버를-도메인과-함께-띄워보자(feat. mkcert)",
        "excerpt":"이슈      광고 마크업을 테스트하기 위한 환경 구축이 필요했다.            광고 마크업은 그 특성상, first party 사이트에 들어가지 않고 third party로 매체 사이트에 들어가게 된다.       그렇기 때문에 테스트를 하기 위해서 실제 광고 마크업이 들어가게 될 first party 사이트, 즉 매체 사이트를 구축할 필요가 있었다.           이때 단순 로컬 html 파일로 테스트하지 않은 이유는, 브라우저 입장에서 실제 매체 사이트와 구분이 불가능해야했다.            실제로 로컬 html 파일에서 문제가 없더라도, 인증서 이슈나 도메인으로 인한 CORS 이슈 등의 문제가 브라우저 상에서 발생할 수 있기 때문이다.           물론 직접 도메인 및 인증서를 발급받아서 매체 사이트를 구축하는 것도 가능하기는 하나            비용 문제가 있기도 하고,       수정 -&gt; 브라우저 렌더링 테스트 의 과정이 빈번하게 반복되어야하는데, 로컬 파일에서 수정한 것을 즉시 확인할 수 있는 환경이 필요했다.           위와 같은 니즈로 인해 로컬에 HTTPS 서버를 도메인과 함께 구동할 수 있는 방법에 대해 살펴보고 정리하였다.   로컬 환경   아래에서 기술할 로컬 환경은 macOS m1 및 python3을 기준으로 작성하였다.  그리고 아래에서 기술할 시나리오는 my-little-domain.com 도메인으로 HTTPS 서버를 띄우는 것을 목표로 작성한 시나리오이다.  아래 시나리오 상에서 my-little-domain.com 대신, 필요한 도메인으로 대체하면 된다.   로컬 HTTP 서버 구동   인증서가 필요없는 http 서버는 아래와 같은 한 줄짜리 파이썬 코드를 통해 구동할 수 있다. (8000번 포트)  python3 -m http.server 8000 # python3 기준   python -m SimpleHTTPServer 8000 # python2 기준   위 명령어를 실행하게 되면, 해당 명령이 실행된 디렉토리의 파일들을 http://localhost:8000 에 접근하여 브라우저에서 볼 수 있다.  하지만, http로 서비스되는 매체 사이트는 이제 거의 없다고 봐야하기 때문에 같은 기능을 하는 https 매체 사이트가 필요했다.   SSL 인증서 발급   HTTPS 서버를 띄우기 위해서는 인증서 발급이 선행되어야한다.   openssl  이때 openssl을 통해 아래와 같이 간단하게 인증서를 발급할 수 있다.   openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -days 365   하지만 위 인증서로 웹서버를 구동하는 경우 브라우저에서는 아래와 같이 워닝을 띄운다.      그럴 수 밖에 없는 것이 openssl을 통해 발급한 인증서는 공인 기관(CA, Certificate Authority)으로부터 검증된 인증서가 아니기 때문이다.  그래서 우리에게는 CA로부터 공인된 인증서가 필요하다.   mkcert   이걸 해주는 오픈소스 프로젝트가 바로 mkcert 이다.  mkcert는 프로젝트에 적혀있는 것처럼 “locally trusted development certificates”, 즉 로컬 한정 공인된 인증서를 만들어주는 오픈소스이다.     A simple zero-config tool to make locally trusted development certificates with any names you’d like.    mkcert를 우선 설치해주자.  os별 설치 방법은 readme에 자세히 나와있으니 참고하면 된다. 나는 brew를 통해 설치했다.   brew install mkcert   설치 후에 아래와 같은 명령어를 입력하여 로컬에 CA로 mkcert를 등록해준다.   mkcert -install   이때, 시스템 내 인증기관을 등록하는 일이기 때문에 비밀번호 입력이 필요하다.   위 명령어 실행 후 macOS 키체인 접근을 통해 확인해보면, 아래와 같이 루트 인증기관으로 mkcert가 등록되어 있음을 확인할 수 있다.      이제 mkcert가 발급한 인증서는 내 로컬 pc에서 유효한 인증서가 되는 것이다.  그럼 로컬에서 CA가 된 mkcert로부터 “localhost” 도메인과 “my-little-domain.com” 도메인을 위한 인증서를 발급받아보자.   mkcert -key-file key.pem -cert-file cert.pem localhost my-little-domain.com   위 명령어를 실행 후에 명령어를 실행한 디렉토리를 확인해보면, 아래와 같이 private key 파일(key.pem)과 인증서 파일(cert.pem)이 생성되었다.  [user@local]  ~/local-https-server  ls -al total 16 drwxr-xr-x   4 user  root   128  3 31 23:52 . drwxr-x---+ 35 user  root  1120  3 31 23:52 .. -rw-r--r--   1 user  root  1566  3 31 23:51 cert.pem -rw-------   1 user  root  1708  3 31 23:51 key.pem   이때 key 파일 및 인증서 파일의 경로 및 파일명은 이후에 웹서버를 구동할 때 필요하므로, 기억해두어야한다.  인증서가 준비되었으니 이제 HTTPS 웹서버를 구동해보자.   웹서버 구동   웹서버를 구동하는 방법은 apache, nginx 등등 정말 다양하다.  그러나 apache, nginx 등을 사용하는 건 일단 설치가 필요하고, 설치 후에도 conf 설정 방법에 대한 이해도 필요하다.  하지만 지금 당장 필요한건 단순히 로컬의 정적인 파일들을 서빙하기만 하면 된다.  그리고 다른 로컬 환경에서도 별도 의존성 없이 실행할 수 있는 방법이 좋을 것이다.  그래서 위와 같은 요구사항을 위해 macOS에서 별도 의존성 없이 실행할수 있는 python http.server를 사용하기로 했다.   아래와 같은 간단한 파이썬 코드(web_server.py)를 작성하고,   # web_server.py from http.server import HTTPServer, SimpleHTTPRequestHandler import ssl  # Specify the path to your SSL certificate and key ssl_certificate = 'cert.pem' ssl_key = 'key.pem'  # Create an HTTP server with SSL support httpd = HTTPServer(('localhost', 443), SimpleHTTPRequestHandler) httpd.socket = ssl.wrap_socket(httpd.socket, certfile=ssl_certificate, keyfile=ssl_key, server_side=True)  # Start the server print(\"Server started at https://localhost:443\") httpd.serve_forever()  (위 코드는 key 파일과 cert 파일과 같은 디렉토리에 작성하였으므로, 상대경로로만 파일들의 경로를 명시하였고, 인증서 파일과 별도 디렉토리에 작성하는 경우 그에 해당하는 절대 또는 상대 경로로 입력해주어야한다. )   아래 명령어를 통해 간단하게 localhost 443 포트에 HTTPS 웹서버를 구동할 수 있다.  sudo python3 web_server.py   이때, sudo가 필요한 이유는, HTTPS용 포트인 443 포트는 1024 미만의 Privileged Ports 이기 때문에 루트 권한이 필요하다.   파이썬 버전에 따른 차이   이렇게 끝인줄로만 알았는데…(그래서 사실 간단하다고 생각하고 있었는데..)  다른 pc에서 동일하게 환경을 구성하다가 AttributeError: module 'ssl' has no attribute 'wrap_socket' 에러와 함께 위 코드가 실패하는 것을 발견했다.   그래서 찾아보니, 파이썬 ssl 패키지의 socket을 생성하는 wrap_socket() 함수가 python 3.7 버전에서 deprecate 되었고, python 3.12 버전에서는 완전히 삭제되었다. (정확히는 사용 방법이 바뀐 것이다.)           Remove the ssl.wrap_socket() function, deprecated in Python 3.7: instead, create a ssl.SSLContext object and call its ssl.SSLContext.wrap_socket method. Any package that still uses ssl.wrap_socket() is broken and insecure. The function neither sends a SNI TLS extension nor validates server hostname. Code is subject to CWE-295: Improper Certificate Validation. (Contributed by Victor Stinner in gh-94199.)      기존에 테스트하던 pc 환경은 python 3.6 버전이어서 위 코드가 문제 없이 동작했던 것이고, 새로운 환경은 python 3.12 버전이어서 에러와 함께 실패한 것이다.   그래서 파이썬 버전에 따른 분기처리를 추가하였고, 수정한 최종 코드는 아래와 같다.   # web_server.py import sys  from http.server import HTTPServer, SimpleHTTPRequestHandler import ssl  # Specify the path to your SSL certificate and key ssl_certificate = 'cert.pem' ssl_key = 'key.pem'  # Create an HTTP server with SSL support     server_address = ('localhost', 443) httpd = HTTPServer(server_address, SimpleHTTPRequestHandler)  if sys.version_info &gt;= (3, 7): # For Python 3.7 and later      # Wrap the socket with SSL     context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)     context.load_cert_chain(certfile=ssl_certificate, keyfile=ssl_key)     httpd.socket = context.wrap_socket(httpd.socket, server_side=True)  else: # For Python versions earlier than 3.7          # Wrap the socket with SSL     httpd.socket = ssl.wrap_socket(httpd.socket, certfile=ssl_certificate, keyfile=ssl_key, server_side=True)  # Start the server print(\"Server started at https://localhost:443\") httpd.serve_forever()   그래서 위 코드는 파이썬 3점대 버전에서 마이너 버전과 무관하게 동작하게 된다.   어쨌든 원래 목적으로 돌아와, 다시 아래와 같은 명령어를 통해 웹 서버를 구동할 수 있고,  sudo python3 web_server.py   https://localhost 에 접근하여 브라우저에서 해당 디렉토리의 파일들을 볼 수 있다.      호스트 파일(/etc/hosts) 수정   사실 위의 과정까지만 하면 필요한 것은 모두 된 것이다. localhost 라는 도메인의 HTTPS 웹서버를 띄운 셈이기 때문이다.  하지만 좀 더 매체 환경과 비슷하게 만들어주기 위해 localhost 의 도메인(이 글에서는 my-little-domain.com)을 통해 접근할 수 있도록 만들어보자.  이를 위해서 로컬 DNS에 해당하는 호스트 파일 /etc/hosts(macOS 및 linux 기준)에 my-little-domain.com을 로컬호스트에 바인딩하기 위한 설정을 추가하자.   echo '127.0.0.1 my-little-domain.com' | sudo tee -a /etc/hosts      이때 /etc/hosts 파일 수정은 root 권한이 필요하므로, output redirection(»)으로 수정할 수 없고, sudo tee -a 로 추가해주어야 한다.   혹은 직접 root 권한으로 파일을 에디터로 열어 수정해도 된다   결과 확인   이제 https://my-little-domain.com에 접근하여 브라우저에서 로컬 파일들을 볼 수 있다! 그리고 브라우저에서도 더 이상 인증서 관련 warning을 띄우지 않고 인정(?)해준다.         사실 이때부터는 브라우저 입장에서는 실제 매체 사이트랑 구별을 할 수가 없게 된다.  브라우저는 여느 사이트와 동일하게 https://my-little-domain.com 에 바인딩된 서버에 조회하여 결과를 가져올 뿐이기 때문에, 그 서버가 어떻게 구성되었는지는 알 길이 없고 알 필요도 없다.   이로써 매체 사이트를 로컬에 mocking하여 로컬 파일 기반으로 자유롭게 마크업 테스트를 할 수 있는 환경을 만들어 보았다.   주의사항   당연하게도, mkcert를 통해 발급한 인증서는 로컬에서만 유효한 인증서이므로, production 환경에서는 CA를 통해 발급받은 인증서를 사용해야한다.   References     GitHub - FiloSottile/mkcert: A simple zero-config tool to make locally trusted development certificates with any names you’d like.   Privileged Ports   What’s New In Python 3.12 — Python 3.12.2 documentation  ","categories": ["HTTPS"],
        "tags": ["SSL","HTTPS","local"],
        "url": "https://dreamsh19.github.io/https/%EB%A1%9C%EC%BB%AC%EC%97%90-HTTPS-%EC%9B%B9%EC%84%9C%EB%B2%84%EB%A5%BC-%EB%8F%84%EB%A9%94%EC%9D%B8%EA%B3%BC-%ED%95%A8%EA%BB%98-%EB%9D%84%EC%9B%8C%EB%B3%B4%EC%9E%90(feat.-mkcert)/",
        "teaser": null
      },{
        "title": "Java의 Integer도 Pool이 있다",
        "excerpt":"아래 자바 코드 실행 결과가 이해가 된다면 이 글을 스킵해도 된다.   private boolean isBoxedIntegerSame(int i) {       Integer i1 = i;       Integer i2 = i;     return i1 == i2;   }  @Test public void testIntegerSame(){ \tSystem.out.println(isBoxedIntegerSame(127)); // true \tSystem.out.println(isBoxedIntegerSame(128)); // false }      isBoxedIntegerSame() 함수의 결과값이 true 혹은 false 중 하나의 값으로 예측했지만 틀렸다면, 자바의 동등 비교(==)에 대한 이해나, Wrapper 타입의 boxing에 대한 이해가 부족했다고 생각하고 이 부분을 좀 더 찾아볼 수 있다.   그러나, 위 예시처럼 결과 값이 인풋에 따라서 달라지는 것은 위 두가지에 대한 이해만으로는 부족하다.   그래서 이 값이 달라지는 이유에 대해서 다루고자 한다.   Autoboxing   우선 몇줄 안되는 isBoxedIntegerSame() 함수의 코드를 살펴보자.  Integer i1 = i  와 같은 코드를 작성하게 되면, 컴파일러에서는 원시 타입인 int를 wrapper 타입인 Integer로 변환하기 위해 autoboxing을 수행한다.  이때 컴파일러는 Integer.valueOf(int i) 함수를 이용하게 된다.  그러므로 위와 같은 코드는 컴파일러에 의해  Integer i1 = Integer.valueOf(i)   와 같은 코드로 변환된다.   그렇다면 Integer.valueOf() 함수의 내부 구현을 살펴보자   Integer.valueOf 내부 구현   아래는 Integer.valueOf()의 구현 전문이다.  /**    * Returns an {@code Integer} instance representing the specified    * {@code int} value.  If a new {@code Integer} instance is not    * required, this method should generally be used in preference to  * the constructor {@link #Integer(int)}, as this method is likely  * to yield significantly better space and time performance by   * caching frequently requested values.   *   * This method will always cache values in the range -128 to 127,   * inclusive, and may cache other values outside of this range.   *   * @param  i an {@code int} value.    * @return an {@code Integer} instance representing {@code i}.    * @since  1.5    */  public static Integer valueOf(int i) {       if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high)           return IntegerCache.cache[i + (-IntegerCache.low)];       return new Integer(i);   }     jdk/src/java.base/share/classes/java/lang/Integer.java at c1c99a669bb7f9928086db6a4ecfc90c410ffbb0 · openjdk/jdk · GitHub   놀랍게도, 단순히 new Integer()를 호출하는게 아니고, 캐시의 개념이 들어가 있는 것을 확인할 수 있다!  구현을 보니, 특정 구간인 경우에 캐시에서 꺼내오고 그렇지 않으면 새로운 인스턴스를 생성하는 것으로 보인다.  이때 특정 구간이라함은, 위 자바독에 따르면 -128 ~ 127 (inclusive)에 해당한다.  캐시이기도 하고, 객체 재사용을 위해 사전에 미리 만들어놓은 풀(pool)의 개념이기도 하다.   이제 글 도입부의 실행결과를 이해할 수 있다.  127은 Integer 풀에 속한 구간이므로, Integer 인스턴스를 항상 풀에서 꺼내오기 때문에 늘 같은 인스턴스를 참조하게 되고. 그렇기 때문에 동등 연산을 수행하면 true가 나온다.  하지만, 128은 Integer 풀을 벗어난 구간이므로, 매번 새로운 인스턴스를 생성하게 되어 동등 연산을 수행하게 되면 false가 된다.   JLS에 명시된 스펙   사실 특정 구간의 Integer에 대해 pool을 만들어두고 재사용하는 것은 시간과 공간 성능 상의 이유겠거니라고 생각했는데(자바의 인스턴스 생성은 비싼 연산이므로..)  이유는 성능 상의 이유가 맞지만, 관련해서 찾아보다보니 해당 구간의 Integer를 동일한 인스턴스를 참조하게 하는 것은 자바 언어의 스펙이었다.   JLS(Java Language Specification) 문서 중 Boxing conversion 파트에 아래와 같은 내용이 있다.     If the value p being boxed is true, false, a byte, a char in the range \\u0000 to \\u007f, or an int or short number between -128 and 127, then let r1 and r2 be the results of any two boxing conversions of p. It is always the case that _r1 == r2.    자바 버전별로 조금씩 표현은 다르지만, 결국 -128 ~ 127 구간의 Integer에 대해서는 Wrapper 타입이더라도, 동등 연산만으로도 값의 비교가 가능해야한다는 내용이다.  (그리고 Integer 외에도 Boolean, Character 등도 같은 개념이 있다.)   관련해서 조금 더 살펴보면, 아래와 같은 내용이 있다.     Ideally, boxing a given primitive value p, would always yield an identical reference. In practice, this may not be feasible using existing implementation techniques. The rules above are a pragmatic compromise. The final clause above requires that certain common values always be boxed into indistinguishable objects. The implementation may cache these, lazily or eagerly.    결국 Wrapper 타입도 이상적으로는 모든 구간에 대해서 동등 연산만으로 비교가 가능해야하지만, 현실적으로 모든 수에 대해 객체를 만들어두는 것은 불가능하므로, 프로그래밍 상의 절충안으로 특정 구간으로 한정한다는 내용이다.   AutoBoxCacheMax   위에서 Integer 풀의 구간은 -128~127 이라고 했는데, 해당 구간은 내부적으로 IntegerCache의 low, high 변수에 의해 조정된다.  Integer 클래스 내에 있는 IntegerCache 클래스의 내부 구현을 살펴보자.   /**    * Cache to support the object identity semantics of autoboxing for values between   * -128 and 127 (inclusive) as required by JLS.   *   * The cache is initialized on first usage.  The size of the cache   * may be controlled by the {@code -XX:AutoBoxCacheMax=&lt;size&gt;} option.    * During VM initialization, java.lang.Integer.IntegerCache.high property   * may be set and saved in the private system properties in the   * sun.misc.VM class.   *   */   private static class IntegerCache {       static final int low = -128;       static final int high;       static final Integer cache[];          static {           // high value may be configured by property           int h = 127;           String integerCacheHighPropValue =               sun.misc.VM.getSavedProperty(\"java.lang.Integer.IntegerCache.high\");           if (integerCacheHighPropValue != null) {               try {                   int i = parseInt(integerCacheHighPropValue);                   i = Math.max(i, 127);                   // Maximum array size is Integer.MAX_VALUE                   h = Math.min(i, Integer.MAX_VALUE - (-low) -1);               } catch( NumberFormatException nfe) {                   // If the property cannot be parsed into an int, ignore it.               }           }           high = h;              cache = new Integer[(high - low) + 1];           int j = low;           for(int k = 0; k &lt; cache.length; k++)               cache[k] = new Integer(j++);              // range [-128, 127] must be interned (JLS7 5.1.7)           assert IntegerCache.high &gt;= 127;       }          private IntegerCache() {}   }     jdk/src/java.base/share/classes/java/lang/Integer.java at c1c99a669bb7f9928086db6a4ecfc90c410ffbb0 · openjdk/jdk · GitHub   이때 low 값은 -128로 고정이지만, high 값은 -XX:AutoBoxCacheMax=&lt;size&gt; 를 VM 옵션으로 지정해주면 조절이 가능하다.   실제로, 아래와 같이 Intellij에서 VM 옵션으로 최댓값을 1000으로 설정하고     글 도입부의 아래 함수를 실행하면, 둘다 true를 리턴하는 것을 확인할 수 있다.   @Test public void testIntegerSame(){ \tSystem.out.println(isBoxedIntegerSame(127)); // true \tSystem.out.println(isBoxedIntegerSame(128)); // true }   그리고 위 IntegerCache의 내부구현을 자세히 살펴보았다면 알겠지만, 해당 상한 값을 127 미만으로 설정하게 되면, 무시하고 상한을 127로 적용한다. (JLS에 명시된 스펙이기 때문이다.)  따라서, 해당 옵션을 100으로 지정하더라도 위 함수의 리턴값이 둘다 false로 바뀌진 않는다.   활용 방안에 대한 개인적인 생각   사실 실무에서 이걸 활용할 일은 많을 것 같진 않다. (활용 사례가 있다면 댓글에 알려주시면 감사하겠습니다.)  굳이 찾자면, 인풋 구간이 -128~127 내로 한정된 함수에 대해 극한의 최적화를 하는데 활용할 수 있을 듯하다.   그리고 이걸 활용했을때 문제가 될 만한 상황은 Integer 클래스에 대해 동등 비교 연산자(==)를 썼을때 예상치 못한 동작을 하는 것이 문제일 것이다.  예를 들면, 테스트코드에서는 -128~127 구간의 수로 테스트하여 테스트를 통과하였으나, 실제 비즈니스 코드 상에서는 그외의 구간의 인풋이 들어와서 테스트 코드에서 기대하던 결과와 다른 결과가 발생한다면, 실제로 문제가 될 수 있다.   하지만, 개인적인 생각으로는 해당 문제가 발생하기 이전에 참조 타입인 Integer 클래스에 대해 equals()가 아닌 ==를 사용하여 동등비교를 하는 것 자체가 바람직하지 않은 듯하다.  물론, 해당 연산을 -127~128 구간에 대해서만 수행하는 것이 보장되어 있다고 하더라도, 비즈니스 로직 상에서 그정도의 최적화까지 필요로 하는 일이 많지 않을 듯 하다.  오히려 위와 같은 스펙을 모르는 개발자가 봤을때는 == 연산자를 쓰는 것을 오류라고 생각할 수 있다.   그리고 자바 스펙에 의존적인 코드이기 때문에, 현재까지의 최신의 버전에서는 잘 동작하겠지만, 이후의 버전에서도 잘 동작한다는 보장이 없다.   그럼에도 최적화가 필요하다면, 위와 같은 자바 스펙 상의 Integer 캐시 개념에 대한 주석정도는 필요할 것 같다.   결론      자바의 Integer 클래스는 -127 ~ 128 구간에 대해 풀을 만들어놓고 객체를 재사용한다.   그리고 이것은 JLS에 명시된 자바의 스펙이다.   이 사실을 활용하면 최적화는 되겠지만, 모르는 사람이 보면 의아할 수 있으니 주석과 함께 사용할 필요는 있어보인다.   결국 실제 활용보다는 pool의 개념, 혹은 디자인 패턴 중 Flyweight 패턴 활용 사례 중 하나 정도로 알고 있으면 좋을 듯하다.   References      jdk/src/java.base/share/classes/java/lang/Integer.java at master · openjdk/jdk · GitHub   Chapter 5. Conversions and Promotions   java - Why Integer class caching values in the range -128 to 127? - Stack Overflow   Flyweight pattern - Wikipedia  ","categories": ["Java"],
        "tags": ["Java"],
        "url": "https://dreamsh19.github.io/java/Java%EC%9D%98-Integer%EB%8F%84-pool%EC%9D%B4-%EC%9E%88%EB%8B%A4/",
        "teaser": null
      },{
        "title": "Airflow DockerOperator의 Private registry 장애전파 방지(LazyLoginDockerOperator 구현기)",
        "excerpt":"이슈     Airflow에서 DockerOperator를 이용하여 매번 private registry에서 도커 이미지를 pull해서 수행하는 배치가 있다.   그런데 외부 조직에서 관리하는 private registry의 장애가 발생하는 경우 registry의 장애가 서비스의 배치까지 전파되는 문제가 있었다.   그리고 private registry의 장애가 빈번해짐에 따라 서비스에 영향을 주지 않게끔 조치가 필요해졌고, registry 장애가 서비스 배치의 장애로 전파되지 않도록 하고자했다.   문제 상황 당시 로그      문제 상황 발생 당시의 로그이다. Docker login 단계에서 private registry에서 500 에러가 발생하면서 해당 DAG가 실패하였다.   최초 시도 : force_pull 옵션 변경   서비스에서는 매번 private registry에서 이미지를 pull하도록 하는 DockerOperator의 force_pull 옵션을 True로 설정하여 사용하고 있었다.  force_pull 옵션은 그 이름에서도 쉽게 알 수 있듯이, DockerOperator가 수행될때마다 무조건 이미지를 pull해서 사용하도록 하는 옵션이다.     force_pull (bool) – Pull the docker image on every run. Default is False.    True로 설정해두고 사용했던건 아마도 원격의 단일 이미지를 참조하게 함으로써 로컬에 캐싱된 이미지로 인한 불일치나 장애를 막기 위함이 아닐까 추측된다.  따라서 force_pull 옵션을 False로 설정하면 최초 수행 시(이미지 버전 업 등)에만 이미지를 pull하고, 그 이후에는 이미 pull 받은 이미지를 로컬에서 가져올테니,  최초 수행 시에만 registry 의존성이 있고, 그 이후에는 registry 장애에 영향을 받지 않을 것이라고 생각했다.  (물론 이게 유효하려면 매번 같은 worker에서 수행된다는 보장이 되어야하는데, 해당 DAG는 고정된 하나의 장비에서 수행되도록 설정되어 있었다.)   그러나.. force_pull 옵션을 비활성화하고 테스트를 해보았으나, private registry가 올바른 응답을 주지 않는 경우에 여전히 DAG는 실패했다.   DockerOperator 소스를 살펴보자   force_pull 옵션이 의도대로 동작하지 않고, 문서에도 특별한 설명이 없어서 소스 코드를 살펴보았다.  아래는 DockerOperator의 execute() 함수 전문이다. (서비스에서 사용하고 있던 providers-docker 2.5.0 버전 기준)  참고로, Airflow의 각종 Operator들은 BaseOperator의 execute() 함수를 구현하여 서로 다른 기능을 제공한다.   def execute(self, context: 'Context') -&gt; Optional[str]: \tself.cli = self._get_cli() \tif not self.cli: \t\traise Exception(\"The 'cli' should be initialized before!\")  \t# Pull the docker image if `force_pull` is set or image does not exist locally \tif self.force_pull or not self.cli.images(name=self.image): \t\tself.log.info('Pulling docker image %s', self.image) \t\tlatest_status = {} \t\tfor output in self.cli.pull(self.image, stream=True, decode=True): \t\t\tif isinstance(output, str): \t\t\t\tself.log.info(\"%s\", output) \t\t\t\tcontinue \t\t\tif isinstance(output, dict) and 'status' in output: \t\t\t\toutput_status = output[\"status\"] \t\t\t\tif 'id' not in output: \t\t\t\t\tself.log.info(\"%s\", output_status) \t\t\t\t\tcontinue  \t\t\t\toutput_id = output[\"id\"] \t\t\t\tif latest_status.get(output_id) != output_status: \t\t\t\t\tself.log.info(\"%s: %s\", output_id, output_status) \t\t\t\t\tlatest_status[output_id] = output_status \treturn self._run_image()  def _get_cli(self) -&gt; APIClient: \tif self.docker_conn_id: \t\treturn self.get_hook().get_conn() \telse: \t\ttls_config = self.__get_tls_config() \t\treturn APIClient(base_url=self.docker_url, version=self.api_version, tls=tls_config)     airflow/airflow/providers/docker/operators/docker.py at providers-docker/2.5.0 · apache/airflow · GitHub   로직 자체는 이미지를 pull해야하는 경우(force_pull이거나, 이미지가 존재하지 않는 경우) 이미지를 pull하고 run 한다. 정도로 간단하다.   그런데 위 코드에서 주목할만한 점은 execute()을 진입하자마자 docker client(코드에서 self.cli)를 _get_cli() 함수를 통해 초기화한다.  그리고 _get_cli() 함수를 살펴보면, docker_conn_id 가 있는 경우 DockerHook을 이용해서 클라이언트를 초기화한다.  이때 docker_conn_id는 private registry에 접근할때의 인증정보를 포함하고 있는 객체이다.      If a login to a private registry is required prior to pulling the image, a Docker connection needs to be configured in Airflow and the connection ID be provided with the parameter docker_conn_id.    서비스에서는 private registry에 접근하기 때문에 docker_conn_id를 설정하여 사용하고 있었고, 그렇기 때문에 _get_cli()에서 해당 분기를 타게 된다.   그렇다면 DockerHook 클래스의 get_conn() 함수를 더 살펴보자. 아래는 해당 함수 전문이다.  def get_conn(self) -&gt; APIClient: \tclient = APIClient(base_url=self.__base_url, version=self.__version, tls=self.__tls) \tself.__login(client) \treturn client   def __login(self, client) -&gt; None: \tself.log.debug('Logging into Docker') \ttry: \t\tclient.login( \t\t\tusername=self.__username, \t\t\tpassword=self.__password, \t\t\tregistry=self.__registry, \t\t\temail=self.__email, \t\t\treauth=self.__reauth, \t\t) \t\tself.log.debug('Login successful') \texcept APIError as docker_error: \t\tself.log.error('Docker login failed: %s', str(docker_error)) \t\traise AirflowException(f'Docker login failed: {docker_error}')     airflow/airflow/providers/docker/hooks/docker.py at providers-docker/2.5.0 · apache/airflow · GitHub   놀랍게도 클라이언트를 생성한 이후에 즉시 로그인을 하는 것을 확인할 수 있다!  그리고 __login() 함수를 따라가다보니 에러발생시 Docker login failed 라는 문제 상황 당시 로그 에서 봤던 익숙한 문구가 눈에 띈다.   결국 소스 코드를 살펴봤을때,  docker client 생성 시점에 로그인을 시도하고, 로그인이 실패하면서, 로컬 이미지의 존재 여부는 조회조차 하지 못한채 실패하는 것이었다.   execute() 함수 오버라이딩을 통한 해결   결국 우리가 필요한 것은, 로컬의 이미지를 조회하고, 없는 경우에만 이미지를 pull하면 된다.  따라서 images API 호출 시점의 docker client는 로그인이 필요하지 않고, pull API 호출 시점의 docker client는 로그인이 필요하다.  그런데 위 execute() 함수의 구현 상 docker client(self.cli)를 중간에 주입할 수가 없는 구조이기 때문에 아래와 같이 execute() 함수 자체를 오버라이딩하는 방식으로 해결하였다.      images API 호출 시점의 docker client는 로그인을 하지 않은 상태의 client 생성   images API 호출 이후~ pull API 호출 직전에 client 로그인 수행   그 외의 로직은 동일   위와 같은 요구사항을 반영하여 구현한 전문은 아래와 같다.(DockerOperator를 상속한 LazyLoginDockerOperator)   from typing import Optional from airflow.providers.docker.operators.docker import DockerOperator from docker import APIClient, tls  class LazyLoginDockerOperator(DockerOperator):      def execute(self, context) -&gt; Optional[str]:         self.cli = self._get_cli()         if not self.cli:             raise Exception(\"The 'cli' should be initialized before!\")          # Pull the docker image if `force_pull` is set or image does not exist locally                  if self.force_pull or not self.cli.images(name=self.image):             if self.docker_conn_id:                 self.cli = self.get_hook().get_conn()              self.log.info('Pulling docker image %s', self.image)             latest_status = {}                          for output in self.cli.pull(self.image, stream=True, decode=True):                 if isinstance(output, str):                     self.log.info(\"%s\", output)                     continue                 if isinstance(output, dict) and 'status' in output:                     output_status = output[\"status\"]                     if 'id' not in output:                         self.log.info(\"%s\", output_status)                         continue                      output_id = output[\"id\"]                      if latest_status.get(output_id) != output_status:                         self.log.info(\"%s: %s\", output_id, output_status)                         latest_status[output_id] = output_status         return self._run_image()      def _get_cli(self) -&gt; APIClient:         return APIClient(base_url=self.docker_url, version=self.api_version, tls=self.__get_tls_config())      def __get_tls_config(self) -&gt; Optional[tls.TLSConfig]:         return self._DockerOperator__get_tls_config()   위와 같이 변경한 LazyLoginDockerOperator를 문제가 된 DAG에 적용하였고, 이후에 동일한 registry 장애가 발생했을때 해당 DAG는 영향을 받지 않고 정상수행됨을 확인했다!  이로써 외부 요인인 registry 장애가 발생했을 때 서비스 배치로의 장애가 전파되는 걸 막을 수 있었고, 장애 대응으로 인한 공수 또한 절감할 수 있었다.   이후 버전에서 해결되었을까   사실 문제 발생 당시에 이미지 존재 여부와 무관하게 실패하는 것이 정상동작은 아니라고 생각하여 bug fix가 있었는지를 찾아보았으나, 딱히 올라오진 않았고, 당시 최신버전에서도 문제가 해결되지 않은 것으로 확인해서 위와 같이 어쩔 수 없이 직접 오버라이딩하는 방식으로 해결하였다.  그리고 현재(2024년 10월) 기준 최신 버전인 providers-airflow 3.14.0 버전에서도 여전히 동일한 이슈가 발생하는 것으로 확인된다.   make docker operators always use `DockerHook` for API calls by Taragolis · Pull Request #28363 · apache/airflow · GitHub  위 PR에서 docker client를 획득하는 로직이 변경되긴 했으나, 단순히 생성 시점이 execute() 호출 시점이 아닌 self.cli 최초 호출 시점에 최초 생성된다는 점만 바뀌었을뿐, 여전히 생성 시점에 로그인을 하는 로직은 바뀌지 않은듯하고, 이 로직이 현재 최신 버전에서도 동일하다.   # DockerHook.api_client()의 일부  if self.docker_conn_id: \t# Obtain connection and try to login to Container Registry only if ``docker_conn_id`` set. \tself.__login(client, self.get_connection(self.docker_conn_id))     airflow/airflow/providers/docker/hooks/docker.py at providers-docker/3.14.0 · apache/airflow · GitHub   따라서 현재 기준 최신 버전에서도 동일한 이슈는 여전히 발생하기 때문에, 단순 버전업만으로는 위와 같은 문제에 대한 해결은 어려워보인다.   References      airflow/airflow/providers/docker at providers-docker/2.5.0 · apache/airflow · GitHub   airflow.providers.docker.operators.docker — apache-airflow-providers-docker Documentation   make docker operators always use `DockerHook` for API calls by Taragolis · Pull Request #28363 · apache/airflow · GitHub  ","categories": ["Airflow"],
        "tags": ["Airflow","Docker"],
        "url": "https://dreamsh19.github.io/airflow/Airflow-DockerOperator%EC%9D%98-Private-registry-%EC%9E%A5%EC%95%A0%EC%A0%84%ED%8C%8C-%EB%B0%A9%EC%A7%80(LazyLoginDockerOperator-%EA%B5%AC%ED%98%84%EA%B8%B0)/",
        "teaser": null
      },{
        "title": "Git commit hash는 어떻게 만들어질까",
        "excerpt":"이슈     개발을 하다보면 git rebase를 자주 사용하는데, rebase 할때 실제 반영되는 내용이 동일한데도 commit hash가 바뀌는 것을 보고, 커밋 시간이나 부모 커밋 등의 정보가 달라졌으니 hash값이 달라지는구나 정도로 어렴풋이 추측만 했다.   그렇다면 서로 다른 커밋을 “다르다”라고 판단하는 기준이 무엇일까가 궁금해졌고,   어렴풋이 추측만 하던, git에서의 id 체계인 commit hash가 어떻게 만들어지는지에 대해 살펴보았다.   결론   결론부터 얘기하자면, 임의의 git repository에 진입해서 아래 명령어를 수행하면, 현재 위치한 HEAD의 commit hash를 재현해낼 수 있다.   (printf \"commit %s\\0\" $(git --no-replace-objects cat-file commit HEAD | wc -c); git cat-file commit HEAD) | sha1sum # 출처 : https://gist.github.com/masak/2415865#file-explanation-md   위 명령어를 수행하고, git show -s 명령어를 통해 현재 commit hash를 확인해보면 동일한 것을 확인 할 수 있다.   아래는 직접 확인해본 결과이다. 역시나 동일함을 확인할 수 있다.     그렇다면, 위 명령어에 대해 하나씩 살펴보자.  우선 마지막의 sha1sum 명령어는 단순 해싱 함수이고, 본 글에서는 hash 함수의 input이 궁금한 것이므로 미뤄두자.   눈에 띄는 것은 git cat-file commit HEAD 명령어가 결국 주요한 input으로 보인다. (앞의 printf 파트는 해당 명령어의 바이트수를 포맷팅하여 출력하는 정도이므로)   그렇다면 git cat-file commit HEAD 명령어는 대체 어떤 내용을 담고 있는지 살펴보자.   확인을 위한 간단한 구성   살펴보기 전에 간단한 git repository를 아래와 같이 생성하여, 해당 repository 하에서 내용을 확인하였다.   mkdir git-hash &amp;&amp; cd git-hash git init touch README.md git add . &amp;&amp; git commit -m \"Intial commit\" echo \"Hello World\" &gt;&gt; README.md git add . &amp;&amp; git commit -m \"Add Hello World to README\"   git cat-file commit 상세   우선 git cat-file 명령어는 git의 object에 대한 세부정보를 출력해주는 명령어이다.  여기서 object란 git에서 내부적으로 리소스를 관리할때 사용하는 객체이며, commit도 object의 한 종류이다.  결국 git cat-file commit HEAD 명령어는 현재(HEAD) 커밋에 대한 세부정보를 출력해줘 정도의 명령어이다.   실제로 명령어를 통해 출력한 결과는 아래와 같다.   [user@server ~/sources/git-hash]$ git cat-file commit HEAD tree db78f3594ec0683f5d857ef731df0d860f14f2b2 parent c612c9316c74c3f7489135e18545a2082e5ebd0e author Seung-Hun Han &lt;dreamsh19@gmail.com&gt; 1730020485 +0900 committer Seung-Hun Han &lt;dreamsh19@gmail.com&gt; 1730020485 +0900  Add Hello World to README   크게 5개(tree, parent, author, committer, commit message)로 구성된 것을 확인할 수 있다. 각각에 대해 살펴보자.   tree   tree db78f3594ec0683f5d857ef731df0d860f14f2b2   해당 커밋 상태에서의 모든 파일에 대한 스냅샷(으로 부터 추출된 hash값)이다.  참고로, git에서의 tree는 git 내부적으로 사용하는 자체 파일 시스템 정도로 이해할 수 있다. git에서 자체 파일 시스템이 필요한 이유는 버저닝에 특화되어 있어야하고, 특정 파일 시스템에 종속되지 않아야하는 등의 요구사항이 필요하기 때문이다.   결국 파일의 내용(blob)이 변경되면 hash값이 바뀐다. 정도로 이해하면 될 것 같고, 커밋의 고유성을 결정할때 가장 직관적인 기준이다. 그리고 tree hash 외의 다른 4개의 항목들은 모두 commit 자체에 대한 메타데이터인 반면, 파일의 내용에 대한 항목은 tree hash가 유일하다.   하지만 예상하겠지만, 파일의 내용만으로는 커밋의 고유성을 보장할 수 없다. (나머지 4개 항목이 필요한 이유이기도 하다.)   간단한 예로, 커밋 A를 수행하고 바로 A를 revert하는 경우를 떠올려보면, 파일의 내용은 달라진게 없지만 2개의 추가적인 커밋이 발생했고, git에서는 이를 구별해야한다.  실제로 확인해보자. README에 “Mistakes” 문구를 추가했다가 곧바로 revert하고 git cat-file commit 명령어를 수행해보았다.   \b[user@server ~/sources/git-hash]$ echo \"Mistakes\" &gt;&gt; README.md \b[user@server ~/sources/git-hash]$ git add . &amp;&amp; git commit -m \"Made a mistake\" [main 1f002c2] Made a mistake  1 file changed, 1 insertion(+) \b[user@server ~/sources/git-hash]$ git revert HEAD [main 3b9a0c2] Revert \"Made a mistake\"  1 file changed, 1 deletion(-) Revert \"Made a mistake\"  This reverts commit 1f002c2bf3f8f779f73db79a88cb7575a77eab91. \b[user@server ~/sources/git-hash]$ git cat-file commit HEAD tree db78f3594ec0683f5d857ef731df0d860f14f2b2 parent 1f002c2bf3f8f779f73db79a88cb7575a77eab91 author Seung-Hun Han &lt;dreamsh19@gmail.com&gt; 1730022719 +0900 committer Seung-Hun Han &lt;dreamsh19@gmail.com&gt; 1730022719 +0900  Revert \"Made a mistake\"  This reverts commit 1f002c2bf3f8f779f73db79a88cb7575a77eab91.   여기서 확인할 수 있는 것은, 2개의 커밋이 발생했음에도 마지막 명령어 결과에서 tree hash값은 db78f35...으로 동일한 것을 확인할 수 있다.   parent   parent c612c9316c74c3f7489135e18545a2082e5ebd0e   부모 커밋의 hash값이다.  커밋의 parent 정보는 다른 커밋과의 관계를 정의하는 유일한 정보이기 때문에, 커밋의 고유성을 결정하기에 필수적인 정보이다.  실제로 같은 tree hash를 가지는 서로 다른 커밋이 있다고 하더라도, 부모 커밋이 다르다면 전혀 다른 커밋이 된다. 당장 위의 revert 예시에서도 동일한 tree hash지만 parent 정보가 다름으로 인해 구별할 수 있다.   author, committer   author Seung-Hun Han &lt;dreamsh19@gmail.com&gt; 1730020485 +0900 committer Seung-Hun Han &lt;dreamsh19@gmail.com&gt; 1730020485 +0900   author와 committer의 정보와 시간 정보가 담겨 있다.  (author와 committer의 차이에 대해서는 자세히 다루지 않겠다. 궁금하다면 Git - Viewing the Commit History 를 참고하면 된다.)  git은 기본적으로 협업을 위한 프로그램이기 때문에 author, committer 정보가 중요할 수 밖에 없다. 같은 부모 커밋에서 같은 내용을 커밋하더라도 수행하는 사람이 다르다면, 서로 다른 커밋으로 취급한다.   개인적으로 author와 commiter 정보에서 특이하다고 느낀 점은 두가지인데, 첫번째는 시간의 정밀도가 초단위까지밖에 없다는 것이고, 두번째는 타임존 정보를 포함하고 있다는 것이다.   1. 초단위 정밀도  시간 정보는 Unix timestamp로 표현되어있는데, 1730020485로 초단위 정밀도이다.  그러면 자연스럽게 “1초 내에 동일한 커밋을 두 번하면 어떻게 되는거지?” 라는 생각이 든다.  그래서 직접 해보았다. main을 부모 커밋으로 가지는 서로 다른 두개의 브랜치를 만들고 각 브랜치에서 똑같은 커밋을 1초 내로 수행해보았다.   for i in {1..2}; do  \tgit checkout main; \tgit checkout -b branch-${i}; \ttouch file.tmp; \tgit add . &amp;&amp; git commit -m \"Create file.tmp\";  done   결과는 놀랍게도 동일한 커밋으로 취급되었다. (실제로 커밋은 두 번 발생했음에도)      그리고 위 명령어 마지막에 sleep 1; 을 추가하여 확인해보니 서로 다른 커밋으로 취급되었다.   그리고 초단위 정밀도를 가지는 걸 이상하게 생각한 것은 나 뿐만이 아닌듯 하다.  What is the resolution of Git’s commit-date or author-date timestamps? - Stack Overflow  위 링크에서 보면, 실제로 같은 커밋을 1초 내에 두번하면 하나의 커밋으로 취급된다는 내용이 있다.   2. 타임존 정보   위에서 살펴보았듯이 시간 정보는 Unix timestamp로 저장되고, Unix timestamp는 타임존에 독립적인데, 타임존 정보가 포함된 것이 의아했다.  처음에는 “타임존 정보가 굳이 포함될 필요가 있을까?”라는 관점에서 의아했으나, 애초에 커밋 정보에 타임존 정보를 포함하고 있으니, 굳이 제외하지 않았다 정도의 관점으로 받아들이면 이상할 것도 없을 듯하다.  물론, 동일한 커밋을 같은 순간에(1초 내) 서로 다른 타임존에서 수행하면 다른 커밋 hash가 도출되고, 서로 다른 커밋으로 간주된다.(아래는 실험 결과)   for i in {3..4}; do  \tgit checkout main; \tgit checkout -b branch-${i}; \ttouch file.tmp; \tgit add .; \tGIT_COMMITTER_DATE=\"$(TZ=UTC+${i} date -R)\" git commit -m \"Create file.tmp\"; done      commit message   Add Hello World to README   커밋의 “이름” 정도라고 보면, 직관적으로는 커밋의 고유성을 위한 key로 채택함에 있어서 큰 무리가 없을 듯 하다.   다만, commit message를 포함하는데 있어서 개인적인 생각을 덧붙여보자면,  위의 네 가지 항목들은 커밋을 했을때 git에 의해 자동으로 결정되는 것들이지, “개발자가 명시적으로” 커밋의 고유성을 위해 지정할 수 있는 항목들은 아니다. (커밋 시간 등을 조정하면 할 수야 있지만 시간을 조정해서 고유성을 확보하는 것이 자연스럽진 않다고 생각이 된다.)  따라서 commit message는 개발자가 명시적으로 커밋의 고유성을 부여하기 위한 수단으로 볼 수 있다.  정확한 비유는 아니지만 일란성 쌍둥이지만 이름으로 구별할수 있게 하겠다.. 정도로 생각했다. (일란성 쌍둥이지만 태어난 시간으로 구별하는 건 자연스럽지 않으니..)   헤더 정보   위에서 git cat-file commit HEAD 명령어에 대해서는 자세히 살펴보았고, 이제 commit의 고유성을 결정하는 5가지 요소에 대해서는 모두 알게 되었다.  그렇다면 글 도입부 printf 파트에서 보면 “commit 바이트수\\0”를 포함하고 있는데, 이 정보는 무엇일까?   commit도 object 중 하나라는 점에서 눈치챘을 수도 있지만,  사실 git은 모든 것을 object 라는 개념으로 관리하고, 이 object의 id 체계로 hash\u001b를 활용하고 있다. commit hash도 이 중 하나일 뿐이다. (위에서 tree가 hash 값을 가지는 것도 같은 이유이다.)  그리고 object의 id를 생성할때 다음과 같이 일관된 형태로 해싱 함수의 input을 구성한다.   &lt;type&gt; &lt;size&gt;\\0&lt;content&gt;   이때 &lt;content&gt;를 제외한 &lt;type&gt; &lt;size&gt;\\0 부분을 git에서는 헤더라고 한다.   실제 git의 소스 코드 git/object-file.c at v2.47.0 · git/git · GitHub를 살펴보면 아래와 같은 코드로 헤더를 생성함을 확인할 수 있다.  (아래는 2.47.0 버전 상의 소스 코드이나, 이전 버전에서도 코드의 형태는 조금씩 다르지만 동일한 로직을 가진다.)     결국 &lt;type&gt; &lt;size&gt;의 형태로 포맷팅하고, 마지막의 +1을 통해 null character를 할당하는 로직이다.  여기서의 type은 object의 타입이며 글의 도입부에 commit이라는 고정 문자열이 입력된 것도 commit 타입임을 명시하는 것이다.  그리고 size는 content의 바이트 수이다. 사실 content에 종속된 정보라 불필요한 정보라고 생각될 수 있지만, 파일 시스템 상의 오류로 content 상의 불일치가 발생한 경우, 이를 탐지하기 위한 장치이다.   SHA1 해싱   여기까지 하면, hash 함수의 input에 대해서는 모두 살펴보았고, 결국 해당 input에 SHA1 해싱을 통해 최종적인 hash값을 도출해낸다.  그런데 SHA1 알고리즘은 컴퓨팅 파워의 증가로, 보안 취약점을 갖고 있는 알고리즘이 되어버렸고, 해시 충돌로 인한 위험성이 있는 것으로 밝혀졌다.   Git에서도 이를 인지하고 있고, 더 안전한 SHA256으로의 전환을 점진적으로 추진한다고 한다.  그리고 기존 SHA1 시스템에서의 해시 충돌 문제를 방지하기 위해 Github에서는 해시 충돌이 발생한 경우 이를 탐지하고 실패처리하는 것을 이미 적용하고 있다. 실제로 Git 프로젝트를 보면 해시 충돌 방지에 대한 소스를 submodule로 관리하고 있음을 확인할 수 있다.   Hash 충돌이 문제가 되는 이유   앞서 언급했듯이, git에서 hash값은 곧 id이다. id 체계는 유니크함을 보장하는 것이 필수적이나, 해시 충돌이 발생한다면 유니크함이 보장되지 않게 된다.   단적인 예로, 파일 A의 해시값이 abcde라고 하고. 악의적인 사용자가 abcde의 해시값을 도출해내는 다른 파일 B를 찾아낸 상황을 가정하자. 악의적인 사용자는 B 기반의 코드로 문제가 없음을 증명한 후 main 브랜치에 머지를 하려고 하면, git은 A와 B가 다르다는 걸 인지하지 못하게 된다. 결국 별 문제 없이 코드가 머지될 수 있고, 최종적으로 저장소에는 머지가 됐음에도 여전히 파일 A로 남아있게 되고, 동작하지 않는 전혀 다른 코드가 되어버린다.   물론, 아주 단적인 예일 뿐이고, 애초에 id 체계의 근간을 흔들 수 있기 때문에 많은 문제가 발생할 수 있다.   요약   내용이 길었지만 결국 commit hash 만드는 과정을 요약하면 아래와 같다.      git의 commit의 hash는 5가지 요소(파일 내용의 스냅샷, 부모 커밋, author, committer 정보, 커밋 메시지)에 의해 결정된다.   그리고 추가적인 헤더 정보(object 타입, 바이트 수)를 추가하며, 이는 commit에 한정된 것은 아니고 object 모두에 해당된다.   위의 결과에 SHA1 해싱을 적용하여 최종적인 값으로 채택한다.   References      How is git commit sha1 formed · GitHub   Git - git-cat-file Documentation   GitHub - git/git: Git Source Code Mirror - This is a publish-only repository but pull requests can be turned into patches to the mailing list via GitGitGadget (https://gitgitgadget.github.io/). Please follow Documentation/SubmittingPatches procedure for any of your improvements.   Git - Viewing the Commit History   What is the resolution of Git’s commit-date or author-date timestamps? - Stack Overflow   Why do git objects include a length and a delimiter as metadata? - Stack Overflow   Git의 commit id는 어떻게 생성될까?   SHA-1 collision detection on GitHub.com - The GitHub Blog   Git Transitioning Away from the Aging SHA-1 Hash - The New Stack  ","categories": ["Git"],
        "tags": ["Git","Hash"],
        "url": "https://dreamsh19.github.io/git/Git-commit-hash%EB%8A%94-%EC%96%B4%EB%96%BB%EA%B2%8C-%EB%A7%8C%EB%93%A4%EC%96%B4%EC%A7%88%EA%B9%8C/",
        "teaser": null
      },{
        "title": "비동기 논블락킹에 대한 이해",
        "excerpt":"이슈      코루틴에 대해서 공부하면서 이게 어떻게 JVM에서 동작하지?라는 의문을 가지고 헤맸던 경험이 있다.   결국 비동기 논블락킹에 대한 이해가 부족했던 것이고, 이 주제는 주기적으로(?) 헷갈리는 주제인 것 같다.   그래서 이번에 나만의 언어로 정리하는게 필요하다고 느껴져서 정리하게 됐다.   물론, 관련된 글은 이미 너무나 많지만, 정확하지 않은 정보로 인해 오히려 헷갈리는 경우가 있고, 각자의 언어로 표현하다보니 나에게는 와닿지 않는 경우가 많았다.   이 글은 사실 미래에 헷갈릴 나를 위한 글이다.   들어가며      이 글은 비동기 논블락킹에 대한 이론적 배경이나 구체적인 방법론 등을 설명하기 위한 글은 아니다.   비동기 논블락킹에 대한 이해를 쉽게 하는 것이 주 목적인 글이므로, 약간의 논리의 비약이 있을 수 있으나, 최대한 본질을 흐리지 않으며 작성하려고 했다.   결론     동기 vs 비동기는 “내 일을 동료한테 줄 수 있는가”의 관점이고,   블락킹 vs 논블락킹 “내가 다른 일을 할 수 있는가”의 관점이다.   그리고 중요한 건 결국 논블락킹이 목적이고, 비동기는 이를 위한 수단이다.            결국 자원 효율성 관점에서 내가 쉬지 않고 다른 일을 할 수 있는 게 중요한 거고, 내가 다른 일을 하기 위해서는 내가 하던 일을 동료에게 줄 수 있어야한다.           전제      비동기든 논블락킹이든 이 논의는 동일한 자원 상황에서의 “스레드의 스케줄링”에 대한 논의이다.            스레드의 스케줄링이라 함은 “누가 무슨 일을 할지”에 대한 논의이며, 그 목적은 당연하게도 누가 무슨 일을 해야 가장 효율적일까?에 있다.       그리고 여기서의 스레드는 “작업을 수행하는 주체”를 의미하고, 경우에 따라 OS 레벨의 프로세스 혹은 스레드가 될 수 있고, 언어 레벨에서는 고루틴, 코루틴과 같은 경량스레드가 될 수 있다.           그리고 이 논의를 위해서는 I/O time이 있는 작업이라는 전제가 필요하다.            애초에 cpu만 사용하는 작업만 있는 경우 코어 수나 클락 수를 늘리지 않는 이상 스레드의 스케줄링만으로는 개선할 여지가 없고, (스케줄링을 통해서 해결할 수 있는 건 starvation 등의 자원의 분배 문제이지, 효율은 개선될 수 없다)       결국 비동기든 논블락킹이든 I/O time 때 스레드가 유휴상태인 채로 점유된 것을 개선하기 위한 것들이기 때문이다.           비유를 통해 이해해보자.   손님에게 주문을 받고 주문받은 음료를 주는 카페 알바를 떠올려보자. Server의 어원 그 자체라고 할 수 있다.  그리고 I/O 작업은 커피머신에 커피를 내리는 작업에 비유할 수 있다. (커피머신이 커피를 내리는 시간은 알바가 아무리 뭘해도 컨트롤할 수 있는 영역이 아니다.)  굳이 따지자면 cpu 작업은 커피머신을 사용하지 않는 모든 작업(계산을 한다던가, 얼음을 넣는다던가 등)에 비유할 수 있다.   이러한 관점에서 아래 상황들을 떠올려보자.   동기 vs 비동기      동기는 내 일을 동료한테 줄 수 없는 형태의 스케줄링이다.   비동기는 내 일을 동료한테 줄 수 있는 형태의 스케줄링이다.   그럼 여기서 동기의 관점을 우선 살펴보자.  동기는 일을 동료한테 줄 수 없다. 이 말은 곧, 동료가 없다는 것과 같다고 볼 수 있고, 동료가 없다는 건 곧 혼자 일한다고 볼 수 있다.  즉, 싱글 스레드라고 취급할 수 있다.  여기서 싱글스레드라고 하면 수많은 동기 방식의 멀티스레드 프레임워크들은 무엇인가?에 대한 의문을 가질 수 있다.  그런데 동기 방식의 멀티스레드는 스레드가 여러개이긴 하나 스레드 간에 협력이랄게 없고, 각각의 스레드가 “독립적으로” n개의 작업을 할 뿐이다.  결국 멀티스레드라고 해도 각자 할 일을 하는 싱글스레드 * n개 밖에 되지 않는다.   이어서 비동기 관점을 살펴보자.  비동기 방식은 내 일을 동료한테 줄 수 있다는 것인데, 이것은 다시 말하면 일단 동료가 있어야한다는 것을 의미한다.  즉, 멀티스레드가 전제가 되어야한다.  (정확히는 비동기 방식이 반드시 멀티스레드일 필요는 없다. 예를 들어, 비동기 방식의 코드를 싱글 스레드로 실행하면 실행은 된다. 다만 이 상황은 내 일을 동료한테 줬는데 그 동료가 나인 경우이다. 즉, 결과적으로 따지고보면 어차피 내가 다하고 있는 거고, 그렇다면 굳이 비동기 방식이 갖는 이점이 없는 것이다. 이 말은 곧 비동기 방식이 “의미가 있으려면” 멀티스레드가 전제되어야 한다는 것을 뜻한다.)   멀티스레드를 전제로 하고 마저 살펴보면,  내 일을 동료한테 줄 수 있다는 것은 결국 스레드간 “협력”이 가능하다는 것을 의미한다. coroutine의 “co”가 cooperative인 것도 이와 같은 맥락이다.  비동기 방식에서 늘 나오는 얘기인 콜백도 결국 “협력”하는 한 방법 중에 하나일 뿐이다.   카페 알바의 예로 보면, 동기 방식은 내가 주문받은 손님은 내가 반드시 음료를 줘야하는 방식이다.  반면에, 비동기 방식은 주문은 내가 받았지만 음료를 주는 건 동료일 수 있다.   블락킹 vs 논블락킹      블락킹은 내가 다른 일을 할 수 없는 관점이다.   논블락킹은 내가 다른 일을 할 수 있는 관점이다.   카페 알바의 예로 보면, 블락킹 방식은 커피머신이 커피를 내리는 동안 아무 일도 하지 못하는 방식이다. (커피머신을 멀뚱멀뚱 쳐다본다던가, 커피 잔을 손으로 계속 들고 있어야된다고 생각해도 될 것 같다.)  반면에, 논블락킹 방식은 커피머신에 커피를 내리고 나서 주문을 받을 수 있는 방식이다.   블락킹 방식은 답답하기는 하지만, 커피머신이 커피를 다 내렸을 때 어떻게하지?에 대한 걱정은 없다.  다만, 논블락킹 방식은 누가봐도 효율적이다. 그런데 커피머신이 커피를 다 내리고 나면 이 커피는 어떻게 하지?에 대한 걱정을 하게 된다.  여기서 비동기와의 연결점이 발생한다.  동기 방식은 이 일을 동료한테 줄 수 없기 때문에, 커피를 다 내리고 나면 커피를 손님한테 건네주는 것도 내가 해야한다.  즉, 커피머신의 작업에 대한 완료처리를 계속 신경을 써야한다는 것이고, 결국 스레드가 이걸로부터 자유로울 수 없다.  그런데, 비동기 방식은 이 일을 동료한테 줄 수 있고, 커피가 다 내려지면 손님에게 건내줘라는 걸 동료한테 전달할 수 있게 되고 그러면 나는 이 일로부터 자유로워질 수 있다.   네 가지 조합에 대해 살펴보자   위의 관점에서 동기와 비동기 그리고 블락킹과 논블락킹이 어떤 관점인지 살펴보았으니, 2x2 조합 각각에 대해 구체적으로 떠올려보자.   동기 블락킹   가장 기초적인 모델이다.  내 일을 동료한테 줄 수도 없고, 다른 일을 할 수도 없다.  결국 싱글 스레드의 sequential한 모델을 떠올릴 수 있다.   예상하겠지만, 싱글 스레드의 문제와 순차 수행의 문제를 갖고 있다.   동기 논블락킹   내 일을 동료한테 줄 수 없으나, 다른 일을 할 수 있다.  커피 머신에 커피를 내리게 한 다음에 주문을 받을 수 있다.  커피를 내리는 동안 손님을 받을 수 있으니까, 동기 블락킹을 방식보다 효율적이다.   그럼 뭐가 문제일까?  싱글 스레드의 문제를 가져간다. 즉, 점유되는 경우 다른 작업들이 같이 지연된다는 단점이 있다.  예를 들어, 위의 상황에서 주문을 받는데 주문받는게 오래걸린다면(스레드를 점유한다면) 커피 머신이 커피를 다 내렸어도 이전 손님에게 커피를 주지 못한다.   비동기 블락킹   내 일을 동료한테 줄 수 있으나, 다른 일을 할 수 없다.  보자마자 이런 생각이 든다. “다른 일을 할 수 없는데 동료한테 왜 줘?”  그리고 이 생각은 실제로 비동기 블락킹 방식이 자연스럽지 않고, 잘 사용되지 않는 이유를 관통하는 관점이다.  Java의 CompletableFuture의 get() 함수를 호출하면 비동기지만 스레드가 블락되는 방식으로 동작한다.   비동기 논블락킹   내 일을 동료한테 줄 수 있고, 다른 일도 할 수 있다.  동기 논블락킹에서의 싱글 스레드 문제를 해결할 수 있다.  예를 들어, 스레드가 점유당한다면, 동료가 대신 커피를 줄 수 있다.  그리고 내 일을 동료에게 준 순간 나와 일, 즉 스레드와 자유의 몸이 됐으므로, 새로운 주문을 받던 이미 다른 동료가 받은 주문에 대한 커피를 손님에게 내어주던, 어떤 작업이든 시작할 수 있다.   앞으로 남은 것   위에서 개념적으로 이해한 것들을 코딩으로 실제로 구현해보고 확인하면 좀 더 확실하게 정리될 것 같다.  코드 기준으로 설명한 것들로 2편을 작성하는 것을 목표로 해야겠다.   References      Why is threading useful on a single core processor? - Quora   비동기 서버에서 이벤트 루프를 블록하면 안 되는 이유 2부 - Java NIO와 멀티플렉싱 기반의 다중 접속 서버  ","categories": ["Async"],
        "tags": [],
        "url": "https://dreamsh19.github.io/async/%EB%B9%84%EB%8F%99%EA%B8%B0-%EB%85%BC%EB%B8%94%EB%9D%BD%ED%82%B9%EC%97%90-%EB%8C%80%ED%95%9C-%EC%9D%B4%ED%95%B4/",
        "teaser": null
      },{
        "title": "Jersey Singleton 객체의 초기화 시점",
        "excerpt":"이슈     현재 운영 중인 Jersey 기반의 서비스 중에 애플리케이션 구동 시점에 응답시간이 지연되는 이슈가 있었다.   구동 시점에만 이슈가 발생했기 때문에 무엇인가 초기화가 늦어지는 것으로 추측하고, 살펴보기 시작했다.   클래스의 초기화 시점   초기화 지연을 의심하여 애플리케이션 내에서 사용하는 Singleton 객체의 초기화 시점에 로그를 찍도록 했다.  예상은 애플리케이션 구동 시점에 모두 초기화될 것으로 예상하였으나, 결과는 놀랍게도 실제 호출 시점에 초기화가 이루어졌다. 그래서 이러한 현상이 발생하는 이유를 찾아보기 시작했다.   초기화가 늦게 이루어지는 클래스는 아래와 같이 Singleton 형태로 구현되어 있었고, 사용하는 쪽에서는 ServiceA.getInstance() 형태로 호출하고 있었다.  public class ServiceA {   \t \tprivate static final ServiceA INSTANCE = new ServiceA();          private ServiceA() {}        public static ServiceA getInstance() {           return INSTANCE;       } }   코드에서 보면 명확하듯이, 클래스가 로딩되는 시점에 static 변수인 INSTANCE 객체가 초기화될 것이라고 생각했고, 그래서 클래스는 애플리케이션 구동 시점에 로딩될테니, 애플리케이션 구동 시점에 Singleton 객체인 INSTANCE가 초기화된다고 생각하고 있었다.   그러나 관련해서 좀 더 찾아보다보니, 여기서 한가지 놓치고 있었던 전제가 있었다.  클래스가 로딩되는 시점에 초기화가 되는 것은 맞지만, 애플리케이션 구동 시점에 클래스가 로딩되는가? 에 대한 전제이다.   그리고 결론부터 이야기하면, 자바의 스펙상, 필요하지 않는 클래스는 미리 로딩하지 않는다.   JLS 12.4.1에 명시된 스펙   JLS(Java Language Spec)에는 클래스의 초기화 시점을 아래와 같이 정의하고 있다.      A class or interface type T will be initialized immediately before the first occurrence of any one of the following:          T is a class and an instance of T is created.     A static method declared by T is invoked.     A static field declared by T is assigned.     A static field declared by T is used and the field is not a constant variable (§4.12.4).      위와 같이 초기화 시점에 대한 구체적인 케이스에 대해 표현되어있지만, 요약하자면 실제로 “필요한 시점에 초기화한다”가 핵심이며, 이때 필요한 시점이라는 것은 클래스에 정의된 필드, 메소드에 접근하는 최초 시점을 의미한다. 결국, 클래스 로딩 역시 최대한 lazy하게 수행하는 것이 자바의 스펙이다.  (클래스 초기화에 대한 참고용으로 위 JLS 문서의 클래스 초기화 예제들을 보면, 필요하지 않는 클래스는 절대 초기화하지 않도록 하는, 극단적으로 lazy하게 수행하도록 강제하는 것을 확인할 수 있다.)    그럼 위와 같은 스펙에 대한 이해를 바탕으로 처음 ServiceA의 코드를 살펴보자.  결국 ServiceA의 코드는 애플리케이션 로딩 시점에 클래스 로딩을 강제할 요소가 전혀 없다. 그렇기 때문에 ServiceA.getInstance()함수가 호출되는 시점에서야 ServiceA의 로딩이 일어나게 된다.   해결방안   처음으로 돌아가, 결국 문제의 원인은 ServiceA의 초기화 시점이 애플리케이션 구동 시점이 아닌 실제 사용 시점이기 때문이다.  그렇다면 해결 방법은 ServiceA를 애플리케이션 구동 시점에 초기화하도록 하면 된다.  이를 위한 구체적인 방법은 아래와 같다.   아래 해결방안에 대해 부연설명을 추가하자면, ServiceA.getInstance() 를 Resource 레벨(스프링에서의 Controller)에서의 생성자에서 호출하고 있었는데, 결국 근본적으로 Resource가 지연 초기화 방식으로 동작하고 있었고, 그것으로 인해 발생한 문제라고 볼 수 있다.  Resource가 즉시 초기화가 된다면, Resource 초기화 시점에 ServiceA도 초기화가 될 것이기 때문에 결국 Resource를 즉시 초기화할 수 있는 방법에 대해 검토하였다.   1. Spring과의 통합   Spring은 필요한 빈을 애플리케이션 구동 시점에 eager하게 모두 로딩하는 것이 디폴트 설정이다.  그리고 아래와 같이 jersey-spring 의존성을 추가하면 Spring beans와 Jersey를 함께 사용할 수 있다.   &lt;dependency&gt; \t&lt;groupId&gt;org.glassfish.jersey.ext&lt;/groupId&gt; \t&lt;artifactId&gt;jersey-spring3&lt;/artifactId&gt; \t&lt;version&gt;${project.version}&lt;/version&gt; &lt;/dependency&gt;   예제 코드는 아래를 참고하면 된다.       jersey/examples/helloworld-spring-webapp/src/main/java/org/glassfish/jersey/examples/helloworld/spring/SpringRequestResource.java at 2.28 · eclipse-ee4j/jersey · GitHub   그러나 이 방법은 애플리케이션의 복잡도를 증가시키기 때문에 기각하였다.   2. @Immediate를 통한 해결   위처럼 새로운 의존성을 추가하지 않고, 기존의 Jersey의 기능을 활용하여 해결할 수 있는 방법을 찾고자했다.   Jersey는 의존성 주입(DI) 구현체로서 내부적으로 hk2를 사용한다.  그리고 hk2에서도 @Immediate annotation을 통해 즉시 초기화를 지원한다.   참고로, hk2는 디폴트 초기화 전략으로 지연 초기화를 채택하고 있다.       glassfish-hk2/hk2-api/src/main/java/org/glassfish/hk2/internal/ImmediateHelper.java at f80f98503c51ca9fd2f2b57def329b05dacab73a · eclipse-ee4j/glassfish-hk2 · GitHub      그렇기 때문에 hk2에서 즉시 초기화를 하기 위해서는 @Immediate annotation을 통해 명시적으로 지정을 해주어야한다.   @Path(\"/monitor/l7check.html\")   @Immediate   public class L7Resource { \t// 아래 세부 구현 }   그러나 이렇게만 설정하면 여전히 즉시 초기화가 일어나지 않는다. 그 이유는 위에서의 전역적인 초기화 전략이 여전히 지연 초기화로 설정되어 있기 때문이다.  따라서 아래와 같은 설정을 통해 전역 설정을 Immediate을 활성화하도록 변경해주어야한다.  @ApplicationPath(\"/\")   public class ApplicationConfig extends ResourceConfig {          @Inject       public ApplicationConfig(ServiceLocator serviceLocator) {          ServiceLocatorUtilities.enableImmediateScope(serviceLocator);          register(L7Resource.class);        // 후략     }   위와 같이 설정하면, 애플리케이션 구동 시점에 싱글턴 객체가 모두 애플리케이션 구동 시점에 즉시 초기화되는 것을 확인할 수 있다.   번외1. Singleton의 구현에 따른 초기화 시점 차이   아래 두가지 코드는 Java에서 싱글톤의 구현으로 흔하게 볼 수 있는 구현이다.  public class ServiceA {   \t \tprivate static final ServiceA INSTANCE = new ServiceA();          private ServiceA() { \t    System.out.println(\"ServiceA instantiated\");     }        public static ServiceA getInstance() {           return INSTANCE;       }          public static void doNothing() {} }   public class ServiceB {          private ServiceB() { \t    System.out.println(\"ServiceB instantiated\");     }        private static class Singleton {           private static final ServiceB INSTANCE = new ServiceB();       }          public static ServiceB getInstance() {           return Singleton.INSTANCE;       }          public static void doNothing() {} }   실제로 서비스 중인 코드에도 두가지 구현이 혼재되어 있었다.  사실 이 이슈를 살펴보기 전까지는 A와 B의 차이가 없다고 생각해서 무엇을 쓰든 상관없다고 생각했다. 왜냐하면, 애플리케이션 구동 시점에 모든 클래스가 항상 로딩된다고 생각했고, 그런 관점에서는 두 코드의 동작에 차이가 없기 때문이다.  그런데, 위에서 살펴본 것처럼 모든 클래스는 항상 로딩되는 것이 아니고 필요한 시점에서야 lazy하게 로딩됨을 알았다. 이 사실을 알고나면, 두 가지 구현은 실제 동작에서 분명한 차이가 발생한다.   실제로 아래와 같은 코드를 수행해보면 그 차이를 명확하게 확인할 수 있다.  import org.junit.jupiter.api.Test;      public class TestRunner {            @Test     public void testA(){           ServiceA.doNothing();       }          @Test       public void testB(){           ServiceB.doNothing();       } }   테스트 수행 결과 ServiceA instantiated만 출력된다.   ServiceA는 doNothing() static 메소드를 호출하면서 ServiceA 클래스가 로딩되고, 로딩될때 static 변수인 INSTANCE가 초기화된다.  그러나, ServiceB는 doNothing() static 메소드를 호출할때 ServiceB 클래스가 마찬가지로 로딩되지만, 이것이 ServiceB.Singleton 클래스의 로딩을 강제하지 않기 때문에, INSTANCE는 초기화되지 않는다. 그리고 실제로 ServiceB.getInstance()를 호출하는 시점에서야 ServiceB.Singleton 클래스가 로딩되면서 INSTANCE 객체가 초기화된다.  그리고 ServiceB의 구현 방식을 Lazy Holder 방식이라고 부르는 것 같고, 실제로 가장 많이 쓰인다고 한다.   번외2. Spring과 지연 초기화   스프링 공식 문서에 따르면, 지연 초기화의 단점에 대해 아래와 같이 설명하면서, 디폴트로 지연 초기화가 아닌 즉시 초기화를 채택한 이유에 대해 설명하고 있다.      There are a few downsides to lazy initialization that mean we believe it’s better to opt-in once you have decided it makes sense to do so. Due to classes no longer being loaded and beans no longer being created until they’re needed, it’s possible for lazy initialization to mask a problem that previously would have been identified at startup. Such problems can include no class def found errors, out of memory errors, and failures due to misconfiguration.    결국 fail-fast 관점에서 지연 초기화는 별로 바람직하지 않고, 빈 초기화에 실패한다면 애플리케이션 구동 시점에 빠르게 실패하는 것이 대부분의 경우 바람직하다.  (그외에 지연 초기화가 의미 있는 경우에 대해 궁금하다면 Lazy Initialization in Spring Boot 2.2 문서를 참고하면 좋을듯 하다)   참고로 Spring에서도 지연 초기화를 활성화하고 싶다면 빈에 @Lazy annotation을 지정함으로써 활성화할 수 있으며, Spring boot 2.2 부터는 spring.main.lazy-initialization=true 설정을 통해 지연 초기화를 전역적으로 활성화할 수 있다.   References      https://docs.oracle.com/javase/specs/jls/se8/html/jls-12.html#jls-12.4.1   web services - How to initialize injected value before REST call? - Stack Overflow   java - How do I get my Jersey 2 Endpoints to eagerly initialize on startup? - Stack Overflow   rest - Initialize singleton in Java Jersey 2 JAX-RS - Stack Overflow   GlassFish HK2 - Dependency Injection Kernel   GitHub - eclipse-ee4j/glassfish-hk2: Dynamic dependency injection framework   Spring Boot Features   [Spring] Bean Lazy Initialization 사용법   Lazy Initialization in Spring Boot 2.2   단일체 패턴 (Singleton Pattern)  ","categories": ["Jersey"],
        "tags": ["Jersey","Java"],
        "url": "https://dreamsh19.github.io/jersey/Jersey-Singleton-%EA%B0%9D%EC%B2%B4%EC%9D%98-%EC%B4%88%EA%B8%B0%ED%99%94-%EC%8B%9C%EC%A0%90/",
        "teaser": null
      },{
        "title": "중복 코드의 Git 히스토리를 병합하기 위한 시행착오",
        "excerpt":"이슈     신규 API 코드와 기존 API 코드를 동시에 서비스하던 중, 기존 API를 deprecate시키고, 기존 API 코드를 삭제하려고 하는 상황이었다.   그런데 문제는 신규 코드 최초 작성 당시, 기존 코드를 복붙하여 작성되었고, 수년 간의 git 히스토리는 기존 코드에만 남아있고, 신규 코드의 git 히스토리는 복붙 이후의 시점만 기록되어 있었다.   아래는 기존 코드에는 히스토리가 다 남아있는 반면, 신규 코드에는 복붙 히스토리만 남아있는 모습이다.            (그리고 여기서의 중요한 포인트는 신규 코드는 기존 코드와 90% 이상 일치하는 중복 코드였기 때문에, 신규 코드는 사실상 동일한 코드에서 git 히스토리만 사라진 코드라고 볼 수 있었다.)                          기존       신규                       ![Image](https://github.com/user-attachments/assets/904ae9a7-e095-4cb4-9d57-8c7839fa3e00)       ![Image](https://github.com/user-attachments/assets/af877689-0eeb-40cf-a949-1d053601bfdf)              여기서 이슈가 발생하는데, 기존 API 코드를 삭제하더라도, git 히스토리는 유지하고자 했다.            개인적으로 각종 히스토리를 파악하는데 git 히스토리를 자주 활용하기도 하고,       단순 히스토리 파악을 포함해서, 미처 몰랐던 선대(?) 개발자들의 의도를 알 수 있어서 같은 실수를 반복하지 않게 하기도 하기 때문이다.       기존 코드를 그냥 삭제해도 히스토리를 보려면 볼수야 있지만, 기존 코드 삭제 이후에는 삭제된 기존 코드가 어떤 것이었는지 알아내서 찾아가야 한다.           그리고 결정적으로 해당 클래스가 비지니스의 대부분의 주요한 로직과 히스토리를 담고 있는 클래스였기 때문에 git 히스토리를 그대로 가져가고자 했다.   그래서 아래와 같은 목표를 달성하기 위한 시행착오를 이 글에서 다루었다.   목표   기존 코드의 git 히스토리를 신규 코드의 git 히스토리와 병합하고, 기존 코드를 삭제한다.   1. Git blame 병합   사실 이것을 시작한 최초의 목적은 글의 서두에서 보여준 기존 코드의 git blame을 신규 코드의 git blame에 반영하고 싶었던 것이었다.  그리고 동일한 니즈에 대한 해결책을 다룬 글이 있었다!   요약하면 rename-rename merge conflict를 이용하는 방법으로, 서로 다른 파일 A,B의 git blame을 합치고자 할때, 아래와 같이 수행한다.     branch1 : A-&gt;C rename   branch2 : B-&gt;C rename   branch1과 branch2를 merge            이때 파일 C에 대해 발생하는 merge conflict를 resolve 해주어야한다.           구체적인 방법은 글에서 자세하게 다루고 있으니, 그대로 따라하면 된다.   참고로, 이때 기존 코드를 신규 코드로(A-&gt;B) 바로 merge하지 않는 이유는, 이렇게 하게 되면, git blame이 merge 시점의 commit으로 모두 덮어써져 버리기 때문에, 달성하고자 하는 목적에 벗어난다.   결과물      기존 코드와 신규 코드의 git blame이 잘 병합되었다.   추가적인 주의사항   위의 글에 따라 반영하면서 추가적으로 직접 겪었던 시행착오와 그에 따른 주의사항을 기록하였다.      merge conflict 해소시에는 기존 파일들의 라인을 선택/제거만 하고, 수정은 하지 않는 것이 좋다. 수정이 필요하다면 이후의 별도 커밋에서 수행하는 것이 git blame 유지에 바람직하다.            그 이유는, merge conflict 해소 시 기존에 있던 라인을 수정하면, git에서는 merge 시점에 새로운 라인을 추가한 것으로 인식하고, 해당 라인에 대한 git blame도 merge 커밋 시점으로 기록된다.                    (git 입장에서는 기존 라인을 수정한 것인지 새로운 라인을 추가한 것인지 자체를 구분하는 것이 불가능하기 때문에)                       그래서 merge conflict 해소 직후 시점에는 컴파일이 되지 않는 상태의 커밋이 불가피하게 존재할 수 있다.(수정이 불가능하므로)           완전히 동일한 라인이 두 개의 파일에 모두 포함되어 있을때, 해당 라인에 대한 git blame을 어떤 파일의 것으로 따라갈지는 git의 자체적인 휴리스틱(해당 라인의 앞뒤 라인의 유사도 등)에 따라 결정되기 때문에 일관된 방법을 찾는 것은 한계가 있다.            따라서 위 글의 예시에서처럼 기계적인 merge conflict resolve는 현실적으로 어렵다.       위 글에서 merge conflict가 기계적으로 resolve 가능했던 것은 합치고자 하는 두 개의 파일의 내용이 서로 다른 상황이었기 때문이다.           rename 시에는 Intellij 등의 IDE 환경에서 rename하는 경우 파일의 “내용”(import나 클래스명 등)을 자동으로 바꿔주기 때문에 cli 환경에서 단순 mv만 실행하여 파일명만 변경하는 것을 권장한다.   2. Github의 파일 히스토리 통합   위에서 git blame을 합쳤으니 목적을 달성하고 마무리하려고 했다.. 그러나 예상치 못한 이슈가 발생했다.  위에서 작업한 내용을 푸시하여 Github에서 파일 히스토리를 확인해보니, rename 이전의 히스토리가 신규 코드 쪽 히스토리만 남아있고, 기존 코드의 히스토리는 누락된 것이다.     (Github UI 상에서 기존 파일에 대한 히스토리가 누락됐다.)   git blame만큼이나 파일 히스토리 또한 중요하기도 하고, 왜 신규 파일의 히스토리만 보존된 것인지 확인하기 위해 살펴보기 시작했다.   Git의 파일 히스토리와 git log 명령어   들어가기에 앞서, git에서의 “파일” 히스토리에 대해 이해할 필요가 있다.  git의 기본 단위는 커밋이고, git에서는 단순히 커밋의 그래프만을 관리하고 추적하기 때문에 내부적으로는 “파일” 단위의 히스토리라는 개념 자체가 없다.   다만, 파일 단위의 히스토리 조회 목적을 위해 git은 유틸성으로 git log ${filename} 형태의 명령어를 제공한다.  그리고 이 git log ${filename} 명령어는 결국엔 커밋 그래프를 순차적으로 탐색하면서 해당 ${filename}이 포함된 커밋만을 필터링해서 출력해주는 방식으로 동작한다. (해당 파일에 대한 커밋 목록을 따로 관리하지 않기 때문에)   그리고 해당 명령어는 기본적으로는 rename 이전 히스토리에 대한 추적을 지원하지 않는다. rename 커밋을 만나면, 필터링의 기준이 되는 파일명이 더 이상 존재하지 않으니 그대로 종료한다.  하지만 rename도 커밋 중의 하나일 뿐이기 때문에, rename 이전의 파일 히스토리도 이어서 보고 싶은 것은 자연스러운 니즈이며, 이러한 니즈를 위해서 git log 에는 --follow 옵션을 제공한다.   git log --follow ${filename} 형태의 명령어를 통해 rename 이전의 파일 히스토리도 함께 조회할 수 있다.   Github의 파일 히스토리   Github에서도 파일 히스토리(특정 파일에 대한 commit 리스트)를 아래와 같이 제공한다.     그리고 Github에서는 git log 디폴트 옵션처럼, 원래 rename 이전의 파일 히스토리를 UI 상에서 지원하지 않았으나, 2022년 업데이트로 rename 이전의 파일 히스토리도 조회할 수 있도록 업데이트되었다.       https://github.blog/changelog/2022-06-06-view-commit-history-across-file-renames-and-moves/   예상하기로는 해당 기능은 동일한 목적의 git log --follow 옵션을 내부적으로 사용할 것 같다.  그런데 위 changelog에서 주목할 점은 git log --follow와 “similar”한 방식으로 동작한다는 문구이다.  결국 git log --follow 수행결과와 동일하지 않다는 것인데, 실제로 수행결과가 유사하기는 하나, 완전히 일치하지 않았다.   왜 이미 동일한 목적으로 존재하는 git log --follow 명령어를 그대로 사용하지 않을까?  그 이유는 git log --follow의 버그에 있다.   git log --follow의 버그   git log --follow의 내부 동작 방식을 살펴보자.  git log --follow\b 의 내부 동작 방식을 자세히 설명한 글이 있어 일부를 발췌하였다.      위의 예시를 단순화하여 표현하면 아래와 같다. (커밋의 이름은 커밋시간 순이며, 그래프를 시간 순으로 정렬하였다.)      예시를 바탕으로 정리하면, (참고로 발췌한 부분의 예시가 반대로 적혀있어 이를 정정하고, 추가 내용을 보강하여 정리하였다.)     commit 4에서 git log --follow b.txt 명령어를 수행한 상황에서 살펴보자.   --follow 옵션은 rename 커밋(commit 3)을 만나면 탐색대상 파일을 바꿔치기(b.txt -&gt; a.txt) 하는 방식으로 동작한다. (as-is, to-be 파일명을 둘다 추적하는 방식으로 동작하지 않는다.)   그렇기 때문에 “바꿔치기가 언제 됐는지”(rename 커밋(commit 3)을 언제 방문했는지) 시점에 따라 결과가 달라진다. 예를 들어,            바꿔치기가 먼저 됐다면(commit 3를 먼저 방문했다면, 4-&gt;3-&gt;2-&gt;1), \bcommit 3 방문 시점에 탐색 대상 파일명이 b.txt -&gt; a.txt로 바꿔치기가 되고, commit 2 방문 시점에는 대상 파일명이 a.txt이므로 commit 2가 포함된다. 결과는 3-&gt;2-&gt;1 로 출력된다.       하지만, 바꿔치기가 나중에 됐다면(commit 3를 나중에 방문했다면, 4-&gt;2-&gt;3-&gt;1), as-is 파일명 a.txt로 기록된 commit 2를 방문하는 시점에 탐색 대상 파일명은 b.txt이므로 commit 2는 누락된다. 결과는 3-&gt;1 로 출력된다.                    위 stackoverflow 예시에서 최초 작성자가 의문을 제기한 케이스                           확인 결과                       그리고 git log의 탐색 순서는 priority queue 기반의 BFS(breadth first search) 방식으로 탐색한다.            자세한 내용은 git log 우선순위 관련 참고           어쨌든 위의 내용은 내부적인 동작 방식에 대한 디테일한 설명인 것이고,  중요한 것은 git log --follow 명령어는 git log의 탐색 순서가 따라 결과 셋이 달라질 수 있다는 것이다.   git log --follow의 한계   위에서 git log --follow가 동시에 최대 한개의 파일만 추적이 가능하다는 것을 알았다.   그럼 본론으로 돌아와 해결하고자 하는 문제에 위 내용을 적용해보자.  1. Git blame 병합 파트에서 적용하고자 하는 방법을 위의 예시와 동일한 도식으로 나타내면 아래와 같다.      그리고 이 상황에서 탐색의 경우의 수에 따른 git log --follow C 수행 시나리오를 떠올려보자.     commit 3을 먼저 방문(4-&gt;3-&gt;2-&gt;1-&gt;0)            commit 3 방문 시점에 탐색 대상 파일을 A로 변경       commit 2는 A가 포함되지 않았으므로 제외       commit 1은 A가 포함됐으므로 출력, commit 0은 A가 포함되지 않았으므로 제외       결과 : 4-&gt;3-&gt;1 (2와 0은 누락)           commit 2를 먼저 방문(4-&gt;2-&gt;3-&gt;1-&gt;0)            commit 2 방문 시점에 탐색 대상 파일을 B로 변경       commit 3는 B가 포함되지 않았으므로 제외       commit 1은 B가 포함되지 않았으므로 제외, commit 0은 B가 포함됐으므로 포함.       결과 : 4-&gt;2-&gt;0 (3과 1은 누락)           두 가지 시나리오를 살펴본 결과, 결국 양쪽에서 rename이 발생한 경우, 어느 쪽을 먼저 탐색하든, 다른 한쪽이 누락될 수 밖에 없다.  이는 git log --follow가 동시에 최대 한개의 파일만 추적하기 때문에 발생하는 한계이며, 히스토리를 보존하고자 하는 본 과제의 목적상 아주 치명적이다.   그리고 해당 한계에 대해 다룬 글이 있어 일부를 발췌하였다.     결국 parent branch가 여러 개 있고, 한쪽 parent에서(혹은 양쪽 모두에서) rename이 있었던 경우 나머지 parent의 커밋 히스토리는 누락된다는 내용이다.   Github의 방식   앞서 Github에서는 파일 히스토리를 git log --follow와 “유사한” 방식으로 보여준다고 했다.  git log --follow는 위와 같은 버그와 한계가 있으니, Github에서는 이를 해결하였을까?   결론부터 얘기하면 아니다.  Github의 UI 상에서 지원하는 파일 히스토리는 git log --follow와 약간 다른 결과를 보여주지만, 본질적으로 본 과제에서 문제가 되는, 한 쪽 파일의 히스토리가 누락되는 문제는 해결되지 않았다.   Github의 파일 히스토리는 아래와 같이 동작하는 것으로 추측하고 있다. (관련 공식 레퍼런스를 찾진 못했다.)  Github url을 통해 아래와 같이 추측하였고, 추측한 내용을 로컬에서 수행한 결과와 화면에서 보여준 결과가 동일하였다.   커밋 c1에 의해 A-&gt;B로의 rename이 있었고, 커밋 c2 상태에서 B의 파일 히스토리를 조회하는 시나리오를 기준으로,     최초에 파일 히스토리 조회시 --follow 옵션 없이 git log c2 B를 수행            파일 히스토리 버튼 클릭시 github.com/{repo}/commits/c2/B url로 연결된다.                  마지막 커밋(rename 커밋, 즉 c1)에 도달한 경우 c1으로 부터 다시 git log c1 A를 수행            Github에서는 rename이 있었던 경우 Rename from A (Browse History) 형태의 링크를 제공하며,       해당 링크는 github.com/{repo}/commits/c1/A?browsing_rename_history=true&amp;new_path=B&amp;original_branch=c2로 연결된다.                  마지막 커밋이 rename이 아닐때까지 1,2를 반복   결국 요약하면, --follow 옵션 없이 git log HEAD filename 형태의 명령어를 HEAD와 filename을 바꿔가며 반복적으로 호출하는 형태이다.  하지만 여전히 동시에 최대 한개의 파일만 추적한다는 동일한 한계를 갖고 있기 때문에, 한 쪽 파일의 히스토리가 누락되는 한계는 여전히 남아있다.   특정 파일의 히스토리를 의도적으로 채택하는 방법(git log의 탐색 우선순위)   위에서 살펴보았듯이, git log --follow든, Github이든 한 쪽의 히스토리가 누락되는 것은 불가피해보인다.  그럼 결국 한 쪽 파일의 히스토리를 선택해야하는 상황에 봉착한다. 본 과제의 목적은 기존 코드의 히스토리를 보존하고자 하는 것이었으므로, 기존 코드의 히스토리를 채택하자.   Github의 방식 파트에서 살펴보았듯이, 기존 파일의 히스토리를 보존하기 위해서는 git log 탐색 순서상 마지막 커밋이 기존 파일의 rename 커밋이 되어야한다.  그렇다면, git log의 탐색 순서는 어떻게 결정되는 것일까?   앞서 “git log의 탐색은 priority queue 기반의 BFS(breadth first search) 방식으로 탐색한다.” 라는 내용을 언급하였다.  이 말은 곧 git log는 우선순위 기반으로 탐색을 한다는 것이고, 우선순위의 디폴트로써 “커밋 시간”의 역순을 채택하고 있다.  결국 특정 커밋(보존하고자 하는 파일의 rename 커밋)을 git log가 마지막에 탐색하도록 하게 하기 위해서는 해당 커밋을 시간순으로 먼저 수행하면 된다.   따라서, 기존 파일의 rename 커밋이 마지막에 탐색되도록 하기 위해, 아래와 같이 기존 파일의 rename 커밋을 먼저 수행 후, 신규 파일의 rename 커밋을 이후에 수행하였다.      git log 우선순위 관련 참고     파트 도입부 에서 문제가 되었던, 히스토리가 신규 파일로 연결되었던 것도, 신규 파일의 rename 커밋이 기존 파일의 rename 커밋보다 시간순으로 먼저 수행되었기 때문이었다.   git log의 탐색 우선순위는 디폴트인 커밋 시간 외에도 git log의 옵션을 통해 지정할 수 있으며, --date-order, --author-date-order, --topo-order 등의 옵션을 제공한다. (Git - git-log Documentation 참고)   결과물   이로써 파트 도입부 에서 Github의 파일 히스토리가 신규 파일로 연결되었던 것을 기존 파일로 연결되도록 하였다.     3. Fast forward merge   여기까지 왔으면, 복잡한 과정을 대부분 지나왔다.  여기서 한 가지 더 추가적인 주의사항이 존재하는데, 이 주의사항을 놓치면 지금까지의 수고가 무용지물이 될 수 있다. 그것은 바로 merge 방식이다.   대부분의 피쳐 개발과 마찬가지로, 위의 모든 과정을 feature 브랜치에서 진행했기 때문에, develop 및 master로의 머지가 필요하다.  이때, 명시적인 merge commit이 발생하는 경우 master 및 develop 입장에서는 커밋 원자성이 훼손되기 때문에 커밋 원자성을 보존하기 위한 위의 노력들이 모두 물거품이 된다.  따라서, 명시적인 merge commit이 발생하지 않으면서 커밋 원자성을 보존할 수 있는 Fast forward merge 방식으로 머지해야한다.                  Non fast forward merge       Fast forward merge                       ![Image](https://github.com/user-attachments/assets/296705c1-8fae-4de4-ae93-5ad79c1bed7e)       ![Image](https://github.com/user-attachments/assets/b4469f8b-cd20-47e1-9c2a-d61a0a9e5c74)           (Non-Fast-forward merge와 Fast forward merge의 차이점. 출처 : 🌳🚀 CS Visualized: Useful Git Commands - DEV Community)   그러나 문제는 Github에서는 아직까지 UI 상에서 fast forward merge를 지원하지 않고, 명시적인 merge commit을 강제한다.  따라서 Github 상의 Pull request를 fast forward merge 하기 위해서는 아래와 같이 로컬에서 fast-forward merge 후 push하는 방식으로 수행할 수 있다.  push 시점에 Github에서는 이를 인지하여 Pull request를 머지 처리한다.     혹은 Github에서 Fast Forward PR · Actions · GitHub Marketplace · GitHub 과 같은 github actions를 활용할 수도 있다.   Fast-forward merge 관련 참고     Github에서 지원하는 다른 머지 방식인 squash and merge , rebase and merge는 fast-forward 와 유사한 방식으로 동작하지만 아래와 같은 이유로 적합하지 않다.            Squash and merge : 커밋 원자성이 훼손되므로, 이 경우에 적합하지 않다.       Rebase and merge : rebase는 결국 commit을 다시 수행하는 과정이라고 볼 수 있는데, 위에서 살펴본 것처럼 수동으로 충돌 해결이 필요하므로 버튼 자체가 disable된다.           그리고 사실 이 문제는 Github에서 fast forward merge를 지원하지 않기 때문에 발생하는 문제이고, Gitlab 등에서는 Fast forward merge를 UI 상에서 지원한다.                       결론   결국 1~3의 과정을 거쳐서 git blame를 병합하고, 기존 코드의 Github 파일 히스토리를 보존하면서 중복 코드를 병합할 수 있었다.   다만, 여기까지 오면서 느낀 점은 두가지인데,     복잡하다.   그리고 결정적으로 복잡함을 이겨내더라도, 온전하게 기존 히스토리를 유지할 수 없다는 한계점 또한 있다.   그렇기 때문에 본 과제를 진행하면서 배운 교훈은 아래와 같다.      위 방법을 사용하는 일이 없도록 하는 것이 가장 좋다.   다시 말해, 코드 복붙은 git 히스토리 관리 입장에서도 바람직하지 않다.            위에서 살펴본 방법으로 사후적인 수습은 가능하나, 복잡하고, 상황에 따라서는 온전한 수습이 되지 않을 수 있다.           코드 복붙이 불가피하다면, 복사 시점에 단순 내용 복사가 아닌 git 히스토리를 같이 복사할 수 있는 아래 방법을 활용하는 것이 좋다.            Git copy file preserving history - Stack Overflow 참고       요약하면, 별도 브랜치에서 각각 A-&gt;B, A-&gt;C를 수행 후 merge, 그 이후 C-&gt;A로 rename하면 A와 B가 동일한 내용과 히스토리를 갖고 복사된다.           References      Mundane git tricks: Combining two files into one while preserving line history - The Old New Thing   View commit history across file renames and moves - GitHub Changelog   Git - git-log Documentation   git log - `git log –follow –graph` skips commits - Stack Overflow   version control - Git log (–follow) not working to show history beyond renames - Stack Overflow   Does ‘git log’ use Depth First Search traversal to display the commits? - Stack Overflow   How to do a fast-forward merge on GitHub? - Stack Overflow   🌳🚀 CS Visualized: Useful Git Commands - DEV Community   About pull request merges - GitHub Docs   Fast Forward PR · Actions · GitHub Marketplace · GitHub   Merge methods | GitLab   Git copy file preserving history - Stack Overflow  ","categories": ["Git"],
        "tags": ["Git"],
        "url": "https://dreamsh19.github.io/git/%EC%A4%91%EB%B3%B5-%EC%BD%94%EB%93%9C%EC%9D%98-git-%ED%9E%88%EC%8A%A4%ED%86%A0%EB%A6%AC%EB%A5%BC-%EB%B3%91%ED%95%A9%ED%95%98%EA%B8%B0-%EC%9C%84%ED%95%9C-%EC%8B%9C%ED%96%89%EC%B0%A9%EC%98%A4/",
        "teaser": null
      }]
